[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Reproducible and Trustworthy Workflows for Data Science",
    "section": "",
    "text": "Welcome\nCourse notes for teaching concepts and practices related to reproducible and trustworthy workflows for data science. Specifically we cover workflows for writing reproducible, robust and valid computer scripts, analytic reports and data analysis pipelines, computational environments, as well as testing and deployment of software written for data analysis. Emphasis is placed on how to collaborate on the above tasks effectively with others using version control tools, such as Git and GitHub. Concepts are learned and applied using real data and case studies.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "lectures/010-intro-to-ds-workflows.html",
    "href": "lectures/010-intro-to-ds-workflows.html",
    "title": "1  How do reproducible and trustworthy workflows impact data science?",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "intro.html",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>How do reproducible and trustworthy workflows impact data science?</span>"
    ]
  },
  {
    "objectID": "lectures/010-intro-to-ds-workflows.html#learning-objectives",
    "href": "lectures/010-intro-to-ds-workflows.html#learning-objectives",
    "title": "1  How do reproducible and trustworthy workflows impact data science?",
    "section": "",
    "text": "Define data science, and the related terms reproducible and audible analysis\nGive examples of workflows that are considered reproducible and trustworthy in the context of a data analysis\nExplain why data analysis benefit from reproducible and audible workflows\nProvide real-life examples of how a failure in reproducible and trustworthy workflows has negatively impacted the outcome of a data analysis project",
    "crumbs": [
      "intro.html",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>How do reproducible and trustworthy workflows impact data science?</span>"
    ]
  },
  {
    "objectID": "lectures/010-intro-to-ds-workflows.html#data-science",
    "href": "lectures/010-intro-to-ds-workflows.html#data-science",
    "title": "1  How do reproducible and trustworthy workflows impact data science?",
    "section": "1.1 Data science",
    "text": "1.1 Data science\n\n\n\n\n\n\nData Science\n\n\n\nThe study, development and practice of reproducible and auditable processes to obtain insight from data.\n\n\nFrom this definition, we must also define reproducible and auditable analysis:\n\n\n\n\n\n\nReproducible and Auditable Analysis\n\n\n\nReproducible analysis: reaching the same result given the same input, computational methods and conditions \\(^1\\).\n\ninput = data\ncomputational methods = computer code\nconditions = computational environment (e.g., programming language & it’s dependencies)\n\nAuditable/transparent analysis: a readable record of the steps used to carry out the analysis as well as a record of how the analysis methods evolved \\(^2\\).\n\nNational Academies of Sciences, 2019\nParker, 2017 and Ram, 2013\n\n\n\n\n1.1.1 Why adopt this definition of data science?\n\nIt is important that insights from data science are trustworthy!\n\nTo quote Russel Hardin, “Trust involves giving discretion to another to affect one’s interests.” Data science insights are very often used to make important, real-life decisions that impact people and the environment (e.g., loan/credit approval, access to certain medical treatments, job applicant interview selection, criminal justice sentencing recommendations, etc). It is important that such analyses be trustworthy!\n\nReproducible and auditable methods are one of the expectations for trustworthy data science!\n\nDon Tapscott, one of the authors of The Naked Corporation, describes trust as: “honest, considerate, accountable, and transparent”. Accountability and transparency are not sufficienct for trust, but they are necessary.\n\n\n1.1.2 We cannot trust non-reproducible and non-auditable analyses because they:\n\nlack evidence that the results could be regenerated (is the analysis is reliable?)\nwe don’t know enough details of how they were created (is the analysis correct?)\nthere is an insufficient record of how and why analysis decisions were made (was a complete analysis performed?)",
    "crumbs": [
      "intro.html",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>How do reproducible and trustworthy workflows impact data science?</span>"
    ]
  },
  {
    "objectID": "lectures/010-intro-to-ds-workflows.html#what-makes-trustworthy-data-science",
    "href": "lectures/010-intro-to-ds-workflows.html#what-makes-trustworthy-data-science",
    "title": "1  How do reproducible and trustworthy workflows impact data science?",
    "section": "1.2 What makes trustworthy data science?",
    "text": "1.2 What makes trustworthy data science?\nSome possible criteria:\n\nIt should be reproducible and auditable\nIt should be correct (in the context of both statistics and software)\nIt should be complete\nIt should be fair, equitable and honest\n\n\nThere are many ways a data science can be untrustworthy… In this course we will focus on workflows that can help build trust. More training in data science ethics will be needed to help round out your education in how to do this. Further training in statistics and machine learning will also help with making sure your analysis is correct.\n\nIs this really important?\nYes! There are both small and big ways this can impact your work.\n\n1.2.1 An example with large impact\nRETRACTED ARTICLE: Safety and efficacy of favipiravir versus hydroxychloroquine in management of COVID-19: A randomised controlled trial\n\n\nA research paper was published in March 2021 that claimed that a drug, Favipiravir, was a safe and effective alternative to another drug, hydroxychloroquine (a medication commonly used to prevent or treat malaria), in mild or moderate COVID-19 infected patients.\nIn September, 2021 the paper we retracted by the editors - in part due to reproducibility issues:\n\n“After concerns were brought to the Editors’ attention after publication, the raw data underlying the study were requested. The authors provided several versions of their dataset. Post-publication peer review confirmed that none of these versions fully recapitulates the results presented in the cohort background comparisons, casting doubt on the reliability of the data. Additional concerns were raised about the randomisation procedure, as the equal distribution of male and female patients is unlikely unless sex is a parameter considered during randomisation. However, based on the clarification provided by the authors, sex was not considered during this process. The Editors therefore no longer have confidence in the results and conclusions presented.”\nThe problem doesn’t just stop once the article is retracted… Between the time the article was published and retracted, the article was cited 17 times!\n\n\n\n1.2.2 How big is this problem?\nSearching the Retraction Watch Database for “Results Not Reproducible” we find 635 records that match!\n\n\n\n1.2.3 Does this just impact academia?\nNo! The use of non-reproducible tools can impact government and industry as well! Consider this case:\n\nSource: https://www.bbc.com/news/uk-scotland-edinburgh-east-fife-53893101\nWhat went wrong?\nAn audit found that the wrong spreadsheet matrix was copied over, and the calculation for only 4 air changes per hour, instead of the required 10 per hour, was done. This error was missed several times by human review of the spreadsheet.\nHow could this have been prevented via using reproducible tools?\nIf code instead of a spreadsheet was used for calculations, then unit tests could have been written to check the calculations. Also, the code could be abstracted to a well named function, or a function with well named arguments, that could have been more easily detected than a hidden formula in a spreadsheet.",
    "crumbs": [
      "intro.html",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>How do reproducible and trustworthy workflows impact data science?</span>"
    ]
  },
  {
    "objectID": "lectures/010-intro-to-ds-workflows.html#examples-with-relatable-impact",
    "href": "lectures/010-intro-to-ds-workflows.html#examples-with-relatable-impact",
    "title": "1  How do reproducible and trustworthy workflows impact data science?",
    "section": "1.3 Examples with relatable impact",
    "text": "1.3 Examples with relatable impact\nLet’s talk about some of these by sharing data-related workflows from the trenches:\n\n\n\n\n\n\nPart 1\n\n\n\n\nThink and write down a non-reproducible, or non-auditable, workflow you have used before at work, on a personal project, or in course work, that negatively impacted your work somehow (make sure to include this in the story).\nAs a Masters student, I started to use R to do my statistical analysis. I obtained the results I needed from running my code in the R console and copying the results into the word document that was my manuscript. Six months later we were working on revisions requested by the reviewers and I could not remember which version of the code I ran to get my results. I eventually figured it out through much trial and error, but the process was inefficient and very stressful.\n–Tiffany Timbers\nWhen prompted, paste your story in the Google doc (link to be shared in class)\n\n\n\n\n\n\n\n\n\nPart 2\n\n\n\nFollow the instructions below for each story share in the class stories from the trenches Google document.\n\nRead the story and reflect on which of the themes listed below was likely the biggest cause of the reproducibility or transparency failure described in the story.\nLabel the story with the emoji corresponding to the chosen theme from the table below:\n\n\n\n\nReproducibility and transparency themes\nEmoji label\n\n\n\n\nCode\n⌨️ (keyboard)\n\n\nComputational environments\n💻 (laptop)\n\n\nSoftware design\n💾 (floppy disk)\n\n\nData analysis pipeline\n➡️ (right arrow)\n\n\nDocumentation\n📄 (page facing up)\n\n\nProject organization\n🗃 (card file box)\n\n\nRandomness\n🎲 (game die)\n\n\nVersion control\n📜 (scroll)\n\n\n\n\n\n\n\n\n\n\n\nPart 3\n\n\n\nFollow the instructions below for each story share in the class stories from the trenches Google document.\n\nRe-read the story and reflect on what the primary cost was from the reproducibility or transparency failure described in the story.\nLabel the story with the emoji corresponding to the chosen cost from the table below:\n\n\n\n\n\n\n\n\nReproducibility and transparency failure cost\nEmoji label\n\n\n\n\nFinancial\n💸 (money with wings)\n\n\nReputational\n🎖 (military medal)\n\n\nTime\n🕰 (mantelpiece clock)\n\n\nUnfixable error\n💥 (collision)\n\n\nWork discarded.\n🗑️(wastebasket)\n\n\n\nSource: http://www.bonkersworld.net/building-software/",
    "crumbs": [
      "intro.html",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>How do reproducible and trustworthy workflows impact data science?</span>"
    ]
  },
  {
    "objectID": "lectures/010-intro-to-ds-workflows.html#wrap-up",
    "href": "lectures/010-intro-to-ds-workflows.html#wrap-up",
    "title": "1  How do reproducible and trustworthy workflows impact data science?",
    "section": "1.4 Wrap up",
    "text": "1.4 Wrap up\n\nWe define data science as the study, development and practice of reproducible and auditable processes to obtain insight from data.\nBoth bolded parts of the definition are important! This course will primarily focus on the first part, but you will get the opportunity to practice the second part in your group projects for this course.\nMany ways a data analysis can be untrustworthy… just because a data analysis is reproducible and auditable, doesn’t mean it is fully trustworthy. But a data analysis is not trustworthy if it cannot be reproduced or studied…",
    "crumbs": [
      "intro.html",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>How do reproducible and trustworthy workflows impact data science?</span>"
    ]
  },
  {
    "objectID": "lectures/020-intro-to-bash.html",
    "href": "lectures/020-intro-to-bash.html",
    "title": "2  Introduction to the Bash Shell",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "intro.html",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to the Bash Shell</span>"
    ]
  },
  {
    "objectID": "lectures/020-intro-to-bash.html#learning-objectives",
    "href": "lectures/020-intro-to-bash.html#learning-objectives",
    "title": "2  Introduction to the Bash Shell",
    "section": "",
    "text": "Explain what the Bash shell is and why we use it in data science computing.\nRecognize the directory hierarchy as it is commonly represented in diagrams, paths and in the file explorer software.\nDistinguish common operators and representations of the different filesystem elements typically used in the Bash shell.\nExplore the filesystem using Bash commands as ls, pwd and cd.",
    "crumbs": [
      "intro.html",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to the Bash Shell</span>"
    ]
  },
  {
    "objectID": "lectures/020-intro-to-bash.html#what-is-the-bash-shell-and-why-do-we-use-it",
    "href": "lectures/020-intro-to-bash.html#what-is-the-bash-shell-and-why-do-we-use-it",
    "title": "2  Introduction to the Bash Shell",
    "section": "2.1 What is the Bash shell and why do we use it?",
    "text": "2.1 What is the Bash shell and why do we use it?\nIn this section we will begin by discussing how we communicate with computers, and end with describing the Bash shell and our rationale for using it in data science computing.\n\n2.1.1 Computer communication\nBefore we get into practically doing things, I want to give some background to the idea of computing. Essentially, computing is about humans communicating with machines to modulate flows of current in the hardware. Early examples of human computer communication were quite primitive and included actually disconnecting a wire and connecting it again in a different spot.\nLuckily, we are not doing this anymore; instead we have graphical user interface with menus and buttons, which is what you are commonly using on your laptop.\nThese graphical interface can be thought of as a layer (or shell) around the internal components of your operating system. Shells exist as an intermediate that both makes it easy for us to express our thoughts, and for computers to interpret them.\n\n\n2.1.2 Text-based communication\nWe will learn how to communicate to a computer via a text-based shell, rather than a graphical one.\nUsing a text-based shell might at first seems counter-intuitive, since the reason for creating a shell in the first place was to facilitate user interaction with the computer. So now that we have these easy to use graphical user interfaces (GUIs), why would anyone in their right mind go back to using a text based interface?\nWell, it’s partly a misconception, GUIs are nice when you are new to something, but text based interfaces are actually faster and easier to use when you know what you are doing.\nWe can compare it to learning a language, in the beginning it’s nice to look things up in a dictionary (or a menu on the computer), but once I know what I want to say, it is just easer to say or type it directly, instead of looking in submenues.\nAnd by extension, it would be even faster to speak or even just think of what you want to do and have it executed by the computer, this is what speech- and brain-computer interfaces are concerned with.\n\n\n2.1.3 Bash\nBash is the most commonly used text shell. You have it installed on your computer by default if you are using Mac or Linux machine, and if you are on a Windows machine you downloaded bash as part of the setup instructions. Sometimes we might use “prompt”, “command line”, or “terminal”, which for the purposes of this lecture, refers to the same thing.\nThe abbreviation BASH stands for Bourne Again SHell. Other shells existed before Bash, and one of the most successful early shells was invented by Stephen Bourne at Bell Labs in 1977, which he called the Bourne Shell. In 1989, the Free Software Foundation improved the Bourne Shell and as a pun named it the Bourne Again Shell, to symbolize it was now “reborn” with new features.\nText-based shells are also called command-line interfaces (CLI). The heart of every CLI is a read-evaluate-print loop (REPL). When we type a command and press Return (also called Enter) the CLI reads the command, evaluates it (i.e., executes it), prints the command’s output, and loops around to wait for another command. Let’s see how to do that next!",
    "crumbs": [
      "intro.html",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to the Bash Shell</span>"
    ]
  },
  {
    "objectID": "lectures/020-intro-to-bash.html#using-the-bash-shell",
    "href": "lectures/020-intro-to-bash.html#using-the-bash-shell",
    "title": "2  Introduction to the Bash Shell",
    "section": "2.2 Using the bash shell",
    "text": "2.2 Using the bash shell\nWhen you open the Bash shell you should see $, which is the default prompt character. By default, the prompt character is usually prefixed by several other pieces of information:\n\nthe current username\nhostname of the computer’s system (follows the @ symbol)\nthe current working directory (where you are currently in the computers operating system)\n\nIt often looks like this:\nusername@hostname:~$\n\nNote: ~ in the example above indicates the current location is the user’s home directory. This is the default location the Bash shell opens to.\n\nSometimes the name of the computational environment also prefixes the prompt character. This name usually comes first and is surrounded by parentheses:\n(base) [username@localhost ~]$\n\n2.2.1 Finding your location in the filesystem\nThe pwd command stands for “print wording directory”. We use this command to find out where our Bash shell is in the computers filesystem\npwd\n/home/username\nEach user has a home directory; the function and location of this directory differs a little bit between operating systems. On Linux it is usually /home/username, on MacOS it is /Users/username, and on Windows it will show up as /c/Users/username (if you have Git Bash installed). Our examples in this module shows the Linux directory structure, but you will see that some of the other modules show what we would see on MacOS or Windows.\n\n\n2.2.2 Viewing the filesystem\nThe ls command stands for “listing”. We use this command to see what files and folders exist somewhere on our computer using the Bash shell. If we want to see the files and folders that exist in our current working directory all we need to type is ls.\nls\nDownloads     Music\nDocuments     todo.txt\nPictures      my_program\n\nNote: our results may be different depending on our operating system and what files or directories we have.\n\nWe can also use ls to see what files and folders exist in other directories in the computers filesystem by giving ls a path to the directory of interest. For example, imagine we are in the computer’s home directory (as shown above) and we want to see what files and directories are in the Documents directory. To do this we would type:\nls Documents\ndsci-310-student\nletters\nsample-team-work-doc.pdf\ntextbooks\nubc-mds\nSometimes there are files in our computer’s filesystem that are prefixed with ., for example .gitignore, .DS_Store and .ipynb_checkpoints. These are known as hidden files, and your computer’s default is to hide them from you, regardless of whether you use a graphical user interface (e.g., Finder, Explorer, Nautilus) or the Bash shell to explore our computer’s filesystem. To force the Bash shell to show us these files we can use the -a flag/option. For example, if we do this to look in the Documents directory, we see a few new things!\nls -a Documents\n.\n..\n.DS_Store\ndsci-310-student\nletters\nsample-team-work-doc.pdf\ntextbooks\nubc-mds\nSome of these are quite uninteresting, for example the .DS_Store file, which is a file only found on Mac operating systems that gets created whenever a directory is viewed with the Finder program. However, . and .. are quite interesting and special to the Bash shell. The first, ., indicates the Bash shell’s current working directory. The second, .., indicates the Bash shell’s parent directory of the current working directory.\n\n\n2.2.3 Navigating the filesystem\nJust as we can view the computer’s filesystem using the Bash shell, we can also navigate the filesystem. To do this we use the cd command. The cd command stands for change directory. We use it similar to ls, in that we say the command and then we provide a file path indicating to where we want to navigate to.\nFor example if we wish to move from the home directory into the Documents directory, we would type:\ncd Documents\nWhen we move directories, the shell does not output anything to tell us where we have landed. So its good to remember the pwd (print workding directory) command so that we can check where we are from time to time:\npwd\n/home/username/Documents\nIf we want to move up the filesystem, this is where we need to use the special characters to refer to the parent directory (..). To move from the Documents directory back into the home directory we need to type:\ncd ..\nAgain, we can use pwd to probe whether we arrived where we hoped to!\npwd\n/home/username\nPhew! We made it! There are a few other cd shortcuts that are useful to know to help us efficiently navigate the filesystem. These include cd or cd ~, which takes you from any location back to the computer’s home directory. As well as cd -, which takes you to the previous directory you were in.\n\n\n2.2.4 Summary table of Bash commands and special characters\nCommands for viewing and navigating the filesystem\n\n\n\nCommand\nPurpose\nExample use\n\n\n\n\npwd\nPrint Working Directory\npwd\n\n\nls\nLiSt contents\nls Documents\n\n\ncd\nChange Directory\ncd Desktop\n\n\n\nSpecial characters\n\n\n\nSymbol\nDefinition\n\n\n\n\n.\nCurrent working directory\n\n\n..\nParent directory\n\n\n~\nHOME directory\n\n\n$\nBash shell prompt",
    "crumbs": [
      "intro.html",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to the Bash Shell</span>"
    ]
  },
  {
    "objectID": "lectures/020-intro-to-bash.html#common-bash-diagrams",
    "href": "lectures/020-intro-to-bash.html#common-bash-diagrams",
    "title": "2  Introduction to the Bash Shell",
    "section": "2.3 Common bash diagrams",
    "text": "2.3 Common bash diagrams\nIn previous courses you may have seen filesystem diagrams represented as a tree structure with icons representing the files and folders contained within the filesystem. An example of one such diagram is shown below:\n\nAnother common way a filesystem is often represented in computational documentation is using plain text. Below we recreate the same filesystem using a text-style diagram:\n/\n├── Users\n│   ├── Varada\n│   │   ├── 571_lectures\n│   │   └── 571_labs\n│   └── Florencia\n│       ├── dashboards\n│       └── notes\n└── Data\n    └── mds",
    "crumbs": [
      "intro.html",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to the Bash Shell</span>"
    ]
  },
  {
    "objectID": "lectures/020-intro-to-bash.html#attribution",
    "href": "lectures/020-intro-to-bash.html#attribution",
    "title": "2  Introduction to the Bash Shell",
    "section": "2.4 Attribution",
    "text": "2.4 Attribution\nThis chapter is derived from lecture notes created by Joel Ostblom, Florencia D’Andrea and Daniel Chen for the University of British Columbia’s Master of Data Science DSCI 521 course.",
    "crumbs": [
      "intro.html",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to the Bash Shell</span>"
    ]
  },
  {
    "objectID": "lectures/030-ssh-for-authentication.html",
    "href": "lectures/030-ssh-for-authentication.html",
    "title": "3  SSH for authentication",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "version-control.html",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>SSH for authentication</span>"
    ]
  },
  {
    "objectID": "lectures/030-ssh-for-authentication.html#learning-objectives",
    "href": "lectures/030-ssh-for-authentication.html#learning-objectives",
    "title": "3  SSH for authentication",
    "section": "",
    "text": "Describe how SSH authentication works\nSetup and use SSH authentication as the authentication method for working with GitHub",
    "crumbs": [
      "version-control.html",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>SSH for authentication</span>"
    ]
  },
  {
    "objectID": "lectures/030-ssh-for-authentication.html#the-secure-shell-protocol-ssh-for-authentication",
    "href": "lectures/030-ssh-for-authentication.html#the-secure-shell-protocol-ssh-for-authentication",
    "title": "3  SSH for authentication",
    "section": "3.1 The secure shell protocol (SSH) for authentication",
    "text": "3.1 The secure shell protocol (SSH) for authentication\nSo far you have likely been using a personal access token to authenticate to GitHub. This works, however there is another very secure and more convenient method of authentication that is widely used: secure shell protocol (SSH). SSH can be use for other forms of authentication as well (beyond GitHub), including logging into remote machines in the cloud. So for many these reasons it is worthwhile learning. Thus, we will spend some time explaining it here, and setup our computers to use this for authenticating with GitHub going forward.",
    "crumbs": [
      "version-control.html",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>SSH for authentication</span>"
    ]
  },
  {
    "objectID": "lectures/030-ssh-for-authentication.html#remotely-accessing-another-computer-using-ssh",
    "href": "lectures/030-ssh-for-authentication.html#remotely-accessing-another-computer-using-ssh",
    "title": "3  SSH for authentication",
    "section": "3.2 Remotely accessing another computer using SSH",
    "text": "3.2 Remotely accessing another computer using SSH\nLet’s start with some definitions:\n\n\n\n\n\n\nDefinition\n\n\n\n\nSecure SHell (SSH) - a common method for remote login to another computer which is secure.\nserver - a machine you are SSHing into. The server sits and waits to be contacted.\nclient - usually your machine. The client initiates contact with the server.",
    "crumbs": [
      "version-control.html",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>SSH for authentication</span>"
    ]
  },
  {
    "objectID": "lectures/030-ssh-for-authentication.html#ssh-key-based-authentication",
    "href": "lectures/030-ssh-for-authentication.html#ssh-key-based-authentication",
    "title": "3  SSH for authentication",
    "section": "3.3 SSH key-based authentication",
    "text": "3.3 SSH key-based authentication\nTwo components:\n\npublic key\nprivate key\n\nThese files have an asymmetrical relationship:\n\nthe public key CANNOT decrypt messages generated by the private key\nthe private key CAN decrypt messages generated by the public key\n\n\n3.3.1 Understanding public key private key concepts\n\nThink of a public key, not as a key, but as a padlock that you can make copies of and put anywhere you want.\nTo put your ‘padlock’ on an another machine, you would copy it to authorized_keys in the ~/.ssh folder.\nThink of a private key as an actual key, it can open the padlock that is stored on the other machine.\n\n\nsource: http://blakesmith.me/2010/02/08/understanding-public-key-private-key-concepts.html\n\n\n3.3.2 How the lock works\n\nKeys are generated using ssh-keygen, to make private key (usually called id_ed25519) and a public key (usually called id_ed25519.pub)\nYou can make copies of id_ed25519.pub (public key/padlock) and distribute them to other machines\nThe other machine uses the public key to encrypt a challenge message to you\nYou need to show that you can decrypt the message to demonstrate that you are in possesion of the associated private key\n\n_Note: GitHub has changed recently the SSH key generation instructions to use Ed25519 algoritm.\n\n\n3.3.3 You can put your lock at many places\nAs long as you are using the same lock (public key), you will be able to open it with the same private key.\n\nsource: http://blakesmith.me/2010/02/08/understanding-public-key-private-key-concepts.html\n\n\n3.3.4 Keeping your private key safe\n\nssh-keygen allows you to put a password or passphrase on the private key\nthis should be shared with NO ONE!\nif your private key does fall into the wrong hands, the person must still know the password or passphrase to use the private key\n\n\nsource - https://xkcd.com/936/",
    "crumbs": [
      "version-control.html",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>SSH for authentication</span>"
    ]
  },
  {
    "objectID": "lectures/030-ssh-for-authentication.html#exercise",
    "href": "lectures/030-ssh-for-authentication.html#exercise",
    "title": "3  SSH for authentication",
    "section": "3.4 Exercise",
    "text": "3.4 Exercise\nSetting up SSH for authentication on GitHub! Follow the docs linked below here to accomplish each step to set this up. At the top of each of the documentation sections, be sure to select the tab for your operating system.\n\nChecking for existing SSH keys on your computer\nGenerating a new SSH key\nAdding your SSH key to the ssh-agent\nAdding a new SSH key to your GitHub account\nTest it out! Create a private GitHub.com repository and try to clone it using the SSH code URL (instead of the HTTPS one), it should look something like this: git@github.com:username/repo-name.git\n\n(whereas HTTPS code URLs look like: https://github.com/username/repo-name.git)",
    "crumbs": [
      "version-control.html",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>SSH for authentication</span>"
    ]
  },
  {
    "objectID": "lectures/040-version-control-1.html",
    "href": "lectures/040-version-control-1.html",
    "title": "4  Version control (for transparency and collaboration) I",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "version-control.html",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Version control (for transparency and collaboration) I</span>"
    ]
  },
  {
    "objectID": "lectures/040-version-control-1.html#learning-objectives",
    "href": "lectures/040-version-control-1.html#learning-objectives",
    "title": "4  Version control (for transparency and collaboration) I",
    "section": "",
    "text": "Explain why and how data analysis projects benefit from both local and remote version control\nUse Git’s basic functions to save changes to local and remote version control, as well as view and restore older versions of files",
    "crumbs": [
      "version-control.html",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Version control (for transparency and collaboration) I</span>"
    ]
  },
  {
    "objectID": "lectures/040-version-control-1.html#version-control---a-review",
    "href": "lectures/040-version-control-1.html#version-control---a-review",
    "title": "4  Version control (for transparency and collaboration) I",
    "section": "4.1 Version control - a review",
    "text": "4.1 Version control - a review\n\n\n\n\n\n\nVersion Control\n\n\n\nVersion control is the process of keeping a record of changes to documents, including when the changes were made and who made them, throughout the history of their development. It also provides the means both to view earlier versions of the project and to revert changes.\n– Data Science: A First Introduction\n\n\nIn this course we will learn to use the most popular version control software tools, Git and GitHub. A schematic of local and remote version control repositories using these tools is shown below:\n\nSource: Data Science: A First Introduction\n\nA poll!\nWe expect some introductory version control knowledge before the contents of this chapter. Essentially, what is covered in Chapter 12: Collaboration with version control from Data Science: A First Introduction. Here’s a poll to check you have the prerequisite knowledge for this chapter. If you cannot easily answer these questions we recommend you go back to Chapter 12: Collaboration with version control from Data Science: A First Introduction and read (or re-read) it.\n\n\n\n\n\n\nActivity 1\n\n\n\nWhich of these is untrue about the Git and GitHub version control software?\na. Allows you to view and/or retrieve older snapshots of the files and directories in a project.\nb. Automatically snapshots your work every 2 minutes.\nc. Provides transparency on who made what changes to files and directories in a document.\nd. Can act as a way to back-up your work.\n\n\n\n\n\n\n\n\nActivity 2\n\n\n\nGitHub is the software you use locally on your computer (i.e., your laptop) to commit changes to the version control history. True or False?\na. True\nb. False\nc. Neither true or false.\n\n\n\n\n\n\n\n\nActivity 3\n\n\n\nYou changed two files (notes.txt and eda.ipynb) but you only want to commit changes to one of them (eda.ipynb) to the version control history. Which Git command allows you to specify this?\na. Add\nb. Commit\nc. Push\nd. Push\n\n\n\n\n\n\n\n\nActivity 4\n\n\n\nAt a minimum, how often should you push your work to GitHub?\na. Every 5 min.\nb. Every 30 min.\nc. At the end of every work session.\nd. Once a week.\n\n\n\n\n\n\n\n\nActivity 5\n\n\n\nYou try to push your most recent commit from your locale version control repository to your remote repository on GitHub and it fails because Git says the remote contains work that you do not have locally. What do should you do next?\na. Commit the changes you made recently in your working directory.\nb. Force push your changes.\nc. Pull the changes from the remote repository that you do not have locally.\n\n\n\n\n\n\n\n\nActivity 6\n\n\n\n You pull changes that exist in your remote version control repository on GitHub that you do not have in your local version control repository, and you get the message\nAuto-merging in &lt;FILENAME&gt; CONFLICT (content):\nMerge conflict in &lt;FILENAME&gt; Automatic merge failed\n\nfix conflicts and then commit the result.\nWhat do you need to do? \na. Push the changes from the local repository that you do not have remotely.\nb. Force pull the changes.\nc. Manually open the file with the conflict and edit it to have the desired version of the changes, as well as remove the special Git syntax used to identify the merge conflict.",
    "crumbs": [
      "version-control.html",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Version control (for transparency and collaboration) I</span>"
    ]
  },
  {
    "objectID": "lectures/040-version-control-1.html#common-git-commands-at-the-command-line",
    "href": "lectures/040-version-control-1.html#common-git-commands-at-the-command-line",
    "title": "4  Version control (for transparency and collaboration) I",
    "section": "4.2 Common Git commands at the command line",
    "text": "4.2 Common Git commands at the command line\nIn the assigned textbook reading we use the JupyterLab Git extension tool to run Git version control commands (adding to the staging area, committing, pushing, pulling). However, here we will start using Git commands at the command line (i.e., Bash shell/terminal) because these commands are more stable than most Git graphical user interfaces (such as the JupyterLab Git extension tool). Additionally, the Git command line tool provides access to all Git features, some of which are missing in several Git graphical user interfaces Thus, below we list the most common Git commands that you would need at the command line:\n\n\n\n\n\n\n\nDescription\nCommand\n\n\n\n\nClone a remote version control repository from GitHub.com to your local computer\ngit clone\n\n\nChecking the status of the local version control repository\ngit status\n\n\nAdding a file to the staging area\ngit add &lt;FILENAME&gt;\n\n\nCommitting staged file to the version control history\ngit commit -m \"Some relevant message about the changes\"\n\n\nPush changes to the local version control repository to the remote repository on GitHub.com\ngit push\n\n\nPull changes from the remote version control repository on GitHub.com to the local repository\ngit pull\n\n\nViewing the version control history\ngit log",
    "crumbs": [
      "version-control.html",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Version control (for transparency and collaboration) I</span>"
    ]
  },
  {
    "objectID": "lectures/040-version-control-1.html#hands-on-practice-with-merge-conflicts",
    "href": "lectures/040-version-control-1.html#hands-on-practice-with-merge-conflicts",
    "title": "4  Version control (for transparency and collaboration) I",
    "section": "4.3 Hands on practice with merge conflicts!",
    "text": "4.3 Hands on practice with merge conflicts!\nOne of the major blockers getting used to using version control is dealing with merge conflicts! 😱\nGit can automatically handle merging two versions of a file if each collaborator changes different lines, however when two collaborators change the same line, Git throws up its hands and says, I cannot handle this responsibility, I need help from a human!\n\nWhen this happens, your human task is to find the merge conflict markers, remove them, and settle on which version of the line(s) where the conflict occurred should remain. Merge conflict markers include:\n\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD to indicate the beginning of the merge conflict\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; to indicate the end of the merge conflict\n======= between the markings mentioned above to indicate the boundary between the two proposed changes. The version of the change before the separator is your &gt; change, and the version that follows the separator was the change that existed on GitHub.\n\n\n\n\n\n\n\nExercise 1\n\n\n\nWe are going to now generate and resolve merge conflicts.I have set-up a template GitHub repository for you so that you can easily generate a merge conflict to resolve. We will do this twice, once with a simple plain text file (e.g., an R script) and once with a more complex text file (e.g., a Jupyter notebook).\nResolving merge conflicts in a simple text file:\n\nClick the green “Use this template” button from this GitHub repository to obtain a copy of it for yourself (do not fork it).\nClone this repository to your computer.\nClick on the Actions tab, and then click on the workflow .github/workflows/main.yml. You then should see a “Run workflow” button with a drop down menu, from that menu select “Run workflow” (this will trigger GitHub Actions to create a commit in your remote repository).\nLocally on your computer, fix the second line in cube.r so that it calculates the cube, not the square (e.g., change x^2 to x^3). Commit your changes to version control via Git and push your changes to GitHub.\nResolve the merge conflict them so that you can see your changes on GitHub.\n\nResolving merge conflicts in a more complex text file:\n\nClick the green “Use this template” button from this GitHub repository to obtain a copy of it for yourself (do not fork it).\nClone this repository to your computer.\nClick on the Actions tab, and then click on the workflow .github/workflows/main.yml. You then should see a “Run workflow” button with a drop down menu, from that menu select “Run workflow” (this will trigger GitHub Actions to create a commit in your remote repository).\nFix the second line in the code cell in cube.ipynb so that it calculates the cube, not the square (e.g., change x^2 to x^3). Commit your changes to version control via Git and push your changes to GitHub.\nResolve the merge conflict them so that you can see your changes on GitHub.",
    "crumbs": [
      "version-control.html",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Version control (for transparency and collaboration) I</span>"
    ]
  },
  {
    "objectID": "lectures/040-version-control-1.html#git-ignore",
    "href": "lectures/040-version-control-1.html#git-ignore",
    "title": "4  Version control (for transparency and collaboration) I",
    "section": "4.4 Git ignore",
    "text": "4.4 Git ignore\nWhat about pesky files that exist on our computer, that change sometimes but we don’t really actively use (e.g., .DS_Store, .ipynb_checkpoints, etc). We can tell Git to ignore such irrelevant files by creating and using a .gitignore file\n\n4.4.1 Create a .gitignore file\nUsing the plain text editor of your choice, create a file called .gitignore inside the root of your Git repository. Inside the text file, list the files and folders you would like to ignore, one per line. For example:\n.ipynb_checkpoints/\n.DS_Store\nSave the file, and add and commit it with Git. Next time you go to use version control, Git will not bother you about the files you listed there, even if they have changed!\n\n\n4.4.2 .gitignore tips and tricks\n\nappend **/ to the beginning of any file/folder names listed in the .gitignore file to have them ignored in subdirectories within the repo as well\ncreate a global .gitignore file so that you do not have to create the same .gitignore for all your homework repos",
    "crumbs": [
      "version-control.html",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Version control (for transparency and collaboration) I</span>"
    ]
  },
  {
    "objectID": "lectures/040-version-control-1.html#going-back-in-time",
    "href": "lectures/040-version-control-1.html#going-back-in-time",
    "title": "4  Version control (for transparency and collaboration) I",
    "section": "4.5 Going back in time!",
    "text": "4.5 Going back in time!\nNow let’s see how we can use the commits we’ve made to look at files in the past or reverting a file to a version from the past.\n\n4.5.1 Just going for a look:\nThe easiest way to go back in time in your version control repository is to use the GitHub.com website. To do this, we click the commits link:\n\nThen we can view the history, along with the commit messages. To view the state of a repository at that time, we click on the “&lt;&gt;” button:\n\n\n\n4.5.2 Travelling in time or bringing something something from the back from the past:\nSometimes you want to be able to explore and run the files from the past, or bring a past version of a file to the future. When we need to do either of those things, we should be working with Git in our local repository.\nHere’s the same history as viewed above, but using the Git command line:\ngit log\ncommit cab2e6a93f113602974390f64e88dad0a5d6eae3 (HEAD -&gt; master, origin/master, origin/HEAD)\nAuthor: Tiffany Timbers &lt;tiffany.timbers@gmail.com&gt;\nDate:   Thu Dec 9 10:33:39 2021 -0800\n\n    added example using docker-compose.yml\n\ncommit b9c1bdcdc0cf8f7bce3fd3874cce0e76b6c474e3 (tag: v4.0)\nMerge: 238dbcb 9b4af1d\nAuthor: Tiffany A. Timbers &lt;tiffany.timbers@gmail.com&gt;\nDate:   Tue Feb 4 10:49:23 2020 -0800\n\n    Merge pull request #1 from kguidonimartins/master\n\n    Update README.md\n\ncommit 238dbcb02a77acb21b251aff7ff954f8c22a91bc\nAuthor: ttimbers &lt;tiffany.timbers@stat.ubc.ca&gt;\nDate:   Tue Feb 4 10:47:03 2020 -0800\n\n    added conda to path of Dockerfile\n\ncommit 9b4af1d03c282a2d747d642a0c8847fd53b4f1d7\n:\nThe : at the end tells us there are more commits than shown. You can navigate this log by scrolling with the mouse, arrow keys, b key, or by pressing space.\nThis log is also searchable. Press / + search term + return to search for a word. n/N goes to next/previous match for the search term.\nTo exit from the Git history log, we press the q (short for “quit”) character.\nTo see what was changed at a given point in history, we can use the git show command. For example we can view what was changed in the commit with the message “added conda to path of Dockerfile” which has the long SHA 238dbcb02a77acb21b251aff7ff954f8c22a91bc we would type:\ngit show 238dbcb\n\n\n\n\n\n\nNote\n\n\n\nWe only used the first 7 digits of the long SHA (we call this the short SHA). This is usually unique enough for Git to autofill in the rest for us.\n\n\ncommit 238dbcb02a77acb21b251aff7ff954f8c22a91bc\nAuthor: ttimbers &lt;tiffany.timbers@stat.ubc.ca&gt;\nDate:   Tue Feb 4 10:47:03 2020 -0800\n\n    added conda to path of Dockerfile\n\ndiff --git a/Dockerfile b/Dockerfile\nindex 05ad853..faf3918 100644\n--- a/Dockerfile\n+++ b/Dockerfile\n@@ -20,7 +20,11 @@ RUN wget --quiet https://repo.anaconda.com/archive/Anaconda3-2019.10-Linux-x86_6\n     echo \"conda activate base\" &gt;&gt; ~/.bashrc && \\\n     find /opt/conda/ -follow -type f -name '*.a' -delete && \\\n     find /opt/conda/ -follow -type f -name '*.js.map' -delete && \\\n-    /opt/conda/bin/conda clean -afy\n+    /opt/conda/bin/conda clean -afy && \\\n+    /opt/conda/bin/conda update -n base -c defaults conda\n\n # install docopt python package\n RUN /opt/conda/bin/conda install -y -c anaconda docopt\n+\n+# put anaconda python in path\n+ENV PATH=\"/opt/conda/bin:${PATH}\"\n\n\n4.5.3 Travelling in time to the past\n\n\n\n\n\n\nActivity\n\n\n\nHere’s the website view of the same commit:\n\nSource\nHow similar are the local and webpage log views? Do you get the same information from both? Which seems easier to read/navigate? Which would be more useful for testing out your code?\n\n\nIf you wanted to move your project’s file and directories (so entire state) back to this point in time you can do that by running git checkout &lt;commit-id&gt; .. The last . is really important, so don’t forget it!\nLet’s do this now to go back to the point just before the changes we explored above:\ngit checkout 9b4af1d .\nWhen we look at the Dockerfile (the file that was changed in the commit after this) we see it does not have the changes we viewed in the 238dbcb! Also, if you run git log you will see the more recent commits in your history are GONE!\nDon’t panic! We can go back to the future! To get there (where we were before we started time travelling, we can use git checkout the HEAD commit nickname:\ngit checkout HEAD .\n\nNote: HEAD is the nickname for the most recent commit in a GitHub repository.\n\nNow, let’s check our git log to make sure we returned safely!\ngit log\ncommit cab2e6a93f113602974390f64e88dad0a5d6eae3 (HEAD -&gt; master, origin/master, origin/HEAD)\nAuthor: Tiffany Timbers &lt;tiffany.timbers@gmail.com&gt;\nDate:   Thu Dec 9 10:33:39 2021 -0800\n\n    added example using docker-compose.yml\n\ncommit b9c1bdcdc0cf8f7bce3fd3874cce0e76b6c474e3 (tag: v4.0)\nMerge: 238dbcb 9b4af1d\nAuthor: Tiffany A. Timbers &lt;tiffany.timbers@gmail.com&gt;\nDate:   Tue Feb 4 10:49:23 2020 -0800\n\n    Merge pull request #1 from kguidonimartins/master\n\n    Update README.md\n\ncommit 238dbcb02a77acb21b251aff7ff954f8c22a91bc\nAuthor: ttimbers &lt;tiffany.timbers@stat.ubc.ca&gt;\nDate:   Tue Feb 4 10:47:03 2020 -0800\n\n    added conda to path of Dockerfile\n\ncommit 9b4af1d03c282a2d747d642a0c8847fd53b4f1d7\nAuthor: Karlo Guidoni Martins &lt;kguidonimartins@gmail.com&gt;\nDate:   Tue Feb 4 13:04:39 2020 -0300\n\n    Update README.md\n\n    Remove the duplicate `### Usage:` header.\n:\n\n\n\n\n\n\nExercise 2\n\n\n\n\nClone this GitHub repository to your computer.\nView the names of the files that were changed in commit 44c17be, and the specific changes made to the file doc/count_report.Rmd.\nCheckout the 44c17be commit and explore it locally.\nThen go back to where you started (remember to use the nickname HEAD).\n\n\n\n\n\n4.5.4 Bringing something something from the back from the past\n\n\n\n\n\n\nActivity\n\n\n\nSay we decided to return to a point in the past, and restart our work from there? How would we do that?\n\n\nThere is not a nice and easy way (that I am aware of) of cherry-picking a version of single file from the past using the JupyterLab Git extension. To do this, I resort to the Git command line tool in the terminal. The general command is:\ngit checkout &lt;HASH&gt; &lt;FILE&gt;\nFor example, to checkout the version of the doc/count_report.Rmd from the commit whose hash starts with 5837143, we would type:\ngit checkout 5837143 doc/count_report.Rmd\nThat will bring that version of the doc/count_report.Rmd into our working directory. We can view it, run it or use it, et cetera. If we want to keep that version going forward for our project, we would have to then add and commit that version of the file to do so.",
    "crumbs": [
      "version-control.html",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Version control (for transparency and collaboration) I</span>"
    ]
  },
  {
    "objectID": "lectures/040-version-control-1.html#what-git-tool-to-use",
    "href": "lectures/040-version-control-1.html#what-git-tool-to-use",
    "title": "4  Version control (for transparency and collaboration) I",
    "section": "4.6 What Git tool to use?",
    "text": "4.6 What Git tool to use?\nThere are many many many different tools for using Git. We have so far discussed two in this class (the JupyterLab Git extension, and the Git command line). Others include GitHub Desktop, GitKraken, Source Tree, RStudio’s Git GUI, and VSCode’s Git GUI. Which one should you use? Anyone that fits you best! I recommend experimenting with a few and then settling in with the one that you find easiest to use. One note is that some commands are limited in some tools (e.g., the example above with the JupyterLab Git extension). If you hit that case in your favorite tool, you can use the Git command line tool to get around it and then go back to primarily using your tool of choice.",
    "crumbs": [
      "version-control.html",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Version control (for transparency and collaboration) I</span>"
    ]
  },
  {
    "objectID": "lectures/050-version-control-2.html",
    "href": "lectures/050-version-control-2.html",
    "title": "5  Version control (for transparency and collaboration) II",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "version-control.html",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Version control (for transparency and collaboration) II</span>"
    ]
  },
  {
    "objectID": "lectures/050-version-control-2.html#learning-objectives",
    "href": "lectures/050-version-control-2.html#learning-objectives",
    "title": "5  Version control (for transparency and collaboration) II",
    "section": "",
    "text": "Describe what branches are in Git and explain why they are useful for collaboration.\nExplain the two most common Git branching strategies (Git flow and GitHub flow) and list their advantages and disadvantages.\nUse Git to create new branches, switch between branches, merge branches locally, and use GitHub pull requests to bring new changes into the projects default branch.\nUse GitHub’s pull requests tool to perform code reviews.",
    "crumbs": [
      "version-control.html",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Version control (for transparency and collaboration) II</span>"
    ]
  },
  {
    "objectID": "lectures/050-version-control-2.html#git-branches",
    "href": "lectures/050-version-control-2.html#git-branches",
    "title": "5  Version control (for transparency and collaboration) II",
    "section": "5.1 Git Branches",
    "text": "5.1 Git Branches\nBranches allow you to have a playground for developing and testing new additions to your code, as well as fixes. This playground lets your explore, experiment and test in a safe place - away from others that might be using or depending on the current version of your code. This is perhaps more obviously useful if your project is deployed and has users (e.g., a data visualization dashboard, an R or Python package, a phone app, etc), but this can also be useful for code that make up a data analyses. As, in addition to the reasons stated above for branching, branching also lets you organize units of work into smaller, more manageable chunks that are easier for collaborators to check over and review.\nOnce the new code additions or fixes have been finalized in their branch playground, they can be merged back to the main code-base. Several branch playgrounds can co-exist, one for each new code addition or fix being worked on. This allows for parallelization of work!\nWe can use a construction of a house as a metaphor for this kind of development. First, you need to build the foundation. Since everything depends on the foundation being built, this would be developed on the main branch. When the foundation is finished, the construction of other parts of the house that don’t depend on each other could take place in parallel, e.g. some contributors might start working on the walls and others on the floor. When either of these features is finished, it can be added back to the house (merged into main). When depicted graphically, this process would look something like this (each dot is a commit).\n\nA more realistic Git branching example for a data analysis project might look something like this:\n\n\n5.1.1 Creating a branch using the command line\nIn the terminal, and in the working directory of the project in which you wish to create a branch, use git switch with the -c flag and specify the branch name. This create a new branch and immediately switch to it. For example, to create a branch named patch-docs (a good branch name if we were planning on working on a fix in the project’s documentation), we would type:\ngit switch -c patch-docs\nTo ask Git what branches now exist and to see where we are, we can use the branch command with the -v flag (for “verbose”, meaning tell us everything!):\ngit branch -v\nAnd we should see something like the output shown below, which lists the branch names in the first column, the SHA of the last commit on each branch in the second column, and the commit message associated with the last commit in the third column:\n  main       f756bbb initial\n* patch-docs f756bbb initial\nAs you work here on this new branch, you can commit your changes to version control locally, and even push your changes to the remote repository. All the changes however will live only on that branch until you do something to move them to another branch. When you want to start discussing your changes with your collaborators to start the process of bringing these changes into the main branch (main code-base) you typically create what is called a pull request. A pull request is a like an asking your collaborators “is it OK to merge my code?” Usually there will be some discussion and a few more revisions to the code, but eventually they will give you the thumbs up when everything looks good and the code can then be merged. We will discuss this more next.\n\nSwitching to branches that already exist\nAfter you have been working on your new brain for some time, you will eventually want to get back to the project’s default branch, or some other branch. To do this, we use git switch &lt;branch_name&gt;. Note that we do not use the -c flag this time, as we are not creating a new branch.\n\n\nGetting remote branches you did not create locally\nIf your collaborator creates a branch that you want to run or test locally, but it only exists on the remote repository you have to tell Git to fetch (go get it) for you first, before you can switch to it. You can specify which branch you specifically want to grab, but for simplicity it is often easy to ask Git to fetch all of them for you, using the fetch command with the --all flag:\ngit fetch --all\n\n\nUseful command line Git branching commands\nBelow is a table of the most commonly used Git commands for branching. This table includes the commands we presented above as well as the a few other useful ones.\n\n\n\n\n\n\n\nDescription\nCommand\n\n\n\n\nlist all local branches\ngit branch -v\n\n\ncreate a new branch & immediately switch to it\ngit switch -c &lt;branch_name&gt; or git checkout -b &lt;branch_name&gt;\n\n\nswitch to a branch\ngit switch &lt;branch_name&gt; or git checkout  &lt;branch_name&gt;\n\n\nfetches all changes to all branches\ngit fetch --all\n\n\nretrieve a branch you don’t have locally on your laptop from GitHub\ngit checkout --track origin/&lt;branch_name\n\n\nmerge changes from another branch\ngit merge &lt;branch_name&gt;\n\n\ndelete a local branch\ngit branch -d &lt;branch_name&gt;\n\n\npush changes to a remote branch\ngit push origin &lt;branch_name&gt;\n\n\n\n\n\n\n\n\n\nExercise 1\n\n\n\n\nMake your own copy of this GitHub repository by clicking the green “Use this template” button. And then clone your copy of the repository to your computer.\nCreate a new branch named better_error_msg in the local repository using Git.\nOn that branch, fix the sqrt.py so that if you run this script with a negative number as an argument you do not get an difficult to understand error, but instead throw a helpful exception informing the user that the number should be positive. Fix to add to sqrt.py:\n\nif number &lt; 0:\n  raise Exception(\"n should not a positive number\")\n\nAdd and commit those changes.\nSwitch back to the main branch and look at the sqrt.py file - do you see the change there?\nSwitch back to the better_error_msg branch - do you see the change there?\nPush your change to the remote GitHub repository, and see where you can find it there!\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote: if you use the Git command line tool and get an error message like the one shown below, this is because you did not state the remote (usually it is origin). Try pushing again using git push origin &lt;branch_name&gt;\nfatal: The current branch better_error_msg has no upstream branch.\nTo push the current branch and set the remote as upstream, use\n    git push --set-upstream origin better_error_msg\nTo have this happen automatically for branches without a tracking\nupstream, see 'push.autoSetupRemote' in 'git help config'.",
    "crumbs": [
      "version-control.html",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Version control (for transparency and collaboration) II</span>"
    ]
  },
  {
    "objectID": "lectures/050-version-control-2.html#creating-a-pull-request",
    "href": "lectures/050-version-control-2.html#creating-a-pull-request",
    "title": "5  Version control (for transparency and collaboration) II",
    "section": "5.2 Creating a pull request",
    "text": "5.2 Creating a pull request\nTo create a pull request, you first need to ensure all the local changes you made on the new branch are also available on the remote repository (i.e., you need to push them). Once that has happened you need to go to the remote GitHub repository, as pull requests are created from there.\nGitHub often tries to be helpful when a change is pushed to a new branch that doesn’t yet exist on the default branch by showing you a yellow banner listing any recently pushed branches.\n\nTo create a pull request, you click the green button “Compare & pull request”. In the new page, add a message to describe the changes you have made, scroll down to review the changed files, and the click the green button that reads “Create pull request”. If you are working together in a team, you could also designate certain team members to review your work and assign relevant labels, via the right hand side panel.\nThe next step is for a collaborator to review your work and merge it in if they approve it.\n\n\n\n\n\n\nExercise 2\n\n\n\n\nUse the green “Compare & pull request” button on the yellow banner to open a pull request.\nGo to the “Pull requests” tab on the remote GitHub repository and explore the “Conversation” and “Files changed” sub-tabs there.",
    "crumbs": [
      "version-control.html",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Version control (for transparency and collaboration) II</span>"
    ]
  },
  {
    "objectID": "lectures/050-version-control-2.html#code-reviews-using-in-line-comments-and-suggested-code-fixes",
    "href": "lectures/050-version-control-2.html#code-reviews-using-in-line-comments-and-suggested-code-fixes",
    "title": "5  Version control (for transparency and collaboration) II",
    "section": "5.3 Code reviews using in-line comments and suggested code fixes",
    "text": "5.3 Code reviews using in-line comments and suggested code fixes\n\nIn the project, you are expected to read and review each other’s code BEFORE accepting a pull request.\nDo not expect all (or even most) pull requests to be perfect in their first submission.\nWe very often need to have a conversation to get pull requests into good shape before merging into master, and GitHub has a very nice tool we can utilize to do this: GitHub code reviews\n\n\n\n\n\n\n\n\nDemo: do a code review\n\n\n\nI am going to demo a code review of a pull request. I have set-up a template GitHub repository for you so that you can easily generate a pull request for you to review if you want to try this later.\nSteps:\n\nClick the green “Use this template” button in this repository to obtain a copy of it for yourself (do not fork it).\nGo to Repository Settings &gt; Actions &gt; General &gt; under “Workflow permissions” make sure read and write permissions are enabled and also GitHub Actions is able to create and approve Pull Requests. Under “Actions permissions” select “Allow all actions and reuseable workflows”. It should look like this:\n\n\n\nClick on the Actions tab, and then click on the workflow .github/workflows/pr.yml. You then should see a “Run workflow” button with a drop down menu, from that menu select “Run workflow”.\nClick on the Pull Requests tab of your copy of the repository, click on the pull request titled “Report most accomplished pilots”, and then click on “Files Changed”. Next click on the star-wars.Rmd file. Review the file and observe the following problems with the R Markdown report that was submitted via the pull request:\n\n\nReasoning of the sentence on line 15\nIncompatibility with the sentence on line 15 with the code in the code chunk named table-of-most-accomplished-pilots\nIncorrect code in code chunk named table-of-most-accomplished-pilots (unested film instead of starships) leads to naming the wrong pilot as the most accomplished pilot on line 19\nIncorrect code in code chunk named table-of-most-accomplished-pilots (unested film instead of starships) leads to the use of the wrong character’s picture in the image that is sourced in the code chunk named top-pilot (it should be a picture of Obi-Wan Kenobi, you could use this URL for example: https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/ewan-mcgregor-obi-wan-1570898048.jpg).\n\n\nAdd comments and suggested changes using the + sign beside the line numbers (the first time you do this will trigger the start of your code review. Need help? See GitHub’s how to on reviewing proposed changes in a pull request.\nAfter you have made all the comments and suggested changes, then add a general comment for the code review, select “Request Changes” and submit your code review.\n\n\n\n\n\n\n\n\n\nDemo: Accept suggested changes from a code review\n\n\n\nSteps:\n\nTo accept the code changes that you provided as suggestions, revisit the Pull Requests tab of your copy of the repository and clicking on the pull request titled “Report most accomplished pilots”. Scroll through the pull request comments and find the code suggestions. Then click on the “Commit suggestion button” for each suggestion.\nClick on the “Show all reviewers” link beside the red “Changes requested”” text. Then click on the ... beside the reviewer and click “Approve changes”.\nFinally click on the green buttons (“Merge Pull Request” & “Confirm merge”) to merge the pull request.\n\n\n\n\n\n\n\n\n\nDiscussion\n\n\n\nIf I didn’t tell you what to review, could you really have done a good job reviewing the code only on GitHub?\nHint: if I didn’t tell you that the top pilot was Obi-Wan Kenobi, how would you have known that?\n\n\n\n5.3.1 Useful things you can do with branches on GitHub\nCreate a new branch:\n\nSource: https://github.com/UBC-DSCI/introduction-to-datascience\nView all branches and their status:\n\nSource: https://github.com/UBC-DSCI/introduction-to-datascience\nAnd delete remote branches:\n\nSource: https://github.com/UBC-DSCI/introduction-to-datascience/branches\n\n\n5.3.2 How do you not accept a pull request?\nIn some cases, it might not make sense to merge a pull request. To close a pull request that should not be merged, scroll to the bottom of the pull request page, and look for a gray “Closes pull request” button. This will end move the pull request to the closed pull requests section (similar to closed issues) and does not merge the changes.",
    "crumbs": [
      "version-control.html",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Version control (for transparency and collaboration) II</span>"
    ]
  },
  {
    "objectID": "lectures/050-version-control-2.html#git-and-github-flow",
    "href": "lectures/050-version-control-2.html#git-and-github-flow",
    "title": "5  Version control (for transparency and collaboration) II",
    "section": "5.4 Git and GitHub flow",
    "text": "5.4 Git and GitHub flow\nBranching generally works well if a team agrees to and follows a consistent workflow. The two leading workflows used in industry are Git flow and GitHub flow. We will describe and discuss both here so you are aware of both, and we will employ GitHub flow in our projects for this course.\n\n5.4.1 GitHub Flow\nKey features: - A single “main” branch that also serves as the deployment branch, we call this main. - Every new contribution is done on a new branch (with a meaningful name) created from main. - Contributions from that new branch are sent back to main by means of a pull request, which ideally is reviewed & tested before merging.\n(note: at the time this figure was made, the default branch on GitHub as still called the master branch)\n\nSource: https://blog.programster.org/git-workflows\n\n\n5.4.2 Git Flow\nKey features: - Two “main” branches that last forever, main for deployment and develop where things are tested before they are released to main. - Three supporting branches: feature, release & hotfix. Both feature & release branches are created from develop. feature branches should be merged into develop before they are incorporated into a release. release branches eventually get merged into main after adequate review and testing.\n(note: at the time this figure was made, the default branch on GitHub as still called the master branch)\n\nSource: https://blog.programster.org/git-workflows\n\n\n\n\n\n\nDiscuss\n\n\n\nWhat do you think are pros & cons of each of these branching workflows?\n\n\n\n\n\n\n\n\nRelevance to course project\n\n\n\n\nYou will be expected to work using the GitHub flow workflow for your projects in this course.\n\n\n\n\n\n5.4.3 What happens when my feature branch falls behind main?\n\nHow to catch up a branch that has fallen behind the main branch using the command line:\n\nrun git pull origin main to pull any changes from the remote version of main that we might not have\nrun git switch &lt;branch&gt; to make sure we are on the branch we want to catch up\nrun git merge main to merge the changes\nrun git push origin &lt;branch&gt; to push our updated branch to the remote",
    "crumbs": [
      "version-control.html",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Version control (for transparency and collaboration) II</span>"
    ]
  },
  {
    "objectID": "lectures/050-version-control-2.html#next",
    "href": "lectures/050-version-control-2.html#next",
    "title": "5  Version control (for transparency and collaboration) II",
    "section": "5.5 Next:",
    "text": "5.5 Next:\n\nfile and directory structure for data analysis projects\nintegrated development environments for data science",
    "crumbs": [
      "version-control.html",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Version control (for transparency and collaboration) II</span>"
    ]
  },
  {
    "objectID": "lectures/060-version-control-3.html",
    "href": "lectures/060-version-control-3.html",
    "title": "6  Project management using GitHub",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "version-control.html",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Project management using GitHub</span>"
    ]
  },
  {
    "objectID": "lectures/060-version-control-3.html#learning-objectives",
    "href": "lectures/060-version-control-3.html#learning-objectives",
    "title": "6  Project management using GitHub",
    "section": "",
    "text": "Use GitHub issues to label and assign tasks\nUse GitHub project boards to manage projects\nUse GitHub milestones to create and track project milestones and goals",
    "crumbs": [
      "version-control.html",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Project management using GitHub</span>"
    ]
  },
  {
    "objectID": "lectures/060-version-control-3.html#project-boards",
    "href": "lectures/060-version-control-3.html#project-boards",
    "title": "6  Project management using GitHub",
    "section": "6.1 Project boards",
    "text": "6.1 Project boards\nExample of a physical Kanban board:\n\nSource: https://medium.com/@mli/lets-get-physical-task-boards-f9d08383e667\nExample of a digital project board from GitHub:\nReading: About project boards - GitHub Help\n\nSource: https://github.com/CodeDoesGood/org-website/projects/1\n\n6.1.1 Why use project boards for collaborative software projects?\n\nTransparency: everyone knows what everyone is doing\nMotivation: emphasis on task completion\nFlexibility: board columns and tasks are customized to each project\n\n\n\n\n\n\n\nExercise\n\n\n\nGetting to know GitHub project boards\nWe are going to each create our own project board for our MDS homework. I have set-up a template GitHub repository for you so that you can easily populate it with relevant issues for your homework this block. You will use these issues to create your MDS homework project board.\nSteps:\n\nClick the green “Use this template” button from this GitHub repository to obtain a copy of it for yourself (do not fork it).\nClick on the Actions tab, and then click on the workflow .github/workflows/create_issues.yml. You then should see a “Run workflow” button with a drop down menu, from that menu select “Run workflow”.\nWait patiently while GitHub Actions to create 24 issues for you in your copy of this repository.\nClick on the Projects tab, and then click “Link a project” and then select “Create a new Project”. Select “Board” as the template option and give the project a a name.\nUse the issues in the repo to set-up a project board for the next two weeks (or more) of your DSCI 310 homework. For each issue you add to the project, assign it to yourself and add a label of “group-work” or “individual-work”.\n\n\n\nAdditional Resources: - Assigning issues and pull requests to other GitHub users - Applying labels to issues and pull requests\n\n\n\n\n\n\nRelevance to course project\n\n\n\n\nYou will be expected to create a project board for each of your groups projects and update it each milestone (at a minimum)\nWe expect that each issue should have at least one person assigned to it",
    "crumbs": [
      "version-control.html",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Project management using GitHub</span>"
    ]
  },
  {
    "objectID": "lectures/060-version-control-3.html#milestones",
    "href": "lectures/060-version-control-3.html#milestones",
    "title": "6  Project management using GitHub",
    "section": "6.2 Milestones",
    "text": "6.2 Milestones\n\nGroup related issues together that are needed to hit a given target (e.g., new release version of a software package)\nCan assign a due date to a milestone\nFrom the milestone page you can see list of statistics that are relevant to each milestone set in that repository\n\nReading: About milestones - GitHub Help\nExample of the readr package milestones:\n\nSource: https://github.com/tidyverse/readr/milestones\n\n\n\n\n\n\nExercise\n\n\n\nGetting to know GitHub milestones\nWe are going to practice creating milestones and associating issues with them. To do this we will continue working with the same repository that you just created a project board for.\nSteps:\n\nClick on the Issues tab, and then click on “Milestones”.\nClick “New milestone” and name it “month 1” and set the due date to be the end of January. Click “Create milestone”.\nGo to the Issues tab, and for each issue that should be associated with the month 1 milestone (i.e., things due before the end of January), click on their checkbox. Then click “Milestone” and select “month 1”\nOnce you are done, go back to the Milestones page to view what the month 1 milestone looks like.\nIf you finish early, do this for month 2.\n\n\n\n\n\n\n\n\n\nRelevance to course project\n\n\n\n\nYou will be expected to create a milestone on each of your project repositories for each course assigned milestone. You must link the relevant issues needed to complete that milestone to it on GitHub.",
    "crumbs": [
      "version-control.html",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Project management using GitHub</span>"
    ]
  },
  {
    "objectID": "lectures/070-filenames-project-organization.html",
    "href": "lectures/070-filenames-project-organization.html",
    "title": "7  Filenames and data science project organization, Integrated development environments",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Filenames and data science project organization, Integrated development environments</span>"
    ]
  },
  {
    "objectID": "lectures/070-filenames-project-organization.html#learning-objectives",
    "href": "lectures/070-filenames-project-organization.html#learning-objectives",
    "title": "7  Filenames and data science project organization, Integrated development environments",
    "section": "",
    "text": "Explain how project organization and file naming contribute to reproducible data science\nOrganize projects and name files in a sound manner\nUse an integrated development environment (IDE) to create, edit and run a script (e.g., VScode in Python, or RStudio in R)\nUse the IDE to access documentation/help, environment variables and efficiently navigate directories\nExtend and customize an IDE with useful extensions/add-ons/tools such as linters and formatters",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Filenames and data science project organization, Integrated development environments</span>"
    ]
  },
  {
    "objectID": "lectures/070-filenames-project-organization.html#filenames---best-practices",
    "href": "lectures/070-filenames-project-organization.html#filenames---best-practices",
    "title": "7  Filenames and data science project organization, Integrated development environments",
    "section": "7.1 Filenames - best practices",
    "text": "7.1 Filenames - best practices\nAttribution: much of these notes come from Jenny Bryan’s talk “Naming things” (original slides and source code)\n\n7.1.1 Names matter\n\n\n\n7.1.2 What works, what doesn’t?\nNO\nmyabstract.docx\nJoe’s Filenames Use Spaces and Punctuation.xlsx\nfigure 1.png\nfig 2.png\nJW7d^(2sl@deletethisandyourcareerisoverWx2*.txt\nYES\n2014-06-08_abstract-for-sla.docx\njoes-filenames-are-getting-better.xlsx\nfig01_talk-scatterplot-length-vs-interest.png\nfig02_talk-histogram-attendance.png\n1986-01-28_raw-data-from-challenger-o-rings.txt\n\n\n7.1.3 Three principles for (file) names\n\nMachine readable\nHuman readable\nPlays well with default ordering\n\n\nAwesome file names :)\n\n\n\n\n7.1.4 Machine readable\n\nRegular expression and globbing friendly\n\nAvoid spaces, punctuation, accented characters, case sensitivity\n\nEasy to compute on\n\nDeliberate use of delimiters\n\n\n\nGlobbing\nExcerpt of complete file listing:\n\n\nExample of globbing to narrow file listing:\n\n\n\nSame using Mac OS Finder search facilities\n\n\n\nSame using regex in R\n\n\n\nPunctuation\nDeliberate use of \"-\" and \"_\" allows recovery of meta-data from the filenames:\n\n\"_\" underscore used to delimit units of meta-data I want later\n\"-\" hyphen used to delimit words so my eyes don’t bleed\n\n\n\n\nThis happens to be R but also possible in the shell, Python, etc.\n\n\nRecap: machine readable\n\nEasy to search for files later\nEasy to narrow file lists based on names\nEasy to extract info from file names, e.g. by splitting\nNew to regular expressions and globbing? be kind to yourself and avoid\n\nSpaces in file names\nPunctuation\nAccented characters\nDifferent files named foo and Foo\n\n\n\n\n\n7.1.5 Human readable\n\nHuman readable\n\nName contains info on content\nConnects to concept of a slug from semantic URLs\n\n\n\nExample\nWhich set of file(name)s do you want at 3 a.m. before a deadline?\n\n\n\nEmbrace the slug\n\n\nslug filenames \n\nslug \n\n\n\n\nRecap: Human readable\nEasy to figure out what the heck something is, based on its name\n\n\n\n7.1.6 Plays well with default ordering\n\nPlays well with default ordering\n\nPut something numeric first\nUse the ISO 8601 standard for dates\nLeft pad other numbers with zeros\n\n\n\nExamples\nChronological order:\n\n\n\nchronological_order)(\n\n\n\nLogical order: Put something numeric first\n\n\n\nDates\nUse the ISO 8601 standard for dates: YYYY-MM-DD\n\n\n\n\nComprehensive map of all countries in the world that use the MM-DD-YYYY format\n\n\nFrom https://twitter.com/donohoe/status/597876118688026624\n\n\nLeft pad other numbers with zeros\n\n\nIf you don’t left pad, you get this:\n10_final-figs-for-publication.R\n1_data-cleaning.R\n2_fit-model.R\nwhich is just sad :(\n\n\nRecap: Plays well with default ordering\n\nPut something numeric first\nUse the ISO 8601 standard for dates\nLeft pad other numbers with zeros\n\n\n\nRecap\n\n\nThree principles for (file) names\n\nMachine readable\nHuman readable\nPlays well with default ordering\n\n\n\nPros\n\nEasy to implement NOW\nPayoffs accumulate as your skills evolve and projects get more complex\n\n\n\nGo forth and use awesome file names :)",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Filenames and data science project organization, Integrated development environments</span>"
    ]
  },
  {
    "objectID": "lectures/070-filenames-project-organization.html#project-organization",
    "href": "lectures/070-filenames-project-organization.html#project-organization",
    "title": "7  Filenames and data science project organization, Integrated development environments",
    "section": "7.2 Project organization",
    "text": "7.2 Project organization\nA good project structure looks similar to this:\nproject/\n├── data/              *.csv\n│   ├── processed/\n│   └── raw/\n├── reports/           *.ipynb *.Rmd\n├── src/               *.py *.R\n├── doc/               *.md\n├── README.md\n└── environment.yaml (or renv.lock)\nThis can differ slightly between projects and for R the src directory is often just called R/, whereas for Python is has the same name as the project (project). You will learn more about project hierarchy when making packages.\n\n\n7.2.1 A tour de data science project organization\nBelow we link to three different data science projects shared via GitHub repositories. The first is a data analysis project, while the latter two are data science tools (an R and a Python package, respectively).\nAs you explore these projects, note the following similarities: - Files related generally to the project are found in the project root (e.g., README, CODE_OF_CONDUCT.md, computational environment files, etc) - Code files are generally found in the src or R directory - data houses raw and processed data - doc houses documentation files and or documents related to the project (sometimes there is both a doc and reports directory when there are two kinds of writing in a project). - Other directories can be added as needed to group files with like/similar functions. Most important is that they have descriptive and obvious names related to the directory contents.\n\nExample projects\n\na data analysis project\na R package\na Python package",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Filenames and data science project organization, Integrated development environments</span>"
    ]
  },
  {
    "objectID": "lectures/070-filenames-project-organization.html#integrated-development-environments",
    "href": "lectures/070-filenames-project-organization.html#integrated-development-environments",
    "title": "7  Filenames and data science project organization, Integrated development environments",
    "section": "7.3 Integrated development environments",
    "text": "7.3 Integrated development environments\nIntegrated development environments (IDEs) are software that provide comprehensive tools for programming in one place.\nThe classic Jupyter notebook interface is typically not considered an IDE, however the newer, JupyterLab interface is.\n\n\nOther very popular IDEs for data science include RStudio and VSCode. We will now explore these and look for similarities to identify key IDE features important for developing code for data science.\n\n\n\n\n\n\n\nNote\n\n\n\nRStudio was originally developed to work with the R programming language, but now it can work with Python well too! If you would like to use RStudio with Python, please follow the instructions here on how to configure this: https://ttimbers.github.io/intro-to-reticulate/setup-instructions/setup-after-installing-python.html\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you would like to use VS Code with Python, then please follow the instructions below on how to configure this:\n\nInstall the VSCode Python Extension in your VSCode application\nCode can be sent line-by-line to the Python console via Shift + Enter. The first time you do this, an interactive iPython console will open up. Future Shift + Enter keystrokes during that work session will send code to that console.\nA bash command line terminal console can be added by clicking the Terminal &gt; New Terminal menu item, and then the “+” symbol and selecting “bash”.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you would like to use VS Code with R, please follow the instructions below on how to configure this:\n\nInstall the VSCode R Extension in your VSCode application.\nIn the RStudio R console, install languageserver R package via install.packages(\"languageserver\").\nInstall the radian Python package (which gives an improved R console in VS Code) via running conda install radian in the terminal.\nIn the VS Code settings (found by clicking the Code &gt; Preferences &gt; Settings menu item) search for “r.path” and paste the path to radian under the textbox for your operating system (you can find the path to radian by typing which radian in the terminal). For example, on my Mac x86 laptop, the path is /Users/tiffany/opt/miniconda3/bin/radian.\nIn the VS Code settings (Code &gt; Settings &gt; Settings) search for “R: Bracketed Paste” and select the checkbox for this setting (this allows you to send code to the R console when it is split across lines without it breaking).\nIn the VS Code settings (Code &gt; Settings &gt; Settings) search for “Rterm: Mac” and set the path to radian (you can find this by typing which radian in the terminal).\nAn interactive R console can be added by clicking the Terminal &gt; New Terminal menu item, and then the “+” symbol and selecting “R Terminal”. Code can be sent line-by-line to the R console via Cmd + Enter (Mac) or Control + Enter (Windows).\n\n\n\n\nKey features of all of these IDEs includes:\n\nscreen split into panes\nfile browser\nconvenient access to R/Python console and a terminal\neditor for code writing\ncode autocompletion\ncode/syntax highlighting",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Filenames and data science project organization, Integrated development environments</span>"
    ]
  },
  {
    "objectID": "lectures/080-conda-lock.html",
    "href": "lectures/080-conda-lock.html",
    "title": "8  Conda lock: reproducible lock files for conda environments",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Conda lock: reproducible lock files for conda environments</span>"
    ]
  },
  {
    "objectID": "lectures/080-conda-lock.html#learning-objectives",
    "href": "lectures/080-conda-lock.html#learning-objectives",
    "title": "8  Conda lock: reproducible lock files for conda environments",
    "section": "",
    "text": "Describe what a conda lock file is and how it differs from a conda environment.yml file\nCreate a conda lock file from a conda environment for any of the major operating systems (Windows, linux, Mac) and processor chips (intel vs arm)\nCreate a conda environment from a lock file",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Conda lock: reproducible lock files for conda environments</span>"
    ]
  },
  {
    "objectID": "lectures/080-conda-lock.html#what-is-conda-lock",
    "href": "lectures/080-conda-lock.html#what-is-conda-lock",
    "title": "8  Conda lock: reproducible lock files for conda environments",
    "section": "8.1 What is conda lock?",
    "text": "8.1 What is conda lock?\nConda lock is a tool that is able to resolve all the dependencies for an environment and save them into a fully reproducible lock file for conda environments. It’s similar to an environment.yml file in that both files will specify packages in an environment, but the conda lock file specifically tells the installer everything specified in the file is exactly what the platform needs to install the environment.",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Conda lock: reproducible lock files for conda environments</span>"
    ]
  },
  {
    "objectID": "lectures/080-conda-lock.html#downsides-of-environment.yml",
    "href": "lectures/080-conda-lock.html#downsides-of-environment.yml",
    "title": "8  Conda lock: reproducible lock files for conda environments",
    "section": "8.2 Downsides of environment.yml",
    "text": "8.2 Downsides of environment.yml\nFirst let’s review the environment.yml and point out some of its problems. Conda’s environment.yml file is one way to create a reproducible conda environment.\nThis file can be created with the conda env export command.\nconda env export &gt; environment.yml\n\n\n\n\n\n\nNote\n\n\n\nYou can read more on managing conda environments in the conda documentation: &lt;https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html\n\n\nOne of the issues with this method is that it will capture all the installed packages and dependencies in the current conda environment into an environment.yml. That sounds great! Until you realize that it also captures any needed OS-specific dependencies, and also writes a prefix line in the file. Both of these prevent a fully reproducible file across multiple machines and collaborators.\nWe can partially solve this issue by using the --from-history flag.\nconda env export --from-history\nThis will only include packages that were explicitly installed, without the dependencies. If you manually go remove the prefix line, this is a good way to create an environment.yml file that is also compatible across platforms.\nFinally, even if you manually curate an environment.yml, you still have to go through the conda (or mamba) solver to make sure all listed package versions are compatible with each other, and install any missing dependencies. The main issue is, the solver still takes time.",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Conda lock: reproducible lock files for conda environments</span>"
    ]
  },
  {
    "objectID": "lectures/080-conda-lock.html#what-conda-lock-aims-to-solve",
    "href": "lectures/080-conda-lock.html#what-conda-lock-aims-to-solve",
    "title": "8  Conda lock: reproducible lock files for conda environments",
    "section": "8.3 What conda lock aims to solve",
    "text": "8.3 What conda lock aims to solve\nWhen we use conda install to install a package, the solver will look up the version of the package you are installing and make sure any currently installed package version are compatible with the package. In this way using conda env export will report the same result of a reproducible environment.\nBut, when we ran conda install, we have already spent the time for the solver to do its work. The main problem conda lock solves is specifying an environment file similar to environment.yml, but it also signals to the installer that the solver has already been run and does not need to run again. This creates an environment file that can also be used to speed up subsequent environment setups.\nWhen we try to install from an environment.yml file, there’s no guarantee that the packages listed include the versions full dependency tree. Conda lock does provide that guarantee that all the packages listed in the file will have a version and full dependency tree. That’s why the solver does not need to run again.\nFinally, you can tell conda lock to do the solving for other platforms. So the solver can be run for operating systems other than the one you are working in. This is useful when you are collaborating with people using different platforms and also creating environment files (i.e., lock files) for docker containers.\n\n\n\n\n\n\nConda Lock\n\n\n\nA conda-lock file is a file that specifies all the packages, its versions, and the full dependency tree to recreate a conda environment. It’s main benefit is that when using the conda-lock file, the solver no longer needs to be run, and can be created to target specific operating system platforms.\n\n\nYou can learn more about conda-lock in the documentation: https://conda.github.io/conda-lock/",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Conda lock: reproducible lock files for conda environments</span>"
    ]
  },
  {
    "objectID": "lectures/080-conda-lock.html#create-conda-lock-files",
    "href": "lectures/080-conda-lock.html#create-conda-lock-files",
    "title": "8  Conda lock: reproducible lock files for conda environments",
    "section": "8.4 Create conda lock files",
    "text": "8.4 Create conda lock files\nConda lock is a separate tool that needs to be installed first, it’s not installed by default in the base conda environment. Conda-lock also relies on a source file, we’ll be using environment.yml.\nconda install -c conda-forge conda-lock\n\n\n\n\n\n\nNote\n\n\n\nYou may want to install conda-lock into a new environment, especially if you want to follow along and do the exercises in this chapter.\nconda create -n condalock conda-lock\nSince you are using a conda install, this will take some time for the solver to resolve. Note, we are just installing conda-lock in the environment, we are not installing python yet.\nDon’t forget to activate your environment\nconda activate condalock\n\n\nWhen creating conda-lock files there are 2 different types of files that can be created, conda-lock.yml or conda-&lt;platform&gt;.lock file. The conda-lock.yml is the newer unified multi-platform lockfile that is the default generated from conda-lock &gt; 1.0. The conda-&lt;platform&gt;.lock is the older platform specific lock files you can create, e.g., conda-linux-64.lock.\n\n\n\n\n\n\nExercise 1\n\n\n\nLet’s create our first conda-lock.yml file\n\nStep 1: Environment Setup\nLet’s populate our current environment by installing pandas, and create an environment.yml file.\nconda install pandas\nWe’ll take a note here on how long this took to install. It may take about 14 seconds depending on your system and computer specs.\nLet’s save our current environment into a file\nconda env export --from-history &gt; environment.yml\nYou should see something like this\nname: condalock\nchannels:\n  - conda-forge\ndependencies:\n  - conda-lock\n  - pandas\nprefix: /home/dan/.pyenv/versions/miniforge3-latest/envs/condalock\nYou’ll notice that python is not listed in the environment.yml file. That’s because we only explicitly installed pandas, python is installed as part of the dependency resolution.\nYou’ll see python in the environment if you search for it in the environment.\n$ conda env export | grep python\n  - brotli-python=1.1.0=py313h46c70d0_2\n  - gitpython=3.1.43=pyhd8ed1ab_0\n  - msgpack-python=1.1.0=py313h33d0bda_0\n  - python=3.13.0=h9ebbce0_100_cp313\n  - python-dateutil=2.9.0=pyhd8ed1ab_0\n  - python-tzdata=2024.2=pyhd8ed1ab_0\n  - python_abi=3.13=5_cp313\n\n\nStep 2: Create a conda-lock.yml file\nWe will now use the environment.yml file to create our conda-lock.yml file.\nconda-lock lock --file environment.yml\nYou’ll see that it will create a multi-platform lock file. We can look into only creating platform specific lock files next.\n$ conda-lock lock --file environment.yml\nLocking dependencies for ['linux-64', 'osx-64', 'osx-arm64', 'win-64']...\nINFO:conda_lock.conda_solver:linux-64 using specs ['conda-lock', 'pandas']\nINFO:conda_lock.conda_solver:osx-64 using specs ['conda-lock', 'pandas']\nINFO:conda_lock.conda_solver:osx-arm64 using specs ['conda-lock', 'pandas']\nINFO:conda_lock.conda_solver:win-64 using specs ['conda-lock', 'pandas']\n - Install lock using: conda-lock install --name YOURENV conda-lock.yml\nYou will now see a conda-lock.yml file for all the platforms.\nThis particular file (on this specific machine) has almost 5000 lines\n$ wc -l conda-lock.yml\n4944 conda-lock.yml\n\n\n\n\n\n\nTip\n\n\n\nIf you run into an error, try manually deleting any exiting conda-lock.yml file in your directory and try again.\n\n\n\n\n\nYou can learn more about conda-lock flags (i.e., options) in the documentation: https://conda.github.io/conda-lock/flags/\nThe CLI Reference will list all the commands for conda-lock: https://conda.github.io/conda-lock/cli/gen/",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Conda lock: reproducible lock files for conda environments</span>"
    ]
  },
  {
    "objectID": "lectures/080-conda-lock.html#specifying-the-platform",
    "href": "lectures/080-conda-lock.html#specifying-the-platform",
    "title": "8  Conda lock: reproducible lock files for conda environments",
    "section": "8.5 Specifying the platform",
    "text": "8.5 Specifying the platform\nBy default you’ll see conda-lock solve for multiple dependencies: ['linux-64', 'osx-64', 'osx-arm64', 'win-64']. Depending on the size of your environment, this may take a long time, especially if you only need to target a single platform (or a subset of platforms). we can use the -p or --platform for the lock command.\n\n\n\n\n\n\nExercise 2\n\n\n\nWe can generate a subset of platforms with the -p or --platform flag. If you are doing this example after the previous example, remember to delete the existing conda-lock.yml file.\n\n\n\n\n\n\nCaution\n\n\n\nBe careful, the -p flag does not mean the same for all the conda-lock commands.\n\n\nconda-lock lock --file environment.yml -p linux-64\nOur new files has much fewer lines, because it contains packages for fewer platforms.\n$ wc -l conda-lock.yml\n1403 conda-lock.yml",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Conda lock: reproducible lock files for conda environments</span>"
    ]
  },
  {
    "objectID": "lectures/080-conda-lock.html#create-environments-from-conda-lock-files",
    "href": "lectures/080-conda-lock.html#create-environments-from-conda-lock-files",
    "title": "8  Conda lock: reproducible lock files for conda environments",
    "section": "8.6 Create environments from conda lock files",
    "text": "8.6 Create environments from conda lock files\nNow that we have a conda-lock.yml file, we can use it to create a new environment with the install command\n\n\n\n\n\n\nExercise 3\n\n\n\nLet’s use our conda-lock.yml file to create a new environment called condalock-new.\nconda-lock install --name condalock-new conda-lock.yml\nWhen we timed this install, the new environment was created in 6 seconds. This much faster than using conda install and relying on the solver.\nWe can now activate this environment with conda activate condalock-new",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Conda lock: reproducible lock files for conda environments</span>"
    ]
  },
  {
    "objectID": "lectures/080-conda-lock.html#conclusion",
    "href": "lectures/080-conda-lock.html#conclusion",
    "title": "8  Conda lock: reproducible lock files for conda environments",
    "section": "8.7 Conclusion",
    "text": "8.7 Conclusion\nThere are many ways of creating a reproducible compute environment. conda comes with a mechanism to create an environment.yml file, but there are a few downsides with using environment.yml with conda, mainly sharing environments causes everyone to go through the same solving process. conda-lock uses the environment.yml file to create a different kind of environment file, conda-lock.yml, and provides a way where only one person needs to compute the dependency solver, everyone else who uses this new file to create or update an environment will have a much faster environment setup.",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Conda lock: reproducible lock files for conda environments</span>"
    ]
  },
  {
    "objectID": "lectures/090-virtual-environments.html",
    "href": "lectures/090-virtual-environments.html",
    "title": "9  Virtual environments",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Virtual environments</span>"
    ]
  },
  {
    "objectID": "lectures/090-virtual-environments.html#learning-objectives",
    "href": "lectures/090-virtual-environments.html#learning-objectives",
    "title": "9  Virtual environments",
    "section": "",
    "text": "Explain what a virtual environment is and why it can be useful for reproducible data analyses\nDiscuss the advantages and limitations of virtual environment tools (e.g., conda and renv) in the context of reproducible data analyses\nUse, create and share virtual environments (for example, with conda and renv)",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Virtual environments</span>"
    ]
  },
  {
    "objectID": "lectures/090-virtual-environments.html#attribution",
    "href": "lectures/090-virtual-environments.html#attribution",
    "title": "9  Virtual environments",
    "section": "Attribution",
    "text": "Attribution\nThe conda virtual environment section of this guide was originally published at http://geohackweek.github.io/ under a CC-BY license and has been updated to reflect recent changes in conda, as well as modified slightly to fit the MDS lecture format.",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Virtual environments</span>"
    ]
  },
  {
    "objectID": "lectures/090-virtual-environments.html#virtual-environments",
    "href": "lectures/090-virtual-environments.html#virtual-environments",
    "title": "9  Virtual environments",
    "section": "9.1 Virtual environments",
    "text": "9.1 Virtual environments\nVirtual environments let’s you have multiple versions of programming languages packages, and other programs on the same computer, while keeping them isolated so they do not create conflicts with each other. In practice virtual environments are used in one or multiple projects. And you might have several virtual environments stored on your laptop, so that you can have a different collection of versions programming languages and their packages for each project, as needed.\nMost virtual environment tools have a sharing functionality which aids in making data science projects reproducible, as not only is there a record of the computational environment, but that computational environment can be shared to others computer - facilitating the reproduction of results from data and code. This facilitation comes from the fact that programming languages and their packages are not static - they change! There are new features added, bugs are fixed, etc, and this can impact how your code runs! Therefore, for a data science project to be reproducible across time, you need the computational environment in addition to the data and the code.\nThere are several other major benefits of using environments:\n\nIf two of your projects on your computer rely on different versions of the same package, you can install these in different environments.\nIf you want to play around with a new package, you don’t have to change the packages versions you use for your data analysis project and risk messing something up (package version often get upgraded when we install other new packages that share dependencies).\nWhen you develop your own packages, it is essential to use environments, since you want to to make sure you know exactly which packages yours depend on, so that it runs on other systems than your own.\n\nThere are MANY version virtual environment tools out there, even if we just focus on R and Python. When we do that we can generate this list:\n\nR virtual environment tools\n\npackrat\nrenv\nconda\n\n\n\nPython virtual environment tools\n\nvenv\nvirtualenv\nconda\nmamba\npoetry\npipenv\n… there may be more that I have missed.\n\nIn this course, we will learn about conda and renv. conda is nice because it can work with both R and Python. Although a downside of conda is that it is not as widely adopted in the R community as Python is, and therefore there are less R packages available from it, and less recent versions of those R packages than available directly from the R package index (CRAN). It is true, that you can create a conda package for any R package that exists on CRAN, however this takes time and effort and is sometimes non-trivial.\nGiven that, we will also learn about renv - a new virtual environment tool in R that is gaining widespread adoption. It works directly with the packages on CRAN, and therefore allows users to crete R virtual environments with the most up to date packages, and all R packages on CRAN, with less work compared to conda.\n\n\n\n\n\n\nNote\n\n\n\nNote on terminology: Technically what we are discussing here in this topic are referred to as virtual environments. However, in practice we often drop the “virtual” when discussing this and refer to these as simply “environments”. That may happen in these lecture notes, as well as in the classroom.",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Virtual environments</span>"
    ]
  },
  {
    "objectID": "lectures/090-virtual-environments.html#conda",
    "href": "lectures/090-virtual-environments.html#conda",
    "title": "9  Virtual environments",
    "section": "9.2 Conda",
    "text": "9.2 Conda\nconda is an open source package and environment management system for any programming language; though it is most popular in the Python community. conda was originally developed by Anaconda Inc. and bundled with their Anaconda distribution of Python. However, conda’s widespread popularity and utility led to its decoupling into its own package.\nIt is now available for installation via: - Anaconda Python distribution - Miniconda Python distribution (this is what we recommended most of you install for this course) - Miniforge (this is what we recommended folks with Mac ARM machines install for this course)\nConda builds of R and Python packages, are in fact R and Python packages and built from R and Python source code, but they are packaged up and built differently, and with a different tool chain. How to create conda packages from R and Python source code is beyond the scope of this course. However, we direct keen learners of this topic to the documentation on how to do this: - Conda-build documentation\nWhat we will focus on learning is how to use conda to create virtual environments, record the components of the virtual environment and share the virtual environment with collaborators in a way that they can recreate it on their computer.\n\n9.2.1 Managing Conda\nLet’s first start by checking if conda is installed (it should be if we followed the recommended course computer setup instructions) by running:\nconda --version\nTo see which conda commands are available, type conda --help. To see the full documentation for any command of these commands, type the command followed by --help. For example, to learn about the conda update command:\nconda update --help\nLet’s update our conda to the latest version. Note that you might already have the latest version since we downloaded it recently.\nconda update conda\nYou will see some information about what there is to update and be asked if you want to confirm. The default choice is indicated with [], and you can press Enter to accept it. It would look similar to this:\nUsing Anaconda Cloud api site https://api.anaconda.org\nFetching package metadata: ....\n.Solving package specifications: .........\n\nPackage plan for installation in environment //anaconda:\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    conda-env-2.6.0            |                0          601 B\n    ruamel_yaml-0.11.14        |           py27_0         184 KB\n    conda-4.2.12               |           py27_0         376 KB\n    ------------------------------------------------------------\n                                           Total:         560 KB\n\nThe following NEW packages will be INSTALLED:\n\n    ruamel_yaml: 0.11.14-py27_0\n\nThe following packages will be UPDATED:\n\n    conda:       4.0.7-py27_0 --&gt; 4.2.12-py27_0\n    conda-env:   2.4.5-py27_0 --&gt; 2.6.0-0\n    python:      2.7.11-0     --&gt; 2.7.12-1\n    sqlite:      3.9.2-0      --&gt; 3.13.0-0\n\nProceed ([y]/n)? y\n\nFetching packages ...\nconda-env-2.6. 100% |################################| Time: 0:00:00 360.78 kB/s\nruamel_yaml-0. 100% |################################| Time: 0:00:00   5.53 MB/s\nconda-4.2.12-p 100% |################################| Time: 0:00:00   5.84 MB/s\nExtracting packages ...\n[      COMPLETE      ]|###################################################| 100%\nUnlinking packages ...\n[      COMPLETE      ]|###################################################| 100%\nLinking packages ...\n[      COMPLETE      ]|###################################################| 100%\nIn this case, conda itself needed to be updated, and along with this update some dependencies also needed to be updated. There is also a NEW package that was INSTALLED in order to update conda. You don’t need to worry about remembering to update conda, it will let you know if it is out of date when you are installing new packages.\n\n\n9.2.2 Managing conda environments\n\nWhat is a conda environment and why is it so useful?\nUsing conda, you can create an isolated R or Python virtual environment for your project. The default environment is the base environment, which contains only the essential packages from Miniconda (and anything else you have installed in it since installing Miniconda). You can see that your shell’s prompt string is prefaced with (base) when you are inside this environment:\n(base) Helps-MacBook-Pro:~ tiffany$\nIn the computer setup guide, we asked you to follow instructions so that this environment will be activated by default every time you open your terminal.\nTo create another environment on your computer, that is isolated from the (base) environment you can either do this through:\n\nManual specifications of packages.\nAn environment file in YAML format (environment.yml).\n\nWe will now discuss both, as they are both relevant workflows for data science. When do you use one versus the other? I typically use the manual specifications of packages when I am creating a new data science project. From that I generate an environment file in YAML format that I can share with collaborators (or anyone else who wants to reproduce my work). Thus, I use an environment file in YAML format when I join a project as a collaborator and I need to use the same environment that has been previously used for that project, or when I want to reproduce someone else’s work.\n\n\n\n9.2.3 Creating environment by manually specifying packages\nWe can create test_env conda environment by typing conda -n &lt;name-of-env&gt;. However, it is often useful to specify more than just the name of the environment, e.g. the channel from which to install packages, the Python version, and a list of packages to install into the new env. In the example below, I am creating the test_env environment that uses python 3.12 and a list of libraries: jupyterlab and pandas.\nconda create -n test_env -c conda-forge python=3.12 jupyterlab pandas=2.2.3\nconda will solve any dependencies between the packages like before and create a new environment with those packages. Usually, we don’t need to specify the channel, but in this case I want to get the very latest version of these packages, and they are made available in conda-forge before they reach the default conda channel.\nTo activate this new environment, you can type conda activate test_env (and conda deactivate for deactivating). Since you will do this often, we created an alias shortcut ca that you can use to activate environments. To know the current environment that you’re in you can look at the prefix of the prompt string in your shell which now changed to (test_env). And to see all your environments, you can type conda env list.\n\n\n9.2.4 Seeing what packages are available in an environment\nWe will now check packages that are available to us. The command below will list all the packages in an environment, in this case test_env. The list will include versions of each package, the specific build, and the channel that the package was downloaded from. conda list is also useful to ensure that you have installed the packages that you desire.\nconda list\n# packages in environment at //miniconda/envs/test_env:\n#\nUsing Anaconda Cloud api site https://api.anaconda.org\nblas                      1.1                    openblas    conda-forge\nca-certificates           2016.9.26                     0    conda-forge\ncertifi                   2016.9.26                py27_0    conda-forge\ncycler                    0.10.0                   py27_0    conda-forge\nfreetype                  2.6.3                         1    conda-forge\nfunctools32               3.2.3.2                  py27_1    conda-forge\nlibgfortran               3.0.0                         0    conda-forge\n\n\n9.2.5 Installing conda package\nUnder the name column of the result in the terminal or the package column in the Anaconda Cloud listing, shows the necessary information to install the package. e.g. conda-forge/rasterio. The first word list the channel that this package is from and the second part shows the name of the package.\nTo install the latest version available within the channel, do not specify in the install command. We will install version 0.35 of rasterio from conda-forge into test_env in this example. Conda will also automatically install the dependencies for this package.\nconda install -c conda-forge rasterio=0.35\nIf you have a few trusted channels that you prefer to use, you can pre-configure these so that everytime you are creating an environment, you won’t need to explicitly declare the channel.\nconda config --add channels conda-forge\n\n\n9.2.6 Removing a conda package\nWe decided that rasterio is not needed in this tutorial, so we will remove it from test_env. Note that this will remove the main package rasterio and its dependencies (unless a dependency was installed explicitly at an earlier point in time or is required be another package).\nconda remove -n test_env rasterio\nUsing Anaconda Cloud api site https://api.anaconda.org\nFetching package metadata .........\nSolving package specifications: ..........\n\nPackage plan for package removal in environment //anaconda/envs/test_env:\n\nThe following packages will be REMOVED:\n\n    rasterio: 0.35.1-np111py27_1 conda-forge\n\nProceed ([y]/n)? y\n\nUnlinking packages ...\n[      COMPLETE      ]|#######################################################################################################| 100%\n\n\n9.2.7 Sharing Environments with others\nTo share an environment, you can export your conda environment to an environment file, which will list each package and its version in the format package=version=build.\nExporting your environment to a file called environment.yaml (it could be called anything, but this is the conventional name and using it makes it easy for others to recognize that this is a conda env file, the extension can be either .yaml or .yml):\nconda env export --from-history -f environment.yml\nRemember that .yaml files are plain text, so you can use a text editor such as VS Code to open them. If you do, you will realize that this environment file has A LOT more packages than jupyterlab and pandas. This is because the default behavior is to also list the dependencies that were installed together with these packages, e.g. numpy. This is good in the sense that it gives an exact copy of everything in your environment.\nWe use the --from-history flag/option above as some dependencies might differ between operating systems, so this file might not work with someone from a different OS. The --from-history flag, looks at the history of the packages you explicitly told conda to install and only list those in the export. The required dependencies will then be handled in an OS-specific manner during the installation, which guarantees that they will work across OSes. This environment.yaml file would be much shorter and look something like this:\nname: test_env\nchannels:\n  - conda-forge\n  - defaults\ndependencies:\n  - conda\n  - python=3.7\n  - pandas==1.0.2\n  - jupyterlab\nImportantly, this will not include the package version unless you included it when you installed with the package==version syntax. For an environment to be reproducible, you NEED to add the version string manually.\n\n\n9.2.8 Creating environment from an environment file\nNow, let’s install environment.yml environment file above so that we can create a conda environment called test_env.\n$ conda env create --file environment.yml\n\n\n\n\n\n\nExercise\n\n\n\nCreate an environment on your laptop with an older version of Python!\n\nClone this GitHub repository.\nTry to run some antiquated (Python 3.0.0 and higher compatible) Python code, such as python -c \"print 'Back from the Future'\". This should fail.\nIn the terminal, navigate to the root of the repository and run: conda env create --file environment.yml (note: If you have a M1/M2 Mac, please run this command instead: CONDA_SUBDIR=osx-64 conda env create –file environment.yml).\nActivate the environment by typing conda activate oldie_but_a_goodie\nTry again to run some antiquated (Python 3.0.0 and higher compatible) Python code, such as python -c \"print 'Back from the Future'\". If you activated your environment correctly, this should succeed!\n\n\n\n\n\n9.2.9 Copying an environment\nWe can make an exact copy of an environment to an environment with a different name. This maybe useful for any testing versus live environments or different Python 3.7 versions for the same packages. In this example, test_env is cloned to create live_env.\nconda create --name live_env --clone test_env\n\n\n9.2.10 Listing all environments on your laptop\nYou may have created an environment and forgotten what you named it, or you want to do some cleanup and delete old environments (next topic), and so you want to see which exist on your computer and remove the ones you are no longer using. To do this we will use the info command along with the --envs flag/option.\nconda info --envs\n\n\n\n\n\n\nNote\n\n\n\nListing all the conda environments on your laptop with this command also shows you where conda stores these environments. Typically conda environments are stored in /Users/&lt;USERNAME&gt;/opt/miniconda3/envs. This means that conda environments are typically in your terminal’s path, resulting in the environments being accessible from any directory on your computer, regardless of where they were created. However, despite this flexibility, commonly one environment is created per project, and the environment.yml file that is used for sharing the conda environment is stored in the project root.\n\n\n\n\n9.2.11 Deleting an environment\nSince we are only testing out our environment, we will delete live_env to remove some clutter. Make sure that you are not currently using live_env.\nconda env remove -n live_env\n\n\n9.2.12 Making environments work well with JupyterLab\nIn brief, you need to install a kernel in the new conda environment in any new environment your create (ipykernel for Python and the r-irkernel package for R), and the nb_conda_kernels package needs to be installed in the environment where JupyterLab is installed.\nBy default, JupyterLab only sees the conda environment where it is installed. Since it is quite annoying to install JupyterLab and its extensions separately in each environment, there is a package called nb_conda_kernels that makes it possible to have a single installation of JupyterLab access kernels in other conda environments. This package needs to be installed in the conda environment where JupyterLab is installed. For the computer setup for this course, we did that in the base environment, so that is where you would need to install nb_conda_kernels to make this work.\nMore details are available in the nb_conda_kernels README).\nRemember that when you forget a specific command you can type in the help command we have created mds-help in you terminal to see a list of all commands we use in MDS.",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Virtual environments</span>"
    ]
  },
  {
    "objectID": "lectures/090-virtual-environments.html#renv",
    "href": "lectures/090-virtual-environments.html#renv",
    "title": "9  Virtual environments",
    "section": "9.3 renv",
    "text": "9.3 renv\nIn R, environments can also be managed by renv, which works with similar principles as conda, and other virtual environment managers, but the commands are different. Detailed documentation for renv, can found at the package website.\nrenv differs from conda in the way that it adds package dependencies. Briefly, when you prompt renv to create (or update) a file to record the project dependencies (done via renv’s snapshot() function), it recursively crawls the files in the project looking for calls to library() or require().\nThe key file renv creates for recording and sharing environments is called renv.lock in the project’s root directory. Other files are created in the project’s root directory when you use renv but renv.lock is the file that documents which programming languages and packages (including versions) are used in the project. It is recommended that when sharing an renv environment that you version control renv.lock, .Rprofile and renv/activate.R to facilitate collaboration. When you setup an renv environment with renv::init() it creates a renv/.gitignore file so that files that renv creates and uses locally but are not helpful to share, are not shared.\nrenv environments work best in the context of RStudio projects - and so it is recommended that you create an RStudio project that corresponds to the root of your data science project repository. If this is not done - renv will crawl files outside of the project, looking for dependencies.\nBelow we create a table with the general virtual environment commands for renv as well as the equivalent conda command for comparison:\n\n\n\n\n\n\n\n\nDescription\nrenv command\nconda command\n\n\n\n\nCreate a new environment without an environment file\nrenv::init()\nconda create -n &lt;ENV_NAME&gt; ...\n\n\nActivate a new environment\nrenv::activate()\nconda activate &lt;ENV_NAME&gt;\n\n\nExport environment to a file\nrenv::snapshot()\nconda env export --from-history -f environment.yml\n\n\nCreate a new environment from an environment file\nrenv::restore()\nconda env create --file environment.yml",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Virtual environments</span>"
    ]
  },
  {
    "objectID": "lectures/100-containerization-1.html",
    "href": "lectures/100-containerization-1.html",
    "title": "10  Introduction to containerization",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Introduction to containerization</span>"
    ]
  },
  {
    "objectID": "lectures/100-containerization-1.html#learning-objectives",
    "href": "lectures/100-containerization-1.html#learning-objectives",
    "title": "10  Introduction to containerization",
    "section": "",
    "text": "Explain what containers are, and why they can be useful for reproducible data analyses\nDiscuss the advantages and limitations of containerization (e.g., Docker) in the context of reproducible data analyses\nCompare and contrast the difference between running software/scripts in a virtual environment, a virtual machine and a container\nEvaluate, choose and justify an appropriate environment management solution based on the data analysis project’s complexity, expected usage and longevity.",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Introduction to containerization</span>"
    ]
  },
  {
    "objectID": "lectures/100-containerization-1.html#documenting-and-loading-dependencies",
    "href": "lectures/100-containerization-1.html#documenting-and-loading-dependencies",
    "title": "10  Introduction to containerization",
    "section": "10.1 Documenting and loading dependencies",
    "text": "10.1 Documenting and loading dependencies\nYou’ve made some beautiful data analysis pipeline/project using make, R, and/or Python. It runs on your machine, but how easily can you, or someone else, get it working on theirs? The answer usually is, it depends…\nWhat does it depend on?\n\nDoes your README and your scripts make it blatantly obvious what programming languages and packages need to run your data analysis pipeline/project?\nDo you also document the version numbers of the programming languages and packages you used? This can have big consequences when it comes to reproducibility… (e.g.,the change to random number generation in R in 2019?)\nDid you document what other software (beyond the the programming languages and packages used) and operating system dependencies are needed to run your data analysis pipeline/project?\n\nVirtual environments can be tremendously helpful with #1 & #2, however, they may or may not be helpful to manage #3… Enter containerization as a possible solution!",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Introduction to containerization</span>"
    ]
  },
  {
    "objectID": "lectures/100-containerization-1.html#what-is-a-container",
    "href": "lectures/100-containerization-1.html#what-is-a-container",
    "title": "10  Introduction to containerization",
    "section": "10.2 What is a container?",
    "text": "10.2 What is a container?\nContainers are another way to generate (and share!) isolated computational environments. They differ from virtual environments (which we discussed previously) in that they are even more isolated from the computers operating system, as well as they can be used share many other types of software, applications and operating system dependencies.\nBefore we can fully define containers, however, we need to define virtualization. Virtualization is a process that allows us to divide the the elements of a single computer into several virtual elements. These elements can include computer hardware platforms, storage devices, and computer network resources, and even operating system user spaces (e.g., graphical tools, utilities, and programming languages).\nContainers virtualize operating system user spaces so that they can isolate the processes they contain, as well as control the processes’ access to computer resources (e.g., CPUs, memory and disk space). What this means in practice, is that an operating system user space can be carved up into multiple containers running the same, or different processes, in isolation. Below we show the schematic of a container whose virtual user space contains the: - R programming language, the Bioconductor package manager, and two Bioconductor packages - Galaxy workflow software and two toolboxes that can be used with it - Python programming language, iPython interpreter and Jupyter notebook package\n\nSchematic of a container for genomics research. Source: https://doi.org/10.1186/s13742-016-0135-4\n\n\n\n\n\n\nExercise 1\n\n\n\nRunning a simple container\nTo further illustrate what a container looks like, and feels like, we can use Docker (containerization software) to run one and explore. First we will run an linux (debian-flavoured) container that has R installed. To run this type:\ndocker run --rm -it rocker/r-ver:4.3.2\nWhen you successfully launch the container, R should have started. Check the version of R - is it the same as your computer’s version of R? Use getwd() and list.files() to explore the containers filesystem from R. Does this look like your computer’s filesystem or something else?\nType q() to quit R and exit the container.\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\nRunning a container with RStudio as a web app\nNext, try to use Docker to run a container that contains the RStudio server web-application installed:\ndocker run --rm -p 8787:8787 -e PASSWORD=\"apassword\" rocker/rstudio:4.3.2\nThen visit a web browser on your computer and type: http://localhost:8787\nIf it worked, then you should be at an RStudio Sign In page. To sign in, use the following credentials:\n\nusername: rstudio\npassword: apassword\n\nThe RStudio server web app being run by the container should look something like this:\n\nType Cntrl + C in the terminal where you launched the container to quit R and RStudio and exit the container.\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\nRunning a container with Jupyter as a web app\nNext, try to use Docker to run a container that contains the Jupyter web-application installed:\ndocker run --rm -p 8888:8888  jupyter/minimal-notebook:notebook-7.0.6\nIn the terminal, look for a URL that starts with http://127.0.0.1:8888/lab?token= (for an example, see the highlighted text in the terminal below). Copy and paste that URL into your browser.\n\nThe Jupyter web-application being run by the container should look something like this:\n\nType Ctrl + c in the terminal where you launched the container to quit Jupyter and exit the container.",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Introduction to containerization</span>"
    ]
  },
  {
    "objectID": "lectures/100-containerization-1.html#contrasting-containers-with-virtual-machines",
    "href": "lectures/100-containerization-1.html#contrasting-containers-with-virtual-machines",
    "title": "10  Introduction to containerization",
    "section": "10.3 Contrasting containers with virtual machines",
    "text": "10.3 Contrasting containers with virtual machines\nVirtual machines are another technology that can be used to generate (and share) isolated computational environments. Virtual machines emulate the functionality an entire computer on a another physical computer. With virtual machine the virtualization occurs at the layer of software that sits between the computer’s hardware and the operating system(s). This software is called a hypervisor. For example, on a Mac laptop, you could install a program called Oracle Virtual Box to run a virtual machine whose operating system was Windows 10, as the screen shot below shows:\n\nA screenshot of a Mac OS computer running a Windows virtual machine. Source: https://www.virtualbox.org/wiki/Screenshots\nBelow, we share an illustration that compares where virtualization happens in containers compared to virtual machines. This difference, leads to containers being more light-weight and portable compared to virtual machines, and also less isolated.\n\nSource: https://www.docker.com/resources/what-container\nKey take home: - Containerization software shares the host’s operating system, whereas virtual machines have a completely separate, additional operating system. This can make containers lighter (smaller in terms of size) and more resource and time-efficient than using a virtual machine.*",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Introduction to containerization</span>"
    ]
  },
  {
    "objectID": "lectures/100-containerization-1.html#contrasting-common-computational-environment-virtualization-strategies",
    "href": "lectures/100-containerization-1.html#contrasting-common-computational-environment-virtualization-strategies",
    "title": "10  Introduction to containerization",
    "section": "10.4 Contrasting common computational environment virtualization strategies",
    "text": "10.4 Contrasting common computational environment virtualization strategies\n\n\n\n\n\n\n\n\n\nFeature\nVirtual environment\nContainer\nVirtual machine\n\n\n\n\nVirtualization level\nApplication\nOperating system user-space\nHardware\n\n\nIsolation\nProgramming languages, packages\nProgramming languages, packages, other software, operating system dependencies, filesystems, networks\nProgramming languages, packages, other software, operating system dependencies, filesystems, networks, operating systems\n\n\nSize\nExtremely light-weight\nlight-weight\nheavy-weight",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Introduction to containerization</span>"
    ]
  },
  {
    "objectID": "lectures/100-containerization-1.html#virtualization-strategy-advantages-and-disadvantages-for-reproducibility",
    "href": "lectures/100-containerization-1.html#virtualization-strategy-advantages-and-disadvantages-for-reproducibility",
    "title": "10  Introduction to containerization",
    "section": "10.5 Virtualization strategy advantages and disadvantages for reproducibility",
    "text": "10.5 Virtualization strategy advantages and disadvantages for reproducibility\nLet’s collaboratively generate a list of advantages and disadvantages of each virtualization strategy in the context of reproducibility:\n\n10.5.1 Virtual environment\n\nAdvantages\n\nExtremely small size\nPorous (less isolated) - makes it easy to pair the virtualized computational environment with files on your computer\nSpecify these with a single text file\n\n\n\nDisadvantages\n\nNot always possible to capture and share operating system dependencies, and other software your analysis depends upon\nComputational environment is not fully isolated, and so silent missed dependencies\n\n\n\n\n10.5.2 Containers\n\nAdvantages\n\nSomewhat light-weight in size (manageable for easy sharing - there are tools and software to facilitate this)\nPossible to capture and share operating system dependencies, and other software your analysis depends upon\nComputational environment is fully isolated, and errors will occur if dependencies are missing\nSpecify these with a single text file\nCan share volumes and ports (advantage compared to virtual machines)\n\n\n\nDisadvantages\n\nPossible security issues - running software on your computer that you may allow to be less isolated (i.e., mount volumes, expose ports)\nTakes some effort to share volumes and ports (disadvantage compared to virtual environments)\n\n\n\n\n10.5.3 Virtual machine\n\nAdvantages\n\nHigh security, because these are much more isolated (filesystem, ports, etc)\nCan share an entirely different operating system (might not so useful in the context of reproducibility however…)\n\n\n\nDisadvantages\n\nVery big in size, which can make it prohibitive to share them\nTakes great effort to share volumes and ports - which makes it hard to give access to data on your computer",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Introduction to containerization</span>"
    ]
  },
  {
    "objectID": "lectures/100-containerization-1.html#container-useage-workflow",
    "href": "lectures/100-containerization-1.html#container-useage-workflow",
    "title": "10  Introduction to containerization",
    "section": "10.6 Container useage workflow",
    "text": "10.6 Container useage workflow\nBelow is a schematic of typical container useage workflow from a blog post by Arnaud Mazin. The source code for container images is stored in a file called a Dockerfile. Similar to an environment.yml file, you would typically version control this with Git and store this in your GitHub repository with your analysis code. From the source Dockerfile we can build a Docker container image. This image is a binary file, from which we can run one, or more, container instances. Container instances are the computational environment in which you would run your analysis. Containers are usually ephemeral - we create them from the image when we want to run the analysis and then stop and remove/delete them after it has run. Docker images can (and should be) versioned and shared via container registries (which are akin to remote code repositories).\n\nSource: OctoTalks",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Introduction to containerization</span>"
    ]
  },
  {
    "objectID": "lectures/100-containerization-1.html#image-vs-container",
    "href": "lectures/100-containerization-1.html#image-vs-container",
    "title": "10  Introduction to containerization",
    "section": "10.7 Image vs container?",
    "text": "10.7 Image vs container?\nAnalogy: The program Chrome is like a Docker image, whereas a Chrome window is like a Docker container.\n\nYou can list the container images on your computer that you pulled using Docker via: docker images. You should see a list like this when you do this:\n$ docker images\nREPOSITORY                 TAG                    IMAGE ID       CREATED        SIZE\nrocker/rstudio             4.3.2                  bc76e0dbd6db   9 days ago     1.87GB\nrocker/r-ver               4.3.2                  c9569cbc2eb0   9 days ago     744MB\ncontinuumio/miniconda3     23.9.0-0               55e8b7e3206b   3 weeks ago    457MB\njupyter/minimal-notebook   notebook-7.0.6         e04c3bedc133   3 weeks ago    1.45GB\nhello-world                latest                 b038788ddb22   6 months ago   9.14kB\nYou can list the states of containers that have been started by Docker on your computer (and not yet removed) via: docker ps -a:\nCONTAINER ID   IMAGE                  COMMAND   CREATED          STATUS          PORTS                                       NAMES\n9160100c7d4b   rocker/r-ver:4.3.2     \"R\"       5 seconds ago    Up 4 seconds                                                friendly_merkle\n0d0871c90313   rocker/rstudio:4.3.2   \"/init\"   33 minutes ago   Up 33 minutes   0.0.0.0:8787-&gt;8787/tcp, :::8787-&gt;8787/tcp   exciting_kepler",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Introduction to containerization</span>"
    ]
  },
  {
    "objectID": "lectures/100-containerization-1.html#what-is-a-container-registry",
    "href": "lectures/100-containerization-1.html#what-is-a-container-registry",
    "title": "10  Introduction to containerization",
    "section": "10.8 What is a container registry",
    "text": "10.8 What is a container registry\nA container registry is a remote repository, or collection of repositories, used to share container images. This is similar to remote version control repositories for sharing code. Instead of code however, it is container images that are pushed and pulled to/from there. For this course we will focus on the widely-used DockerHub container registry: https://hub.docker.com/.\nHowever, there are many container registries that can be used, including: - https://github.com/ (yes! GitHub now also hosts container images in addition to code!) - https://quay.io/ - https://aws.amazon.com/ecr/ (yes! Amazon now also hosts container images too!)\n\n\n\n\n\n\nExercise 4\n\n\n\nExploring container registries\nLet’s visit the repositories for the two container images that we used in the exercise earlier in class:\n\nrocker/rstudio\njupyter/minimal-notebook\n\nQuestion: how did we get the images for the exercise earlier in class? We were just prompted to type docker run...\nAnswer: docker run ... will first look for images you have locally, and run those if they exist. If they do not exist, it then attempts to pull the image from DockerHub.",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Introduction to containerization</span>"
    ]
  },
  {
    "objectID": "lectures/100-containerization-1.html#how-do-we-specify-a-container-image",
    "href": "lectures/100-containerization-1.html#how-do-we-specify-a-container-image",
    "title": "10  Introduction to containerization",
    "section": "10.9 How do we specify a container image?",
    "text": "10.9 How do we specify a container image?\nContainer images are specified from plain text files! In the case of the Docker containerization software, we call these Dockerfiles. We will explain these in more detail later, however for now it is useful to look at one to get a general idea of their structure:\nExample Dockerfile:\nFROM continuumio/miniconda3\n\n# Install Git, the nano-tiny text editor and less (needed for R help)\nRUN apt-get update && \\\n    apt-get install --yes \\\n    git \\\n    nano-tiny \\\n    less\n\n# Install Jupyter, JupterLab, R & the IRkernel\nRUN conda install -y --quiet \\\n    jupyter \\\n    jupyterlab=3.* \\\n    r-base=4.1.* \\\n    r-irkernel\n\n# Install JupyterLab Git Extension\nRUN pip install jupyterlab-git\n\n# Create working directory for mounting volumes\nRUN mkdir -p /opt/notebooks\n\n# Make port 8888 available for JupyterLab\nEXPOSE 8888\n\n# Copy JupyterLab start-up script into container\nCOPY start-notebook.sh /usr/local/bin/\n\n# Change permission of startup script and execute it\nRUN chmod +x /usr/local/bin/start-notebook.sh\nENTRYPOINT [\"/usr/local/bin/start-notebook.sh\"]\n\n# Switch to staring in directory where volumes will be mounted\nWORKDIR \"/opt/notebooks\"\nThe commands in all capitals are Docker commands. Dockerfiles typically start with a FROM command that specifies which base image the new image should be built off. Docker images are built in layers - this helps make them more light-weight. The FROM command is usually followed by RUN commands that usually install new software, or execute configuration commands. Other commands in this example copy in needed configuration files, expose ports, specify the working directory, and specify programs to execute at start-up.\n\n\n\n\n\n\nExercise 5\n\n\n\nDemonstration of container images being built from layers\nLet’s take a look at the Dockerfile for the jupyter/docker-stacks r-notebook container image: - Dockerfile\nQuestions: 1. What image does it build off? 2. What image does that build off? 3. And then the next one?",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Introduction to containerization</span>"
    ]
  },
  {
    "objectID": "lectures/110-containerization-2.html",
    "href": "lectures/110-containerization-2.html",
    "title": "11  Using and running containers",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Using and running containers</span>"
    ]
  },
  {
    "objectID": "lectures/110-containerization-2.html#learning-objectives",
    "href": "lectures/110-containerization-2.html#learning-objectives",
    "title": "11  Using and running containers",
    "section": "",
    "text": "Use the containerization software (e.g., Docker command line and Docker compose) to run the software needed for your analysis\nUse port mapping with the containerization software to run web apps to access IDEs (e.g., Jupyter, RStudio, VSCode)\nUse volume mounting with the containerization software to allow the container access to local host files (e.g., analysis code and data files)",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Using and running containers</span>"
    ]
  },
  {
    "objectID": "lectures/110-containerization-2.html#launching-containers-using-docker-at-the-command-line",
    "href": "lectures/110-containerization-2.html#launching-containers-using-docker-at-the-command-line",
    "title": "11  Using and running containers",
    "section": "11.1 Launching containers using Docker at the command line",
    "text": "11.1 Launching containers using Docker at the command line\nDocker has many ways we can run containers (GUI, command line, Docker compose configuration files). Here we will learn how to use the command line interface for running container instances from container images. We will then build on this knowledge to move to using Docker compose configuration files for increased efficiency. We are opting to use command line and configuaration file tools so that we can automate and scale the running of containers. It is also more reproducible.\nBelow we demonstrate how to launch and run containers using the continuumio/miniconda3 image as an example. We will slowly walk through the 4 basic steps need to run a container:\n\nStep 1 - launch the Docker app (for OSX & Windows only)\nOn Mac and Windows, Docker runs in a virtual machine (called Docker Desktop). To run a Docker container, you must have this running. To do this, use launchpad/Finder/Start menu/etc to find and launch Docker Desktop.\n\nNote: Docker might already be running, if so great, but if its not, the commands below will not work. So it is always good to check!\n\n\n\nStep 2 - get container image from Docker Hub\nTo run a container instance, you need to have a container image locally. One way to get a container image locally is to pull (i.e., download) one from a container registry like Docker Hub. To do this we use the docker pull &lt;CONTAINER&gt; command. You will commonly see the container to pull written as: container_registry/container_user/container_image:container_image_version_tag.\nIf you only see container_user/container_image:container_image_version_tag Docker will assume you are using Docker Hub as the container registry (this is the default). Sometimes the container_image_version_tag is ommitted, when this happens the most recent container is pulled, and has the version tag latest. In data analysis projects we try to avoid using this strategy because Docker is lazy and if you already have the latest version on your local host machine, it will not update it when you re-run docker pull container_user/container_image even if there is a newer version of the container image on the remote container registry.\nWe will now pull the miniconda3 docker image, container image verion 23.9.0-0, made and shared by continuumio, and hosted on Docker Hub by runnig the following in the terminal:\ndocker pull continuumio/miniconda3:23.9.0-0\nWe can verify that it successfully pulled by typing: docker images. When you do that, you should see something like:\nREPOSITORY                 TAG       IMAGE ID       CREATED         SIZE\ncontinuumio/miniconda3     23.9.0-0  55e8b7e3206b   3 weeks ago    457MB\n\n\n\n\n\n\nNote\n\n\n\n\nYou can skip this step and just got onto docker run ... as that command will pull the image if you do not have it locally.\nIf you ever need to delete a container image from your computer, you can run docker rmi &lt;IMAGE_ID&gt; to do so.\n\n\n\n\n\nStep 3 - launch a container from the image and poke around!\nWe use docker run to launch a container instance from a container image. By default the container instance will execute it’s programmed launch behaviour and immediately stop. Thus, if we want to use the container in a more interactive way, we need to tell Docker that by adding the -it flag to our docker run command.\ndocker run -it continuumio/miniconda3:23.9.0-0`\nIf it worked, then your command line prompt should now look something like this:\nroot@5d8e6fff1b12:/#\nYou can standard bash shell commands like ls, cd, pwd to explore the container. You can type exit to leave when you are done (your prompt will look normal again)!\n\n\nStep 4 - clean up your container!\nAfter you close a container it still “hangs” around… It has stopped running but the container instance still exists in memory (reminder - you can view any existing containers using docker ps -a). You can remove the container by typing docker rm &lt;container_id&gt;. After you remove the container you can prove to yourself that the container is no longer “hanging around” via docker ps -a, but that you still have the image installed (which you can see via docker images).\n\nNote: to remove running containers, you will need to first stop them via docker stop &lt;container_id&gt;\n\n\n\nThat’s a lot of work…\nWe walked through those steps slowly to that you understand how everything is working. It is not the most efficient way to work with Docker container instances however when using docker run. What we typically do is tell Docker to delete the container upon exit using the --rm flag in the run command.\nThe command below runs a new container instance from the same image. This one command is equivalent to steps 2-4 above!\ndocker run \\\n    --rm \\\n    --it \\\n    continuumio/miniconda3:23.9.0-0\n\n\n\n\n\n\nNote\n\n\n\nwe are using \\ above to split a bash command across lines to make it more readable. You will see that throughout this chapter.\n\n\n\n\n11.1.1 Mounting volumes to containers\nOften times we want to use the software made available to us in containers on files on our computers. To do this, we need to explicitly tell Docker to mount a volume to the container. We can do this via: -v &lt;path_to_computer_directory&gt;:&lt;absolute_path_to_container_directory&gt;\nOften, we want to mount the volume from our current directory (where we are working) and we can do that with a short-form of /$(pwd) in place of the path to our computer’s directory.\nTo mount our current directory to a container from the continuumio/miniconda3 image we type the following on your laptop:\ndocker run \\\n    --rm \\\n    -it \\\n    -v /$(pwd):/home/my_mounted_volume \\\n    continuumio/miniconda3\nNavigate to the directory where you mounted your files via: cd /home/my_mounted_volume and type ls to ensure you can see them.\n\n\n\n\n\n\nNote\n\n\n\nif you are mounting volumes to a container from a Docker image that runs a web app, be sure to read the documentation to see where you should mount that volume. Usually the web apps are only exposed to certain directories and you will only be able to access the files in the mounted volume if you mount them to the correct place. For example, in the rocker/rstudio image that we loaded earlier, volumes need to be mounted within /home/rstudio/ to be able to access them via the RStudio server web app.\n\n\n\nWindows notes for mounting volumes:\n\nWindows machines need to explicitly share drives with Docker - this should be part of your computer setup!\nOn Windows, the laptop path depends what shell you are using, here are some details:\n\nIf you are going to run it in Windows terminal, then the command to share the current directory should be:\n\ndocker run --rm -it -v /$(pwd):&lt;PATH_ON_CONTAINER&gt; &lt;IMAGE_NAME&gt;\n\nIf you are going to run it in Power Shell, then the command should be:\n\ndocker run --rm -it -v &lt;ABSOLUTE_PATH_ON_YOUR_COMPUTER&gt;:&lt;PATH_ON_CONTAINER&gt; &lt;IMAGE_NAME&gt;\n(pwd and variants do not seem to work). And the path must be formatted like: C:\\Users\\tiffany.timbers\\Documents\\project\\:/home/project\n\n\n\n\n11.1.2 Mapping ports to containers with web apps\nDocker documentation on Container networking\nIf we want to use a graphical user interface (GUI) with our containers, for example to be able to use the computational environment in the container in an integrated development environment (IDE) such as RStudio or JupyterLab, then we need to map the correct port from the container to a port on our computer.\n\n\n\n\n\n\nNote\n\n\n\nIn computer science, ports are points where network connections start and end. They can be physical (e.g., USB ports, Ethernet ports, etc) or virtual. In the case of virtual ports, they are really a software-based addressing mechanism that identifies points to connect specific processes or types of network services. When we are discussing ports in the context of containerization, we are referring to virtual ports.\n\n\nTo do this, we use the -p flag with docker run, specifying the port in the host (your computer) on the left-hand side, and the port on the container/Docker host on the right-hand side of :. For example, to run the rocker/rstudio container image we would type -p 8787:8787 to map the ports as shown in the docker run command below:\ndocker run \\\n    --rm \\\n    -p 8787:8787 \\\n    -e PASSWORD=\"apassword\" \\\n    rocker/rstudio:4.3.2\nThen to access the web app, we need to navigate a browser url to http://localhost:&lt;CONTAINER_PORT&gt;. In this case we would navigate to http://localhost:8787 to use the RStudio server web app from the container.\nNote that we can only map one port on our computer (the container/Docker host) to a container at any given time. However, our computer (the container/Docker host) has many ports we can choose from to map. So if we wanted to run a second rocker/rstudio container, then we could map it to a different port as shown below:\ndocker run \\\n    --rm \\\n    -p 8788:8787 \\\n    -e PASSWORD=\"apassword\" \\\n    rocker/rstudio:4.3.2\nWhen we do this, to run the app in a browser on our computer, we need to go to http://localhost:8788 (instead of http://localhost:8787) to access this container as we mapped it to the 8788 port on our computer (and not 8787).\nAnother important note is that the container port is specific to the container, and the web app installed therein. So we cannot change that without changing the container image, and/or application installed therein. Where do you learn what port is exposed in a container image? The image documentation should specify this. For example, in the rocker/rstudio container image documentation it states:\n\nSource: https://hub.docker.com/r/rocker/rstudio\n\n\n11.1.3 Specifying the image architecture/platform\nNewer M1 and M2 Macs use a new processor chip, called ARM, that is a different architecture compared to the previous Macs, and current Windows and Linux machines (which use Intel Processors). Given that containerization software virtualizes at the level of the operating system user space, these different architectures lead to building containers with different architectures.\nAlso given that Newer M1 and M2 Macs are still the minority of computers in use, it is a better practice to work with container architectures that work for the majority of in use computers, which are those that have Intel Processors. To tell Docker to do this, we add the --platform=linux/amd64 argument to our Docker run and build commands.\nTo make this process even smoother and less error prone, we should also set our Docker Desktop to use Rosetta 2 x86/AMD64 emulation on M1/M2 Macs . To use this, you must: - make sure Rosetta 2 is installed on your Mac (instructions to install it here) - Select “Use Virtualization framework” and “Use Rosetta for x86/amd64 emulation on Apple Silicon” in the General settings tab of Docker Desktop.\n\n\n\n\n\n\nNote\n\n\n\n\nIn computer science, emulation works to let you run run software and execute programs originally designed one computer system on another computer system. Emulation is similar to virtualization in concept, but differs from it in that it focuses on enabling software designed for entirely different architectures to be executed.\nYou must also be using macOS Ventura or later to use this feature.\nYou will still need to use the --platform linux/amd64 command when building or running images even when using Rosetta 2 emulation, because your computer can run and build both linux/arm64 and linux/amd64 images. So you have to be clear which architecture you want to work with.\n\n\n\n\n\n11.1.4 Changing the containers default command\nWhen we launch containers, they execute the command at runtime that was specified in their Dockerfile. For example, in the case of the Rocker and Juypter images, this default behaviour is to run a web app. Sometimes the default container behaviour is not what we want to do. Instead we want to do something different, like run the container as a Bash shell to test out some installation commands (common when we are building and customizing our own containers) or using the container to run a script non-interactively (like executing a data analysis pipeline from beginning to end).\nTo do this, we can append a new command to the end of our docker run commands. For example to open the rocker/rstudio:4.3.2 image in an interactive bash shell, we would run:\ndocker run \\\n    --rm \\\n    -it \\\n    rocker/rstudio:4.3.2 \\\n    bash\nNotice the command above does not specify the ports, nor does it set the password. This is because in this instance, we are not using this container to run it as a web app, we are just running it as a bash shell (which needs no ports, nor authentication).\nThe general form for for running things non-interactively is this:\ndocker run \\\n    --rm \\\n    -v PATH_ON_YOUR_COMPUTER:VOLUME_ON_CONTAINER DOCKER_IMAGE PROGRAM_TO_RUN \\\n    PROGRAM_ARGUMENTS\nWhat of instead running the container interactively, we wanted to run a script? Let’s take this R script, named snowman.R, shown below, which uses the cowsay::say function to print some asci art with a cute message!\n# snowman.R\n\nlibrary(cowsay)\n\nsay(\"Snow again this week?\", \"snowman\")\nAssuming that script is in our current working directory, we can mount volumes and then run the script as follows:\n$ docker run \\\n    --rm \\\n    -v /$(pwd):/home/rstudio \\\n    ttimbers/dockerfile-practice:v0.1.0 \\\n    Rscript /home/rstudio/snowman.R\nWhich should result in:\n-----\nSnow again this week?\n ------\n    \\\n     \\\n     _[_]_\n      (\")\n  &gt;--( : )--&lt;\n    (__:__) [nosig]\nNow that was a silly example, but this can be made powerful so that we can run an analysis pipeline, such as a Makefile non-interactively using Docker!\nLet’s do this exercise to demonstrate:\n\nClone this GitHub repository: https://github.com/ttimbers/breast_cancer_predictor_py\nNavigate into the root of the breast_cancer_predictor_py project on your computer using the command line and enter the following command to reset the project to a clean state (i.e., remove all files generated by previous runs of the analysis):\n\ndocker run \\\n    --rm \\\n    -v .:/home/jovyan \\\n    ttimbers/breast_cancer_predictor_py:d285fc9 \\\n    make clean\n\nTo run the analysis in its entirety, enter the following command in the terminal in the project root:\n\ndocker run \\\n    --rm \\\n    -v .:/home/jovyan \\\n    ttimbers/breast_cancer_predictor_py:d285fc9 \\\n    make all\nNote: If you are on a M1/M2 Mac, don’t forget to include --platform=linux/amd64 in your run command.",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Using and running containers</span>"
    ]
  },
  {
    "objectID": "lectures/110-containerization-2.html#docker-command-line-commands",
    "href": "lectures/110-containerization-2.html#docker-command-line-commands",
    "title": "11  Using and running containers",
    "section": "11.2 Docker command line commands",
    "text": "11.2 Docker command line commands\nThe table below summarizes the Docker commands we have learned so far and can serve as a useful reference when we are using Docker:\n\n11.2.1 Docker commands\n\n\n\n\n\n\n\ncommand/flag\nWhat it does\n\n\n\n\npull\nDownloads a Docker image from Docker Hub\n\n\nimages\nTells you what container images are installed on your machine\n\n\nrmi\nDeletes a specified container image from your machine\n\n\nps -a\nTells you what containers are running on your machine\n\n\nstop\nStops a specified running container\n\n\nrm\nRemoves a specified stopped container\n\n\nrun\nLaunches a container from an image\n\n\nexit\nExits a Docker container\n\n\n\n\n\n11.2.2 Docker run commands\n\n\n\n\n\n\n\ncommand/flag\nWhat it does\n\n\n\n\n-it\nTells Docker to run the container interactively\n\n\n--rm\nMakes a container ephemeral (deletes it upon exit)\n\n\n-v\nMounts a volume of your computer to the Docker container\n\n\n-p\nSpecifies the ports to map a web app to\n\n\n-e\nSets environment variables in the container (e.g., PASSWORD=“apassword”)\n\n\n--platform\nSpecifies the image architecture, commonly used on M1/M2 Macs to set it to linux/amd64",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Using and running containers</span>"
    ]
  },
  {
    "objectID": "lectures/110-containerization-2.html#docker-compose-to-launch-containers",
    "href": "lectures/110-containerization-2.html#docker-compose-to-launch-containers",
    "title": "11  Using and running containers",
    "section": "11.3 Docker compose to launch containers",
    "text": "11.3 Docker compose to launch containers\nIt can be fiddly and error prone to type long commands into the terminal, or a GUI every time you want to launch a container. A better approach is to use Docker compose to specify how you want to launch the container.\nDocker compose uses a YAML file, specifically named docker-compose.yml, to record how the container should be launched. This file can include details including: - the docker image and version to use - how to mount volumes - what ports to map - what environment variables to set.\nHere is an example of a docker-compose.yml file for use with the rocker/rstudio container image:\nservices:\n  analysis-env:\n    image: rocker/rstudio:4.3.2\n    ports:\n      - \"8787:8787\"\n    volumes:\n      - .:/home/rstudio/project\n    environment:\n      PASSWORD: password\nTo launch the container interactively using this file, you would type the docker-compose command shown below.\ndocker-compose up\nIf you are using a web app, as in the case of the rocker/rstudio or jupyter/minimal-notebook container images, you still need to manually navigate to the web app in your browser and enter the correct URL to access it.\nTo stop and clean up the container, you would type Cntrl + C in the terminal where you launched the container, and then type\ndocker-compose rm\nLet’s take a look at an example docker-compose.yml being used in a project: - https://github.com/ttimbers/breast_cancer_predictor_py",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Using and running containers</span>"
    ]
  },
  {
    "objectID": "lectures/110-containerization-2.html#running-a-docker-container-non-interactively-using-the-docker-compose",
    "href": "lectures/110-containerization-2.html#running-a-docker-container-non-interactively-using-the-docker-compose",
    "title": "11  Using and running containers",
    "section": "11.4 Running a Docker container non-interactively using the Docker Compose",
    "text": "11.4 Running a Docker container non-interactively using the Docker Compose\nWe can also use Docker Compose to run containers non-interactively! We can do this by specifying that we want to run the container (instead of up to launch in interactively). We use the --rm flag with the run command to make the container ephemeral (delete it upon exit). Then we specify the name of the service from the docker-compose.yml file that we want to run (in our docker-compose.yml files so far we only have one service, the environment for running the analysis). And finally we add the command we want to run non-interactively using the container (in the example below we use make to run the data analysis pipeline script).\ndocker-compose run --rm analysis-env make all",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Using and running containers</span>"
    ]
  },
  {
    "objectID": "lectures/120-containerization-3.html",
    "href": "lectures/120-containerization-3.html",
    "title": "12  Customizing and building containers",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Customizing and building containers</span>"
    ]
  },
  {
    "objectID": "lectures/120-containerization-3.html#learning-objectives",
    "href": "lectures/120-containerization-3.html#learning-objectives",
    "title": "12  Customizing and building containers",
    "section": "",
    "text": "Write a container file (e.g., Dockerfile) that can be used to reproducibly build a container image that would contain the needed software and environment dependencies of your Data Science project\nUse manual and automated tools (e.g., Docker, GitHub Actions) to build and share container images\nList good container base images for Data Science projects",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Customizing and building containers</span>"
    ]
  },
  {
    "objectID": "lectures/120-containerization-3.html#building-container-images-from-dockerfiles",
    "href": "lectures/120-containerization-3.html#building-container-images-from-dockerfiles",
    "title": "12  Customizing and building containers",
    "section": "12.1 Building container images from Dockerfile’s",
    "text": "12.1 Building container images from Dockerfile’s\n\nA Dockerfile is a plain text file that contains commands primarily about what software to install in the Docker image. This is the more trusted and transparent way to build Docker images.\nOnce we have created a Dockerfile we can build it into a Docker image.\nDocker images are built in layers, and as such, Dockerfiles always start by specifiying a base Docker image that the new image is to be built on top off.\nDocker containers are all Linux containers and thus use Linux commands to install software, however there are different flavours of Linux (e.g., Ubuntu, Debian, CentOs, RedHat, etc) and thus you need to use the right Linux install commands to match your flavour of container. For this course we will focus on Ubuntu- or Debian-based images and thus use apt-get as our installation program.\n\n\n12.1.1 Workflow for building a Dockerfile\n\nChoose a base image to build off (from https://hub.docker.com/).\nCreate a Dockerfile named Dockerfile and save it in an appropriate project repository. Open that file and type FROM &lt;BASE_IMAGE&gt; on the first line.\nIn a terminal, type docker run --rm -it &lt;IMAGE_NAME&gt; and interactively try the install commands you think will work. Edit and try again until the install command works.\nWrite working install commands in the Dockerfile, preceeding them with RUN and save the Dockerfile.\nAfter adding every 2-3 commands to your Dockerfile, try building the Docker image via docker build --tag &lt;TEMP_IMAGE_NAME&gt; &lt;PATH_TO_DOCKERFILE_DIRECTORY&gt;.\nOnce the entire Dockerfile works from beginning to end on your laptop, then you can finally move to building remotely (e.g., creating a trusted build on GitHub Actions).\n\n\n\n12.1.2 Demo workflow for creating a Dockfile locally\nWe will demo this workflow together to build a Docker image locally on our machines that has R and the cowsay R package installed.\nLet’s start with the debian:stable image, so the first line of our Dockerfile should be as such:\nFROM debian:stable\nNow let’s run the debian:stable image so we can work on our install commands to find some that work!\n$ docker run --rm -it debian:stable\nNow that we are in a container instance of the debian:stable Docker image, we can start playing around with installing things. To install things in the Debian flavour of Linux we use the command apt-get. We will do some demo’s in class today, but a more comprehensive tutorial can be found here.\nTo install R on Debian, we can figure out how to do this by following the CRAN documentation available here.\nFirst they recommend updating the list of available software package we can install with apt-get to us via the apt-get update command:\nroot@5d0f4d21a1f9:/# apt-get update\nNext, they suggest the following commands to install R:\nroot@5d0f4d21a1f9:/# apt-get install r-base r-base-dev\nOK, great! That seemed to have worked! Let’s test it by trying out R!\nroot@5d0f4d21a1f9:/# R\n\nR version 3.5.2 (2018-12-20) -- \"Eggshell Igloo\"\nCopyright (C) 2018 The R Foundation for Statistical Computing\nPlatform: x86_64-pc-linux-gnu (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\n&gt;\nAwesome! This seemed to have worked! Let’s exit R (via q()) and the Docker container (via exit). Then we can add these commands to the Dockerfile, proceeding them with RUN and try to build our image to ensure this works.\nOur Dockerfile so far:\nFROM debian:stable\n\nRUN apt-get update\n\nRUN apt-get install r-base r-base-dev -y\n$ docker build --tag testr1 src\nWait! That didn’t seem to work! Let’s focus on the last two lines of the error message:\nDo you want to continue? [Y/n] Abort.\nThe command '/bin/sh -c apt-get install r-base r-base-dev' returned a non-zero code: 1\nOhhhh, right! As we were interactively installing this, we were prompted to press “Y” on our keyboard to continue the installation. We need to include this in our Dockerfile so that we don’t get this error. To do this we append the -y flag to the end of the line contianing RUN apt-get install r-base r-base-dev. Let’s try building again!\nGreat! Success! Now we can play with installing R packages!\nLet’s start now with the test image we have built from our Dockerfile:\n$ docker run -it --rm testr1\nNow while we are in the container interactively, we can try to install the R package via:\nroot@51f56d653892:/# Rscript -e \"install.packages('cowsay')\"\nAnd it looks like it worked! Let’s confirm by trying to call a function from the cowsay package in R:\nroot@51f56d653892:/# R\n\n&gt; cowsay::say(\"Smart for using Docker are you\", \"yoda\")\nGreat, let’s exit the container, and add this command to our Dockerfile and try to build it again!\nroot@51f56d653892:/# exit\nOur Dockerfile now:\nFROM debian:stable\n\nRUN apt-get update\n\nRUN apt-get install r-base r-base-dev -y\n\nRUN Rscript -e \"install.packages('cowsay')\"\nBuild the Dockerfile into an image:\n$ docker build --tag testr1 src\n\n$ docker run -it --rm testr1\nLooks like a success, let’s be sure we can use the cowsay package:\nroot@861487da5d00:/# R\n\n&gt; cowsay::say(\"why did the chicken cross the road\", \"chicken\")\nHurray! We did it! Now we can automate this build on GitHub, push it to Docker Hub and share this Docker image with the world!\n\nSource: https://giphy.com/gifs/memecandy-ZcKASxMYMKA9SQnhIl",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Customizing and building containers</span>"
    ]
  },
  {
    "objectID": "lectures/120-containerization-3.html#tips-for-installing-things-programmatically-on-debian-flavoured-linux",
    "href": "lectures/120-containerization-3.html#tips-for-installing-things-programmatically-on-debian-flavoured-linux",
    "title": "12  Customizing and building containers",
    "section": "12.2 Tips for installing things programmatically on Debian-flavoured Linux",
    "text": "12.2 Tips for installing things programmatically on Debian-flavoured Linux\n\n12.2.1 Installing things with apt-get\nBefore you install things with apt-get you will want to update the list of packages that apt-get can see. We do this via apt-get update.\nNext, to install something with apt-get you will use the apt-get install command along with the name of the software. For example, to install the Git version control software we would type apt-get install git. Note however that we will be building our containers non-interactively, and so we want to preempt any questions/prompts the installation software we will get by including the answers in our commands. So for example, to apt-get install we append --yes to tell apt-get that yes we are happy to install the software we asked it to install, using the amount of disk space required to install it. If we didn’t append this, the installation would stall out at this point waiting for our answer to this question. Thus, the full command to Git via apt-get looks like:\napt-get install --yes git\n\n\n12.2.2 Breaking shell commands across lines\nIf we want to break a single command across lines in the shell, we use the \\ character. For example, to reduce the long line below which uses apt-get to install the programs Git, Tiny Nano, Less, and wget:\napt-get install --yes git nano-tiny less wget\nWe can use \\ after each program, to break the long command across lines and make the command more readable (especially if there were even more programs to install). Similarly, we indent the lines after \\ to increase readability:\napt-get install --yes \\\n    git \\\n    nano-tidy \\\n    less \\\n    wget\n\n\n12.2.3 Running commands only if the previous one worked\nSometimes we don’t want to run a command if the command that was run immediately before it failed. We can specify this in the shell using &&. For example, if we want to not run apt-get installation commands if apt-get update failed, we can write:\napt-get update && \\\n    apt-get install --yes git",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Customizing and building containers</span>"
    ]
  },
  {
    "objectID": "lectures/120-containerization-3.html#dockerfile-command-summary",
    "href": "lectures/120-containerization-3.html#dockerfile-command-summary",
    "title": "12  Customizing and building containers",
    "section": "12.3 Dockerfile command summary",
    "text": "12.3 Dockerfile command summary\nMost common Dockerfile commands I use:\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\nFROM\nStates which base image the new Docker image should be built on top of\n\n\nRUN\nSpecifies that a command should be run in a shell\n\n\nENV\nSets environment variables\n\n\nEXPOSE\nSpecifies the port the container should listen to at runtime\n\n\nCOPY or ADD\nadds files (or URL’s in the case of ADD) to a container’s filesystem\n\n\nENTRYPOINT\nConfigure a container that will run as an executable\n\n\nWORKDIR\nsets the working directory for any RUN, CMD, ENTRYPOINT, COPY and ADD instructions that follow it in the Dockerfile\n\n\n\nAnd more here in the Dockerfile reference.",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Customizing and building containers</span>"
    ]
  },
  {
    "objectID": "lectures/120-containerization-3.html#choosing-a-base-image-for-your-dockerfile",
    "href": "lectures/120-containerization-3.html#choosing-a-base-image-for-your-dockerfile",
    "title": "12  Customizing and building containers",
    "section": "12.4 Choosing a base image for your Dockerfile",
    "text": "12.4 Choosing a base image for your Dockerfile\n\nSource: https://themuslimtimes.info/2018/10/25/if-i-have-seen-further-it-is-by-standing-on-the-shoulders-of-giants/\n\n12.4.1 Good base images to work from for R or Python projects!\n\n\n\nImage\nSoftware installed\n\n\n\n\nrocker/tidyverse\nR, R packages (including the tidyverse), RStudio, make\n\n\ncontinuumio/anaconda3\nPython 3.7.4, Ananconda base package distribution, Jupyter notebook\n\n\njupyter/scipy-notebook\nIncludes popular packages from the scientific Python ecosystem.\n\n\n\nFor mixed language projects, I would recommend using the rocker/tidyverse image as the base and then installing Anaconda or miniconda as I have done here: https://github.com/UBC-DSCI/introduction-to-datascience/blob/b0f86fc4d6172cd043a0eb831b5d5a8743f29c81/Dockerfile#L19\nThis is also a nice tour de Docker images from the Jupyter core team: https://jupyter-docker-stacks.readthedocs.io/en/latest/using/selecting.html#selecting-an-image",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Customizing and building containers</span>"
    ]
  },
  {
    "objectID": "lectures/120-containerization-3.html#dockerfile-faq",
    "href": "lectures/120-containerization-3.html#dockerfile-faq",
    "title": "12  Customizing and building containers",
    "section": "12.5 Dockerfile FAQ:",
    "text": "12.5 Dockerfile FAQ:\n\n1. Where does the Dockerfile live?\nThe Dockerfile should live in the root directory of your project.\n\n\n2. How do I make an image from a Dockerfile?\nThere are 2 ways to do this! I use the first when developing my Dockerfile (to test quickly that it works), and then the second I use when I think I am “done” and want to have it archived on Docker Hub.\n\nBuild a Docker image locally on your laptop\nBuild a Docker image and push it to DockerHub using GitHub Actions,\n\n\n\n3. How do I build an image locally on my laptop\nFrom the directory that contains your Dockerfile (usually your project root):\ndocker build --tag IMAGE_NAME:VERSION .\n\n\n\n\n\n\nNote\n\n\n\n--tag let’s you name and version the Docker image. You can call this anything you want. The version number/name comes after the colon\n\n\nAfter I build, I think try to docker run ... to test the image locally. If I don’t like it, or it doesn’t work, I delete the image with docker rmi {IMAGE_NAME}, edit my Dockerfile and try to build and run it again.",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Customizing and building containers</span>"
    ]
  },
  {
    "objectID": "lectures/120-containerization-3.html#build-a-docker-image-from-a-dockerfile-on-github-actions",
    "href": "lectures/120-containerization-3.html#build-a-docker-image-from-a-dockerfile-on-github-actions",
    "title": "12  Customizing and building containers",
    "section": "12.6 Build a Docker image from a Dockerfile on GitHub Actions",
    "text": "12.6 Build a Docker image from a Dockerfile on GitHub Actions\nBuilding a Docker image from a Dockerfile using an automated tool (e.g., DockerHub or GitHub Actions) lets others trust your image as they can clearly see which Dockerfile was used to build which image.\nWe will do this in this course by using GitHub Actions (a continuous integration tool) because is provides a great deal of nuanced control over when to trigger the automated builds of the Docker image, and how to tag them.\nAn example GitHub repository that uses GitHub Actions to build a Docker image from a Dockerfile and publish it on DockerHub is available here: https://github.com/ttimbers/gha_docker_build\nWe will work through a demonstration of this now starting here: https://github.com/ttimbers/dockerfile-practice",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Customizing and building containers</span>"
    ]
  },
  {
    "objectID": "lectures/120-containerization-3.html#version-docker-images-and-report-software-and-package-versions",
    "href": "lectures/120-containerization-3.html#version-docker-images-and-report-software-and-package-versions",
    "title": "12  Customizing and building containers",
    "section": "12.7 Version Docker images and report software and package versions",
    "text": "12.7 Version Docker images and report software and package versions\nIt is easier to create a Docker image from a Dockerfile and tag it (or use it’s digest) than to control the version of each thing that goes into your Docker image.\n\ntags are human readable, however they can be associated with different builds of the image (potentially using different Dockerfiles…)\ndigests are not human readable, but specify a specific build of an image\n\nExample of how to pull using a tag:\ndocker pull ttimbers/dockerfile-practice:v1.0\nExample of how to pull using a digest:\ndocker pull ttimbers/dockerfile-practice@sha256:cc512c9599054f24f4020e2c7e3337b9e71fd6251dfde5bcd716dc9b1f8c3a73\nTags are specified when you build on Docker Hub on the Builds tab under the Configure automated builds options. Digests are assigned to a build. You can see the digests on the Tags tab, by clicking on the “Digest” link for a specific tag of the image.\n\n12.7.1 How to get the versions of your software in your container\nEasiest is to enter the container interactively and poke around using the following commands:\n\npython --version and R --version to find out the versions of Python and R, respectively\npip freeze or conda list in the bash shell to find out Python package versions\nEnter R and load the libraries used in your scripts, then use sessionInfo() to print the package versions\n\n\n\n12.7.2 But I want to control the versions!\n\n\n12.7.3 How to in R:\n\nThe Rocker team’s strategy\nThis is not an easy thing, but the Rocker team has made a concerted effort to do this. Below is their strategy:\n\nUsing the R version tag will naturally lock the R version, and also lock the install date of any R packages on the image. For example, rocker/tidyverse:3.3.1 Docker image will always rebuild with R 3.3.1 and R packages installed from the 2016-10-31 MRAN snapshot, corresponding to the last day that version of R was the most recent release. Meanwhile rocker/tidyverse:latest will always have both the latest R version and latest versions of the R packages, built nightly.\n\nSee VERSIONS.md for details, but in short they use the line below to lock the R version (or view in r-ver Dockerfile here for more context):\n    && curl -O https://cran.r-project.org/src/base/R-3/R-${R_VERSION}.tar.gz \\\nAnd this line to specify the CRAN snapshot from which to grab the R packages (or view in r-ver Dockerfile here for more context):\n    && Rscript -e \"install.packages(c('littler', 'docopt'), repo = '$MRAN')\" \\\n\n\n\n12.7.4 How to in Python:\nPython version:\n\nconda to specify an install of specific Python version, either when downloading (see example here, or after downloading with conda install python=3.6).\nOr you can install a specific version of Python yourself, as they do in the Python official images (see here for example), but this is more complicated.\n\nFor Python packages, there are a few tools: - conda (via conda install scipy=0.15.0 for example) - pip (via pip install scipy=0.15.0 for example)\n\n\n12.7.5 Take home messages:\n\nAt a minimum, tag your Docker images or reference image digests\nIf you want to version installs inside the container, use base images that version R & Python, and add what you need on top in a versioned manner!",
    "crumbs": [
      "projects-envs-containers.html",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Customizing and building containers</span>"
    ]
  },
  {
    "objectID": "lectures/130-data-validation.html",
    "href": "lectures/130-data-validation.html",
    "title": "13  Data validation",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "data-testing.html",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data validation</span>"
    ]
  },
  {
    "objectID": "lectures/130-data-validation.html#learning-objectives",
    "href": "lectures/130-data-validation.html#learning-objectives",
    "title": "13  Data validation",
    "section": "",
    "text": "Explain why it is important to validate data used in a data analysis project, and give examples of consequences that might occur with invalid data.\nDiscuss where data validation should happen in a data analysis project.\nList the major checks that should be performed when validating data for data analysis, and justiy why they should be used.\nUse the Python Pandera package to create data schema for checking and to validate data.\nUse the Python Pandera package to drop invalid rows.\nList other commonly used data validation packages for Python and R.",
    "crumbs": [
      "data-testing.html",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data validation</span>"
    ]
  },
  {
    "objectID": "lectures/130-data-validation.html#the-role-of-data-validation-in-data-analysis",
    "href": "lectures/130-data-validation.html#the-role-of-data-validation-in-data-analysis",
    "title": "13  Data validation",
    "section": "13.1 The role of data validation in data analysis",
    "text": "13.1 The role of data validation in data analysis\nRegardless of the statistical question you are asking in your data analysis project, you will be reading in data to Python or R to visualize and/or model. If there are data quality issues, these issues will be propagated and will become data visualization and/or data model issues. This may remind you of an old saying from the mid 20th century:\n“Garbage in, garbage out.”\nThus, to ensure that our data visualization and/or modeling results are correct, robust and of high quality, it is important that we validate, or check, the quality of the data before we perform such analyses. It is important to note that data validation is not sufficient for a correct, robust and of high quality analysis, but it is necessary.",
    "crumbs": [
      "data-testing.html",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data validation</span>"
    ]
  },
  {
    "objectID": "lectures/130-data-validation.html#where-does-data-validation-fit-in-data-analysis",
    "href": "lectures/130-data-validation.html#where-does-data-validation-fit-in-data-analysis",
    "title": "13  Data validation",
    "section": "13.2 Where does data validation fit in data analysis?",
    "text": "13.2 Where does data validation fit in data analysis?\nIf we are going to validate, or check, our data for our data analysis, at what stage in our analysis should we do this? At a minimum, this should be done after data sourcing or extraction, but before data is used in any analysis. In the context of a project where data splitting is needed (e.g., predictive questions using supervised machine learning), this should be done before the data is split.\n\nIf there are larger, more severe consequences of the data analysis being incorrect (e.g., autonomous driving), and the data undergoes file input/output as it is passed through a series of scripts, it may be advisable for data validation, checking to be done each time the data is read. This can be made more efficient by modularizing the data validation/checking into functions. This likely should be done however, regardless of the application of the data analysis, as modularizing the data validation/checking into functions also allows this code to be tested to ensure it is correct, and that invalid data is handled as intended (more on this in the testing chapter later in this book).",
    "crumbs": [
      "data-testing.html",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data validation</span>"
    ]
  },
  {
    "objectID": "lectures/130-data-validation.html#data-validation-checks",
    "href": "lectures/130-data-validation.html#data-validation-checks",
    "title": "13  Data validation",
    "section": "13.3 Data validation checks",
    "text": "13.3 Data validation checks\nWhat kind of data validation, or checks, should be done to ensure the data is of high quality? This does somewhat depend on the type of data being used (e.g., tabular, images, language). Here we will list validations, or checks, that should be done on tabular data. If the reader is interested in validations, or checks, that should be done for more complex data types (e.g., images, language) we refer them to the deepchecks checks gallery for data integrity:\n\ndeepchecks image/vision data integrity checks\ndeepchecks language/NLP data integrity checks\n\n\n13.3.1 Data validation checklist\n\nCorrect data file format2\nCorrect column names1\nNo empty observations4\nMissingness not beyond expected threshold1,2\nCorrect data types in each column1,2\nNo duplicate observations1,2\nNo outlier or anomalous values1,2,3\nCorrect category levels (i.e., no string mismatches or single values)1\nTarget/response variable follows expected distribution1\nNo anomalous correlations between target/response variable and features/explanatory variables1\nNo anomalous correlations between features/explanatory variables1\n\n\nChecklist references\n\nChorev et al (2022). Deepchecks: A Library for Testing and Validating Machine Learning Models and Data. Journal of Machine Learning Research 23 1-6\nMicrosoft Industry Solutions Engineering Team (2024). Engineering Fundamentals Playbook: Testing Data Science and MLOps Code Chapter\nBreck et al (2017). The ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction. Proceedings of IEEE Big Data 1123-1132\nHynes et al (2017). The data linter: Lightweight, automated sanity checking for ml data sets. In NIPS MLSys Workshop 1(2017) 5",
    "crumbs": [
      "data-testing.html",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data validation</span>"
    ]
  },
  {
    "objectID": "lectures/130-data-validation.html#introduction-to-pythons-pandera",
    "href": "lectures/130-data-validation.html#introduction-to-pythons-pandera",
    "title": "13  Data validation",
    "section": "13.4 Introduction to Python’s Pandera",
    "text": "13.4 Introduction to Python’s Pandera\nPython’s Pandera is a package designed to make data validation/checking of dataframes and other dataframe-like objects easy, readable and robust. Key features of Pandera that we will discuss include:\n\nThe ability to define a data schema and use it to validate dataframes and other dataframe-like objects\nCheck the types and properties of columns\nPerform statistical validation of data\nExecute all validation in a lazy manner, so that validation rules are executed before raising an error\nHandle invalid data in a number of ways, including throwing errors, writing data validation logs, and dropping observations that are invalid\n\n\n13.4.1 Validating data with Pandera\nIn the simplest use case, Pandera can be use to validate data by first defining an instance of the DataFrameSchema class. This object specifies the properties we expect (and thus would like to check) for our dataframe index and columns. After the DataFrameSchema instance has been created and defined, the DataFrameSchema.validate method can be applied to a pandas.DataFrame instance to validate, or check, all of the properties we specified that we expect for our dataframe index and columns in the DataFrameSchema instance.\n\n\n13.4.2 Dataframe Schema\nWhen we create an instance of the DataFrameSchema class, we can specify the the properties we expect (and thus would like to check) for our dataframe index and columns.\n\nCreating pa.DataFrameSchema and setting required columns\nTo create an instance of the DataFrameSchema class we first import the Pandera package using the alias pa, and then the function pa.DataFrameSchema. Below we demonstrate creating an instance of the DataFrameSchema class for the first two columns of the Wisconsin Breast Cancer data set from the UCI Machine Learning Repository (Dua and Graff 2017).\n\nimport pandas as pd\nimport pandera as pa\n\n\nschema = pa.DataFrameSchema(\n    {\n        \"class\": pa.Column(),\n        \"mean_radius\": pa.Column()\n    }\n)\n\n\n\n\n\n\n\nNote\n\n\n\nBy default all columns listed are required to be in the dataframe for it to pass validation. If we wanted to make a column optional, we would set required=False in the column constructor.\n\n\n\n\nSpecifying column types\nWe can specify the type we expect each column to be, by writing the type as the first argument to pa.Column. Possible values include:\n\na string alias, as long as it is recognized by pandas.\na python type: int, float, double, bool, str\na numpy data type\na pandas extension type: it can be an instance (e.g pd.CategoricalDtype([\"a\", \"b\"])) or a class (e.g pandas.CategoricalDtype) if it can be initialized with default values.\na pandera DataType: it can also be an instance or a class.\n\nSee the Pandera Data Type Validation docs for details beyond what we present here.\nIf we continue our example from above, we can specify that we expect the class column to be a string and the mean_radius column to be a float as shown below:\n\nschema = pa.DataFrameSchema(\n    {\n        \"class\": pa.Column(str),\n        \"mean_radius\": pa.Column(float)\n    }\n)\n\n\n\nMissingness/null values\nBy default Column objects assume there should be no null/missing values. If you want to allow missing values, you need to set nullable=True in the column constructor. We demonstrate that below for the mean_radius column of our working example. Note that we do not set this to be true for our class column as we likely do not want to be working with observations where the target/response variable is missing.\n\nschema = pa.DataFrameSchema(\n    {\n        \"class\": pa.Column(str),\n        \"mean_radius\": pa.Column(float, nullable=True)\n    }\n)\n\nIf you wanted to allow a percentage of the values for a particular column to be allowed to be missing, then you could do this by writing a lambda function in a call to pa.Check in the column constructor. We show an example of that below where allow up to 5% of the mean_radius column values to be missing.\n\n\n\n\n\n\nNote\n\n\n\nThis is putting the cart a bit before the horse here, as we have not yet introduced pa.Check. We will do that in the next section, so please fell free to skip this and come back to this example after you have read that.\n\n\n\nschema = pa.DataFrameSchema(\n    {\n        \"class\": pa.Column(str),\n        \"mean_radius\": pa.Column(float, \n                                pa.Check(lambda s: s.isna().mean() &lt;= 0.05, \n                                    element_wise=False, \n                                    error=\"Too many null values in 'mean_radius' column.\"), \n                                nullable=True)\n    }\n)\n\n\n\n\n\n\n\nNote\n\n\n\nAbove we have created our custom check on-the-fly using a lambda function. We could do this here because the check was fairly simple. If we needed a custom check that was more complex (e.g., needs to generate data as part of the check) then we would be better to register our custom check. For situations like this, we direct the reader to the Pandera Extension docs.\n\n\n\n\nChecking values in columns\nPandera has a function pa.Check that is useful for checking values within columns. For any type of data, there is usually some reasonable range of values that we would expect. These usually come from domain knowledge about the data. For example, a column named age for a data set about adult human patients age in years should probably be an integer and have a range of values between 18 and 122 (the oldest person whose age has ever been independently verified). To specify a check for a range like this, we can use the pa.Check.between method. We demonstrate how to do this below with our working example to check that the mean_radius values are between 5 and 45, inclusive.\n\nschema = pa.DataFrameSchema(\n    {\n        \"class\": pa.Column(str),\n        \"mean_radius\": pa.Column(float, pa.Check.between(5, 45), nullable=True)\n    }\n)\n\nIn our working example, we might also want to check that the class column only contains the strings we think are acceptable for our category label, which would be \"Benign\" and \"Malignant\". We can do this using the pa.Check.isin method, which we demonstrate below:\n\nschema = pa.DataFrameSchema(\n    {\n        \"class\": pa.Column(str, pa.Check.isin([\"Benign\", \"Malignant\"])),\n        \"mean_radius\": pa.Column(float, pa.Check.between(5, 45), nullable=True)\n    }\n)\n\nThere are many more built-in pa.Check methods. A list can be found in the Pandera Check API docs: - https://pandera.readthedocs.io/en/stable/reference/generated/pandera.api.checks.Check.html#pandera.api.checks.Check\nIf there is a check you wish to do that is not part of the Pandera Check API you have two options:\n\nUse a lambda function with boolean logic inside of pa.Check (good for simple checks, similar to the percentage of missingness in the section above), or\nRegister our custom check (see how to in Pandera Extension docs)\n\n\n\nDuplicates\nPandera does not yet have a method to check for duplicate rows in a dataframe, however, you can apply pa.Check to the entire data frame using a lambda function with boolean logic. Thus, we can easily apply Pandas duplicated function in a Lambda Function to check for duplicate rows. We show an example of that below:\n\nschema = pa.DataFrameSchema(\n    {\n        \"class\": pa.Column(str, pa.Check.isin([\"Benign\", \"Malignant\"])),\n        \"mean_radius\": pa.Column(float, pa.Check.between(5, 45), nullable=True)\n    },\n    checks=[\n        pa.Check(lambda df: ~df.duplicated().any(), error=\"Duplicate rows found.\")\n    ]\n)\n\n\n\nEmpty observations\nSimilar to duplicates, there is no Pandera function for this. So again we can use pa.Check applied to the entire data frame using a lambda function with boolean logic.\n\nschema = pa.DataFrameSchema(\n    {\n        \"class\": pa.Column(str, pa.Check.isin([\"Benign\", \"Malignant\"])),\n        \"mean_radius\": pa.Column(float, pa.Check.between(5, 45), nullable=True)\n    },\n    checks=[\n        pa.Check(lambda df: ~df.duplicated().any(), error=\"Duplicate rows found.\"),\n        pa.Check(lambda df: ~(df.isna().all(axis=1)).any(), error=\"Empty rows found.\")\n    ]\n)\n\n\n\n\n13.4.3 Data validation\nOnce we have specified our the properties we expect (and thus would like to check) for our dataframe index and columns by creating an instance of pa.DataFrameSchema, we can use the pa.DataFrameSchema.validate method on a dataframe to check if the dataframe is valid considering the schema we specified.\nTo demonstrate this, below we create two very simple versions of the Wisconsin Breast Cancer data set. One which we expect to pass our validation checks, and one where we introduce three data anomalies that should cause some checks to fail.\nFirst we create two data frames:\n\nimport numpy as np\n\nvalid_data = pd.DataFrame({\n    \"class\": [\"Benign\", \"Benign\", \"Malignant\"],\n    \"mean_radius\": [6.0, 31.2, 22.8]\n})\n\ninvalid_data = pd.DataFrame({\n    \"class\": [\"Benign\", \"Benign\", \"benign\", \"Malignant\"],\n    \"mean_radius\": [6.0, 6.0, 31.2, -9999]\n})\n\nLet’s see what happens when we apply pa.DataFrameSchema.validate to our valid data:\n\nschema.validate(valid_data)\n\n\n\n\n\n\n\n\nclass\nmean_radius\n\n\n\n\n0\nBenign\n6.0\n\n\n1\nBenign\n31.2\n\n\n2\nMalignant\n22.8\n\n\n\n\n\n\n\nIt returns a dataframe and does not throw an error. Excellent! What happens when we pass clearly invalid data?\n\nschema.validate(invalid_data)\n\n\n---------------------------------------------------------------------------\nSchemaError                               Traceback (most recent call last)\nCell In[11], line 1\n----&gt; 1 schema.validate(invalid_data)\n\nFile ~/.pyenv/versions/ds/lib/python3.11/site-packages/pandera/api/pandas/container.py:126, in DataFrameSchema.validate(self, check_obj, head, tail, sample, random_state, lazy, inplace)\n    114     check_obj = check_obj.map_partitions(  # type: ignore [operator]\n    115         self._validate,\n    116         head=head,\n   (...)\n    122         meta=check_obj,\n    123     )\n    124     return check_obj.pandera.add_schema(self)\n--&gt; 126 return self._validate(\n    127     check_obj=check_obj,\n    128     head=head,\n    129     tail=tail,\n    130     sample=sample,\n    131     random_state=random_state,\n    132     lazy=lazy,\n    133     inplace=inplace,\n    134 )\n\nFile ~/.pyenv/versions/ds/lib/python3.11/site-packages/pandera/api/pandas/container.py:156, in DataFrameSchema._validate(self, check_obj, head, tail, sample, random_state, lazy, inplace)\n    147 if self._is_inferred:\n    148     warnings.warn(\n    149         f\"This {type(self)} is an inferred schema that hasn't been \"\n    150         \"modified. It's recommended that you refine the schema \"\n   (...)\n    153         UserWarning,\n    154     )\n--&gt; 156 return self.get_backend(check_obj).validate(\n    157     check_obj,\n    158     schema=self,\n    159     head=head,\n    160     tail=tail,\n    161     sample=sample,\n    162     random_state=random_state,\n    163     lazy=lazy,\n    164     inplace=inplace,\n    165 )\n\nFile ~/.pyenv/versions/ds/lib/python3.11/site-packages/pandera/backends/pandas/container.py:105, in DataFrameSchemaBackend.validate(self, check_obj, schema, head, tail, sample, random_state, lazy, inplace)\n    100 components = self.collect_schema_components(\n    101     check_obj, schema, column_info\n    102 )\n    104 # run the checks\n--&gt; 105 error_handler = self.run_checks_and_handle_errors(\n    106     error_handler,\n    107     schema,\n    108     check_obj,\n    109     column_info,\n    110     sample,\n    111     components,\n    112     lazy,\n    113     head,\n    114     tail,\n    115     random_state,\n    116 )\n    118 if error_handler.collected_errors:\n    119     if getattr(schema, \"drop_invalid_rows\", False):\n\nFile ~/.pyenv/versions/ds/lib/python3.11/site-packages/pandera/backends/pandas/container.py:180, in DataFrameSchemaBackend.run_checks_and_handle_errors(self, error_handler, schema, check_obj, column_info, sample, components, lazy, head, tail, random_state)\n    169         else:\n    170             error = SchemaError(\n    171                 schema,\n    172                 data=check_obj,\n   (...)\n    178                 reason_code=result.reason_code,\n    179             )\n--&gt; 180         error_handler.collect_error(\n    181             validation_type(result.reason_code),\n    182             result.reason_code,\n    183             error,\n    184             result.original_exc,\n    185         )\n    187 return error_handler\n\nFile ~/.pyenv/versions/ds/lib/python3.11/site-packages/pandera/api/base/error_handler.py:54, in ErrorHandler.collect_error(self, error_type, reason_code, schema_error, original_exc)\n     47 \"\"\"Collect schema error, raising exception if lazy is False.\n     48 \n     49 :param error_type: type of error\n     50 :param reason_code: string representing reason for error\n     51 :param schema_error: ``SchemaError`` object.\n     52 \"\"\"\n     53 if not self._lazy:\n---&gt; 54     raise schema_error from original_exc\n     56 # delete data of validated object from SchemaError object to prevent\n     57 # storing copies of the validated DataFrame/Series for every\n     58 # SchemaError collected.\n     59 if hasattr(schema_error, \"data\"):\n\nFile ~/.pyenv/versions/ds/lib/python3.11/site-packages/pandera/backends/pandas/container.py:201, in DataFrameSchemaBackend.run_schema_component_checks(self, check_obj, schema_components, lazy)\n    199 for schema_component in schema_components:\n    200     try:\n--&gt; 201         result = schema_component.validate(\n    202             check_obj, lazy=lazy, inplace=True\n    203         )\n    204         check_passed.append(is_table(result))\n    205     except SchemaError as err:\n\nFile ~/.pyenv/versions/ds/lib/python3.11/site-packages/pandera/api/dataframe/components.py:162, in ComponentSchema.validate(self, check_obj, head, tail, sample, random_state, lazy, inplace)\n    133 def validate(\n    134     self,\n    135     check_obj,\n   (...)\n    142 ):\n    143     # pylint: disable=too-many-locals,too-many-branches,too-many-statements\n    144     \"\"\"Validate a series or specific column in dataframe.\n    145 \n    146     :check_obj: data object to validate.\n   (...)\n    160 \n    161     \"\"\"\n--&gt; 162     return self.get_backend(check_obj).validate(\n    163         check_obj,\n    164         schema=self,\n    165         head=head,\n    166         tail=tail,\n    167         sample=sample,\n    168         random_state=random_state,\n    169         lazy=lazy,\n    170         inplace=inplace,\n    171     )\n\nFile ~/.pyenv/versions/ds/lib/python3.11/site-packages/pandera/backends/pandas/components.py:136, in ColumnBackend.validate(self, check_obj, schema, head, tail, sample, random_state, lazy, inplace)\n    132             check_obj = validate_column(\n    133                 check_obj, column_name, return_check_obj=True\n    134             )\n    135         else:\n--&gt; 136             validate_column(check_obj, column_name)\n    138 if lazy and error_handler.collected_errors:\n    139     raise SchemaErrors(\n    140         schema=schema,\n    141         schema_errors=error_handler.schema_errors,\n    142         data=check_obj,\n    143     )\n\nFile ~/.pyenv/versions/ds/lib/python3.11/site-packages/pandera/backends/pandas/components.py:92, in ColumnBackend.validate.&lt;locals&gt;.validate_column(check_obj, column_name, return_check_obj)\n     88         error_handler.collect_error(\n     89             validation_type(err.reason_code), err.reason_code, err\n     90         )\n     91 except SchemaError as err:\n---&gt; 92     error_handler.collect_error(\n     93         validation_type(err.reason_code), err.reason_code, err\n     94     )\n\nFile ~/.pyenv/versions/ds/lib/python3.11/site-packages/pandera/api/base/error_handler.py:54, in ErrorHandler.collect_error(self, error_type, reason_code, schema_error, original_exc)\n     47 \"\"\"Collect schema error, raising exception if lazy is False.\n     48 \n     49 :param error_type: type of error\n     50 :param reason_code: string representing reason for error\n     51 :param schema_error: ``SchemaError`` object.\n     52 \"\"\"\n     53 if not self._lazy:\n---&gt; 54     raise schema_error from original_exc\n     56 # delete data of validated object from SchemaError object to prevent\n     57 # storing copies of the validated DataFrame/Series for every\n     58 # SchemaError collected.\n     59 if hasattr(schema_error, \"data\"):\n\nFile ~/.pyenv/versions/ds/lib/python3.11/site-packages/pandera/backends/pandas/components.py:72, in ColumnBackend.validate.&lt;locals&gt;.validate_column(check_obj, column_name, return_check_obj)\n     69 def validate_column(check_obj, column_name, return_check_obj=False):\n     70     try:\n     71         # pylint: disable=super-with-arguments\n---&gt; 72         validated_check_obj = super(ColumnBackend, self).validate(\n     73             check_obj,\n     74             deepcopy(schema).set_name(column_name),\n     75             head=head,\n     76             tail=tail,\n     77             sample=sample,\n     78             random_state=random_state,\n     79             lazy=lazy,\n     80             inplace=inplace,\n     81         )\n     83         if return_check_obj:\n     84             return validated_check_obj\n\nFile ~/.pyenv/versions/ds/lib/python3.11/site-packages/pandera/backends/pandas/array.py:81, in ArraySchemaBackend.validate(self, check_obj, schema, head, tail, sample, random_state, lazy, inplace)\n     75 check_obj = self.run_parsers(\n     76     schema,\n     77     check_obj,\n     78 )\n     80 # run the core checks\n---&gt; 81 error_handler = self.run_checks_and_handle_errors(\n     82     error_handler,\n     83     schema,\n     84     check_obj,\n     85     head=head,\n     86     tail=tail,\n     87     sample=sample,\n     88     random_state=random_state,\n     89 )\n     91 if lazy and error_handler.collected_errors:\n     92     if getattr(schema, \"drop_invalid_rows\", False):\n\nFile ~/.pyenv/versions/ds/lib/python3.11/site-packages/pandera/backends/pandas/array.py:145, in ArraySchemaBackend.run_checks_and_handle_errors(self, error_handler, schema, check_obj, **subsample_kwargs)\n    134         else:\n    135             error = SchemaError(\n    136                 schema=schema,\n    137                 data=check_obj,\n   (...)\n    143                 reason_code=result.reason_code,\n    144             )\n--&gt; 145             error_handler.collect_error(\n    146                 validation_type(result.reason_code),\n    147                 result.reason_code,\n    148                 error,\n    149                 original_exc=result.original_exc,\n    150             )\n    152 return error_handler\n\nFile ~/.pyenv/versions/ds/lib/python3.11/site-packages/pandera/api/base/error_handler.py:54, in ErrorHandler.collect_error(self, error_type, reason_code, schema_error, original_exc)\n     47 \"\"\"Collect schema error, raising exception if lazy is False.\n     48 \n     49 :param error_type: type of error\n     50 :param reason_code: string representing reason for error\n     51 :param schema_error: ``SchemaError`` object.\n     52 \"\"\"\n     53 if not self._lazy:\n---&gt; 54     raise schema_error from original_exc\n     56 # delete data of validated object from SchemaError object to prevent\n     57 # storing copies of the validated DataFrame/Series for every\n     58 # SchemaError collected.\n     59 if hasattr(schema_error, \"data\"):\n\nSchemaError: Column 'class' failed element-wise validator number 0: isin(['Benign', 'Malignant']) failure cases: benign\n\n\n\nWow, that’s a lot, but what is clear is that an error was thrown. If we read through the error message to the end we see the important, and useful piece of the error message:\npandera.errors.SchemaError: Column 'class' failed element-wise validator number 0: isin(['Benign', 'Malignant']) failure cases: benign\nThe error arose because in our invalid_data, the column class contained the string \"benign\", and we specified in our pa.DataFrameSchema instance that we only accept two string values in the class column, \"Benign\" and \"Malignant\".\nWhat about the other errors we expect from our invalid data? For example, we there’s a value of -9999 in the mean_radius column that is well outside of the range we said was valid in the schema (5, 45), and we have a duplicate row as well? Why are these validation errors not reported? Pandera’s default is to throw an error after the first instance of non-valid data. To change this behaviour, we can set lazy=True. When we do this we see that all errors get reported.\n\nschema.validate(invalid_data, lazy=True)\n\n\n---------------------------------------------------------------------------\nSchemaErrors                              Traceback (most recent call last)\nCell In[12], line 1\n----&gt; 1 schema.validate(invalid_data, lazy=True)\n\nFile ~/.pyenv/versions/ds/lib/python3.11/site-packages/pandera/api/pandas/container.py:126, in DataFrameSchema.validate(self, check_obj, head, tail, sample, random_state, lazy, inplace)\n    114     check_obj = check_obj.map_partitions(  # type: ignore [operator]\n    115         self._validate,\n    116         head=head,\n   (...)\n    122         meta=check_obj,\n    123     )\n    124     return check_obj.pandera.add_schema(self)\n--&gt; 126 return self._validate(\n    127     check_obj=check_obj,\n    128     head=head,\n    129     tail=tail,\n    130     sample=sample,\n    131     random_state=random_state,\n    132     lazy=lazy,\n    133     inplace=inplace,\n    134 )\n\nFile ~/.pyenv/versions/ds/lib/python3.11/site-packages/pandera/api/pandas/container.py:156, in DataFrameSchema._validate(self, check_obj, head, tail, sample, random_state, lazy, inplace)\n    147 if self._is_inferred:\n    148     warnings.warn(\n    149         f\"This {type(self)} is an inferred schema that hasn't been \"\n    150         \"modified. It's recommended that you refine the schema \"\n   (...)\n    153         UserWarning,\n    154     )\n--&gt; 156 return self.get_backend(check_obj).validate(\n    157     check_obj,\n    158     schema=self,\n    159     head=head,\n    160     tail=tail,\n    161     sample=sample,\n    162     random_state=random_state,\n    163     lazy=lazy,\n    164     inplace=inplace,\n    165 )\n\nFile ~/.pyenv/versions/ds/lib/python3.11/site-packages/pandera/backends/pandas/container.py:123, in DataFrameSchemaBackend.validate(self, check_obj, schema, head, tail, sample, random_state, lazy, inplace)\n    121         return check_obj\n    122     else:\n--&gt; 123         raise SchemaErrors(\n    124             schema=schema,\n    125             schema_errors=error_handler.schema_errors,\n    126             data=check_obj,\n    127         )\n    129 return check_obj\n\nSchemaErrors: {\n    \"DATA\": {\n        \"DATAFRAME_CHECK\": [\n            {\n                \"schema\": null,\n                \"column\": \"class\",\n                \"check\": \"isin(['Benign', 'Malignant'])\",\n                \"error\": \"Column 'class' failed element-wise validator number 0: isin(['Benign', 'Malignant']) failure cases: benign\"\n            },\n            {\n                \"schema\": null,\n                \"column\": \"mean_radius\",\n                \"check\": \"in_range(5, 45)\",\n                \"error\": \"Column 'mean_radius' failed element-wise validator number 0: in_range(5, 45) failure cases: -9999.0\"\n            },\n            {\n                \"schema\": null,\n                \"column\": null,\n                \"check\": \"Duplicate rows found.\",\n                \"error\": \"DataFrameSchema 'None' failed series or dataframe validator 0: &lt;Check &lt;lambda&gt;: Duplicate rows found.&gt;\"\n            }\n        ]\n    }\n}\n\n\n\n\n\n13.4.4 Handling invalid data\nBy default Pandera will throw an error when a check is not passed. Depending on your situation, this can be a desired expected behaviour (e.g., a static data analysis published in a report) or a very undesired behaviour that could potentially be dangerous (e.g., autonomous driving application). In the latter case, we would want to do something different than throw an error. Possibilities we will cover here include dropping invalid observations and writing log files that report the errors.\n\n\n13.4.5 Dropping invalid observations\nIn an in-production system, dropping non-valid data could be a reasonable path forward instead of throwing an error. Another situation where this might be a reasonable thing to do is when training a machine learning model with a million observations. You don’t want to throw an error in the middle of training if only one observation is invalid!\nTo change the behaviour of pa.DataFrameSchema.validate to instead return a dataframe with the invalid rows dropped we need to do two things:\n\nadd drop_invalid_rows=True to our pa.DataFrameSchema instance\nadd lazy=True to our call to the pa.DataFrameSchema.validate method\n\nBelow we demonstrate this with our working example.\n\nschema = pa.DataFrameSchema(\n    {\n        \"class\": pa.Column(str, pa.Check.isin([\"Benign\", \"Malignant\"]), nullable=True),\n        \"mean_radius\": pa.Column(float, pa.Check.between(5, 45), nullable=True)\n    },\n    checks=[\n        pa.Check(lambda df: ~df.duplicated().any(), error=\"Duplicate rows found.\"),\n        pa.Check(lambda df: ~(df.isna().all(axis=1)).any(), error=\"Empty rows found.\")\n    ],\n    drop_invalid_rows=True\n)\n\nschema.validate(invalid_data, lazy=True)\n\n\n\n\n\n\n\n\nclass\nmean_radius\n\n\n\n\n0\nBenign\n6.0\n\n\n1\nBenign\n6.0\n\n\n\n\n\n\n\nHmmm… Why did the duplicate row sneak through? This is because Pandera’s dropping rows only works on data, or column, checks, not the DataFrame-wide checks like our checks for duplicates or empty rows. Thus to make sure we drop these, we need to rely on Pandas to do this. We demonstrate how we can do this below:\n\nschema.validate(invalid_data, lazy=True).drop_duplicates().dropna(how=\"all\")\n\n\n\n\n\n\n\n\nclass\nmean_radius\n\n\n\n\n0\nBenign\n6.0\n\n\n\n\n\n\n\n\n\n13.4.6 Writing data validation logs\nIs removing the rows sufficient? Not at all! A human should be told that there was invalid data so that upstream data collection, cleaning and transformation processes can be reviewed to minimize the chances of future invalid data. One way to do this is to again specify lazy=True so that all errors can be observed and reported. Then we can get the SchemaErrors and write them to a log file. We show below how to do this for our working example so that the valid rows are returned as a dataframe named validated_data and the errors are logged as a file called validation_errors.log:\n\nimport json\nimport logging\nimport pandas as pd\nimport pandera as pa\nfrom pandera import Check\n\n# Configure logging\nlogging.basicConfig(\n    filename=\"validation_errors.log\",\n    filemode=\"w\",\n    format=\"%(asctime)s - %(message)s\",\n    level=logging.INFO,\n)\n\n# Define the schema\nschema = pa.DataFrameSchema(\n    {\n        \"class\": pa.Column(str, pa.Check.isin([\"Benign\", \"Malignant\"]), nullable=True),\n        \"mean_radius\": pa.Column(float, pa.Check.between(5, 45), nullable=True),\n    },\n    checks=[\n        pa.Check(lambda df: ~df.duplicated().any(), error=\"Duplicate rows found.\"),\n        pa.Check(lambda df: ~(df.isna().all(axis=1)).any(), error=\"Empty rows found.\"),\n    ],\n    drop_invalid_rows=False,\n)\n\n# Initialize error cases DataFrame\nerror_cases = pd.DataFrame()\ndata = invalid_data.copy()\n\n# Validate data and handle errors\ntry:\n    validated_data = schema.validate(data, lazy=True)\nexcept pa.errors.SchemaErrors as e:\n    error_cases = e.failure_cases\n\n    # Convert the error message to a JSON string\n    error_message = json.dumps(e.message, indent=2)\n    logging.error(\"\\n\" + error_message)\n\n# Filter out invalid rows based on the error cases\nif not error_cases.empty:\n    invalid_indices = error_cases[\"index\"].dropna().unique()\n    validated_data = (\n        data.drop(index=invalid_indices)\n        .reset_index(drop=True)\n        .drop_duplicates()\n        .dropna(how=\"all\")\n    )\nelse:\n    validated_data = data",
    "crumbs": [
      "data-testing.html",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data validation</span>"
    ]
  },
  {
    "objectID": "lectures/130-data-validation.html#the-data-validation-ecosystem",
    "href": "lectures/130-data-validation.html#the-data-validation-ecosystem",
    "title": "13  Data validation",
    "section": "13.5 The data validation ecosystem",
    "text": "13.5 The data validation ecosystem\nWe have given a tour of one of the packages in the data validation ecosystem, however there are a few others that are good to know about. We list the others below:\nPython:\n\nPandera: https://pandera.readthedocs.io\nGreat Expectation: https://docs.greatexpectations.io\nDeep Checks: https://docs.deepchecks.com\n\nR\n\npointblank: https://rstudio.github.io/pointblank",
    "crumbs": [
      "data-testing.html",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data validation</span>"
    ]
  },
  {
    "objectID": "lectures/140-intro-to-testing-code.html",
    "href": "lectures/140-intro-to-testing-code.html",
    "title": "14  Introduction to testing code for data science",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "data-testing.html",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Introduction to testing code for data science</span>"
    ]
  },
  {
    "objectID": "lectures/140-intro-to-testing-code.html#learning-objectives",
    "href": "lectures/140-intro-to-testing-code.html#learning-objectives",
    "title": "14  Introduction to testing code for data science",
    "section": "",
    "text": "Fully specify a function and write comprehensive tests against the specification.\nReproducibly generate test data (e.g., data frames, models, plots).\nDiscuss the observability of unit outputs in data science (e.g., plot objects), and how this should be taken into account when designing software.\nExplain why test-driven development (TDD) affords testability\nUse exceptions when writing code.\nTest if a function is throwing an exception when it should, and that is does not do so when it shouldn’t.\nEvaluate test suite quality.",
    "crumbs": [
      "data-testing.html",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Introduction to testing code for data science</span>"
    ]
  },
  {
    "objectID": "lectures/140-intro-to-testing-code.html#functions",
    "href": "lectures/140-intro-to-testing-code.html#functions",
    "title": "14  Introduction to testing code for data science",
    "section": "14.1 Functions",
    "text": "14.1 Functions\n\nFunctions are individual units of code that perform a specific task.\nThey are useful for increasing code modularity - this helps with reusability and readability.\nFunctions can be easily tested to ensure the correct function outputs are given and that they handle user errors in expected ways. These checks help to increase the robustness of your code.\n\nWhen should you write a function? In practice, when you start re-writing code for the second or third time, it really is time to abstract your code into a function. When does this happen in data science? Think back to your DSCI 100 projects, you may have had redundant code when you:\n\nRepeatedly applied some data cleaning/manipulation to columns in your raw data suring the data wrangling process\nCreated many similar data visualizations during your exploratory data analysis\nCreated many similar tidymodels workflows when you were tuning your predictive models",
    "crumbs": [
      "data-testing.html",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Introduction to testing code for data science</span>"
    ]
  },
  {
    "objectID": "lectures/140-intro-to-testing-code.html#workflow-for-writing-functions-and-tests-for-data-science",
    "href": "lectures/140-intro-to-testing-code.html#workflow-for-writing-functions-and-tests-for-data-science",
    "title": "14  Introduction to testing code for data science",
    "section": "14.2 Workflow for writing functions and tests for data science",
    "text": "14.2 Workflow for writing functions and tests for data science\nHow should we get started writing functions and tests for data science? There are many ways one could proceed, however some paths will be more efficient and less error-prone, and more robust than others. Borrowing from software development best practices, one recommended workflow is shown below:\n\nWrite the function specifications and documentation - but do not implement the function. This means that you will have an empty function, that specifies and documents what the name of the function is, what arguments it takes, and what it returns.\nPlan the test cases and document them. Your tests should assess whether the function works as expected when given correct inputs, as well as that it behaves as expected when given incorrect inputs (e.g., throws an error when the wrong type is given for an argument). For the cases of correct inputs, you will want to test the top, middle and bottom range of these, as well as all possible combinations of argument inputs possible. Also, the test data should be as simple and tractable as possible while still being able to assess your function.\nCreate test data that is useful for assessing whether your function works as expected. In data science, you likely need to create both the data that you would provide as inputs to your function, as well as the data that you would expect your function to return.\nWrite the tests to evaluate your function based on the planned test cases and test data.\nImplement the function by writing the needed code in the function body to pass the tests.\nIterate between steps 2-5 to improve the test coverage and function.\n\n\n14.2.1 Example of workflow for writing functions and tests for data science\nLet’s say we want to write a function for a task we repeatedly are performing in our data analysis. For example, summarizing the number of observations in each class. This is a common task performed for almost every classification problem to examine how many classes there are to understand if we are facing a binary or multi-class classification problem, as well as to examine whether there are any class imbalances that we may need to deal with before tuning our models.\n\n1. Write the function specifications and documentation - but do not implement the function:\nThe first thing we should do is write the function specifications and documentation. This can effectively represented by an empty function and roxygen2-styled documentation in R as shown below:\n\n#' Count class observations\n#'\n#' Creates a new data frame with two columns,\n#' listing the classes present in the input data frame,\n#' and the number of observations for each class.\n#'\n#' @param data_frame A data frame or data frame extension (e.g. a tibble).\n#' @param class_col unquoted column name of column containing class labels\n#'\n#' @return A data frame with two columns.\n#'   The first column (named class) lists the classes from the input data frame.\n#'   The second column (named count) lists the number of observations for each class from the input data frame.\n#'   It will have one row for each class present in input data frame.\n#'\n#' @export\n#'\n#' @examples\n#' count_classes(mtcars, am)\ncount_classes &lt;- function(data_frame, class_col) {\n  # returns a data frame with two columns: class and count\n}\n\n\n\n2. Plan the test cases and document them:\nNext, we should plan out our test cases and start to document them. At this point we can sketch out a skeleton for our test cases with code but we are not yet ready to write them, as we first will need to reproducibly create test data that is useful for assessing whether your function works as expected. So considering our function specifications, some kinds of input we might anticipate our function may receive, and correspondingly what it should return is listed in a table below:\n\n\n\n\n\n\n\n\nType of input\nFunction input\nFunction should return\n\n\n\n\nCorrect user input\nData frame with five classes, with 3 observations per class, and an unquoted column name of column containing class labels.\nA new data frame with two columns and five rows. One column with the class names, and one with the counts of 3 observations for each class.\n\n\nCorrect user input\nData frame with two classes, with 3 observations per class, and an unquoted column name of column containing class labels.\nA new data frame with two columns and two rows. One column with the class names, and one with the counts of 3 for each class.\n\n\nCorrect user input\nData frame with two classes, with 3 observations for one class, 2 observations for another, and an unquoted column name of column containing class labels.\nA new data frame with two columns and two rows. One column with the class names, and one with the counts of 3 for one class and 2 for the other class.\n\n\nCorrect user input\nData frame with one class, with 3 observations per class, and an unquoted column name of column containing class labels.\nA new data frame with two columns and one row. One column with the class names, and one with the counts of 3 observations for each class.\n\n\nCorrect user input\nData frame with two classes, with multiple observations for one class, and a single observation for the other class. Also an unquoted column name of column containing class labels.\nA new data frame with two columns and two rows. One column with the class names, and one with the counts of observations for each class.\n\n\nCorrect user input\nAn empty data frame with no classes, and no observations per class. Also an unquoted column name of column containing class labels.\nA new empty data frame with two columns and 0 rows. One column with the class names, and one with the counts of observations for each class.\n\n\nIncorrect user input\nData frame with two classes, with multiple observations per class and a vector of class labels as a separate object\nAn error that reports the value for the class_col argument should be an unquoted column name of column containing class labels from the data frame given in to the first argument.\n\n\nIncorrect user input\nA list as the value for the argument to data_frame, and the name of one of the list elements as the value for the argument class_col\nAn error that reports the value for the the data_frame object should be a data frame or data frame extension (e.g. a tibble)\n\n\n\nNext, I sketch out a skeleton for the unit tests. For R, we will use the well maintained and popular testthat R package for writing our tests. For extra resources on testthat beyond what is demonstrated here, we recommend reading: - testthat documentation - Testing chapter of the R packages book\nWith testthat we create a test_that statement for each related group of tests for a function. For our example, we will create the four test_that statements shown below:\n\nlibrary(testthat)\n\n\ntest_that(\"`count_classes` should return a data frame or data frame extension\", {\n  # tests to be added here\n})\n\ntest_that(\"`count_classes` should return a data frame, or data frame extension,\nwith the number of rows that corresponds to the number of unique classes\nin the column passed to `class_col`\", {\n  # tests to be added here\n})\n\ntest_that(\"`count_classes` should return a data frame, or data frame extension,\nwhose values in the `count` column correspond to the number of observations\nfor the group in the `class` column from the original data frame\", {\n  # tests to be added here\n})\n\ntest_that(\"`count_classes` should throw an error when incorrect types\nare passed to `data_frame` and `class_col` arguments\", {\n  # tests to be added here\n})\n\n\n\n3. Create test data that is useful for assessing whether your function works as expected:\nNow that we have a plan, we can create reproducible test data for that plan! When we do this, we want to keep our data as small and tractable as possible. We want to test things we know the answer to, or can at a minimum calculate by hand. We will use R code to reproducibly create the test data. We will need to do this for the data we will feed in as inputs to our function in the tests, as well as the data we expect our function to return.\n\n# function input for tests\nfive_classes_3_obs &lt;- data.frame(class_lables = rep(c(\"class1\", \"class2\", \"class3\", \"class4\", \"class5\"), 3))\ntwo_classes_3_obs &lt;- data.frame(class_lables = rep(c(\"class1\", \"class2\"), 3))\ntwo_classes_3_and_2_obs &lt;- data.frame(class_lables = c(rep(c(\"class1\", \"class2\"), 2), \"class1\"))\ntwo_classes_3_and_1_obs &lt;- data.frame(class_lables = c(rep(\"class1\", 3), \"class2\"))\none_class_3_obs &lt;- data.frame(class_lables = rep(\"class1\", 3))\nempty_df  &lt;- data.frame(class_lables = character(0))\nvector_class_labels &lt;- rep(c(\"class1\", \"class2\"), 3)\ntwo_classes_3_obs_as_list &lt;- list(class_lables = rep(c(\"class1\", \"class2\"), 3))\n\n# expected function output\nfive_classes_3_obs_output &lt;- data.frame(class = c(\"class1\", \"class2\", \"class3\", \"class4\", \"class5\"),\n                                        count = rep(3, 5))\ntwo_classes_3_obs_output &lt;- data.frame(class = c(\"class1\", \"class2\"),\n                                count = c(3, 3))\ntwo_classes_3_and_2_obs_output &lt;- data.frame(class = c(\"class1\", \"class2\"),\n                                      count = c(3, 2))\ntwo_classes_3_and_1_obs_output &lt;- data.frame(class = c(\"class1\", \"class2\"),\n                                      count = c(3, 1))\none_class_3_obs_output &lt;- data.frame(class = \"class1\",\n                              count = 3)\nempty_df_output &lt;- data.frame(class = character(0),\n                              count = numeric(0))\n\n\n\n4. Write the tests to evaluate your function based on the planned test cases and test data:\nNow that we have the skeletons for our tests, and our reproducible test data, we can actually write the internals for our tests! We will do this by using expect_* functions from the testthat package. The table below shows some of the most commonly used expect_* functions. However, there are many more that can be found in the testthat expectations reference documentation.\n\ntestthat test structure:\ntest_that(\"Message to print if test fails\", expect_*(...))\n\n\n\nCommon expect_* statements for use with test_that\n\nIs the object equal to a value?\n\nexpect_identical - test two objects for being exactly equal\nexpect_equal - compare R objects x and y testing ‘near equality’ (can set a tolerance)\nexpect_equivalent - compare R objects x and y testing ‘near equality’ (can set a tolerance) and does not assess attributes\n\n\n\nDoes code produce an output/message/warning/error?\n\nexpect_error - tests if an expression throws an error\nexpect_warning - tests whether an expression outputs a warning\nexpect_output - tests that print output matches a specified value\n\n\n\nIs the object true/false?\nThese are fall-back expectations that you can use when none of the other more specific expectations apply. The disadvantage is that you may get a less informative error message.\n\nexpect_true - tests if the object returns TRUE\nexpect_false - tests if the object returns FALSE\n\n\ntest_that(\"`count_classes` should return a tibble\", {\n  expect_s3_class(count_classes(two_classes_3_obs, class_lables), \"tibble\")\n})\n\n── Failure: `count_classes` should return a tibble ─────────────────────────────\ncount_classes(two_classes_3_obs, class_lables) is not an S3 object\n\n\nError:\n! Test failed\n\ntest_that(\"`count_classes` should return a data frame, or data frame extension,\nwith the number of rows that corresponds to the number of unique classes\nin the column passed to `class_col`\", {\n  expect_equivalent(count_classes(five_classes_3_obs, class_lables), five_classes_3_obs_output)\n  expect_equivalent(count_classes(two_classes_3_obs, class_lables), two_classes_3_obs_output)\n  expect_equivalent(count_classes(one_class_3_obs, class_lables), one_class_3_obs_output)\n  expect_equivalent(count_classes(empty_df, class_lables), empty_df_output)\n})\n\n── Failure: `count_classes` should return a data frame, or data frame extension,\nwith the number of rows that corresponds to the number of unique classes\nin the column passed to `class_col` ──\ncount_classes(five_classes_3_obs, class_lables) not equivalent to `five_classes_3_obs_output`.\ntarget is NULL, current is data.frame\n\n── Failure: `count_classes` should return a data frame, or data frame extension,\nwith the number of rows that corresponds to the number of unique classes\nin the column passed to `class_col` ──\ncount_classes(two_classes_3_obs, class_lables) not equivalent to `two_classes_3_obs_output`.\ntarget is NULL, current is data.frame\n\n── Failure: `count_classes` should return a data frame, or data frame extension,\nwith the number of rows that corresponds to the number of unique classes\nin the column passed to `class_col` ──\ncount_classes(one_class_3_obs, class_lables) not equivalent to `one_class_3_obs_output`.\ntarget is NULL, current is data.frame\n\n── Failure: `count_classes` should return a data frame, or data frame extension,\nwith the number of rows that corresponds to the number of unique classes\nin the column passed to `class_col` ──\ncount_classes(empty_df, class_lables) not equivalent to `empty_df_output`.\ntarget is NULL, current is data.frame\n\n\nError:\n! Test failed\n\ntest_that(\"`count_classes` should return a data frame, or data frame extension,\nwhose values in the `count` column correspond to the number of observations\nfor the group in the `class` column from the original data frame\", {\n  expect_equivalent(count_classes(two_classes_3_and_2_obs, class_lables), two_classes_3_and_2_obs_output)\n  expect_equivalent(count_classes(two_classes_3_and_1_obs, class_lables), two_classes_3_and_1_obs_output)\n})\n\n── Failure: `count_classes` should return a data frame, or data frame extension,\nwhose values in the `count` column correspond to the number of observations\nfor the group in the `class` column from the original data frame ──\ncount_classes(two_classes_3_and_2_obs, class_lables) not equivalent to `two_classes_3_and_2_obs_output`.\ntarget is NULL, current is data.frame\n\n── Failure: `count_classes` should return a data frame, or data frame extension,\nwhose values in the `count` column correspond to the number of observations\nfor the group in the `class` column from the original data frame ──\ncount_classes(two_classes_3_and_1_obs, class_lables) not equivalent to `two_classes_3_and_1_obs_output`.\ntarget is NULL, current is data.frame\n\n\nError:\n! Test failed\n\ntest_that(\"`count_classes` should throw an error when incorrect types\nare passed to `data_frame` and `class_col` arguments\", {\n  expect_error(count_classes(two_classes_3_obs, vector_class_labels))\n  expect_error(count_classes(two_classes_3_obs_as_list, class_lables))\n})\n\n── Failure: `count_classes` should throw an error when incorrect types\nare passed to `data_frame` and `class_col` arguments ──\n`count_classes(two_classes_3_obs, vector_class_labels)` did not throw an error.\n\n── Failure: `count_classes` should throw an error when incorrect types\nare passed to `data_frame` and `class_col` arguments ──\n`count_classes(two_classes_3_obs_as_list, class_lables)` did not throw an error.\n\n\nError:\n! Test failed\n\n\nWait what??? Most of our tests fail…\nYes, we expect that, we haven’t written our function body yet!\n\n\n\n5. Implement the function by writing the needed code in the function body to pass the tests:\nFINALLY!! We can write the function body for our function! And then call our tests to see if they pass!\n\n#' Count class observations\n#'\n#' Creates a new data frame with two columns,\n#' listing the classes present in the input data frame,\n#' and the number of observations for each class.\n#'\n#' @param data_frame A data frame or data frame extension (e.g. a tibble).\n#' @param class_col unquoted column name of column containing class labels\n#'\n#' @return A data frame with two columns.\n#'   The first column (named class) lists the classes from the input data frame.\n#'   The second column (named count) lists the number of observations for each class from the input data frame.\n#'   It will have one row for each class present in input data frame.\n#'\n#' @export\n#'\n#' @examples\n#' count_classes(mtcars, am)\ncount_classes &lt;- function(data_frame, class_col) {\n  if (!is.data.frame(data_frame)) {\n    stop(\"`data_frame` should be a data frame or data frame extension (e.g. a tibble)\")\n  }\n\n  data_frame |&gt;\n    dplyr::group_by({{ class_col }}) |&gt;\n    dplyr::summarize(count = dplyr::n()) |&gt;\n    dplyr::rename_at(1, ~ \"class\")\n}\n\n\n\n\n\n\n\nNote\n\n\n\n\nwe recommending using the syntax PACKAGE_NAME::FUNCTION() when writing functions that will be sourced into other files in R to make it explicitly clear what external packages they depend on. This becomes even more important when we create R packages from our functions later.\ngroup_by will throw a fairly useful error message of class_col is not found in data_frame, and we we can let group_by handle that error case instead of writing our own exception to throw an error on.\n\n\n\n\ntest_that(\"`count_classes` should return a tibble\", {\n  expect_s3_class(count_classes(two_classes_3_obs, class_lables), \"data.frame\")\n})\n\nTest passed 🌈\n\ntest_that(\"`count_classes` should return a data frame, or data frame extension,\nwith the number of rows that corresponds to the number of unique classes\nin the column passed to `class_col`\", {\n  expect_equivalent(count_classes(five_classes_3_obs, class_lables), five_classes_3_obs_output)\n  expect_equivalent(count_classes(two_classes_3_obs, class_lables), two_classes_3_obs_output)\n  expect_equivalent(count_classes(one_class_3_obs, class_lables), one_class_3_obs_output)\n  expect_equivalent(count_classes(empty_df, class_lables), empty_df_output)\n})\n\nTest passed 🌈\n\ntest_that(\"`count_classes` should return a data frame, or data frame extension,\nwhose values in the `count` column correspond to the number of observations\nfor the group in the `class` column from the original data frame\", {\n  expect_equivalent(count_classes(two_classes_3_and_2_obs, class_lables), two_classes_3_and_2_obs_output)\n  expect_equivalent(count_classes(two_classes_3_and_1_obs, class_lables), two_classes_3_and_1_obs_output)\n})\n\nTest passed 🥳\n\ntest_that(\"`count_classes` should throw an error when incorrect types\nare passed to `data_frame` and `class_col` arguments\", {\n  expect_error(count_classes(two_classes_3_obs, vector_class_labels))\n  expect_error(count_classes(two_classes_3_obs_as_list, class_lables))\n})\n\nTest passed 🥳\n\n\nNo message from the test, means the tests passed!\n\nAre we done? For the purposes of this demo, yes! However in practice you would usually cycle through steps 2-5 two-three more times to further improve our tests and and function!\n\n\n\n14.2.2 Where do the function and test files go?\nIn the workflow above, we skipped over where we should put our functions we will use in our data analyses, as well as where we put the tests for our function, and how we call those tests!\nWe summarize the answer to these questions below, but highly recommend you explore and test out our demonstration GitHub repository that has a minimal working example of this: https://github.com/ttimbers/demo-tests-ds-analysis\nWe also have a Python example here: https://github.com/ttimbers/demo-tests-ds-analysis-python\n\nWhere does the function go?\nIn R, functions should be abstracted to R scripts (plain text files that end in .R) which live in the project’s R directory. Commonly we name the R script with the same name as the function (however, we might choose a more general name if the R script contains many functions).\nIn the analysis file where we call the function (e.g. eda.ipynb) we need to call source(\"PATH_TO_FILE_CONTAINING_FUNCTION\") before we are able to use the function(s) contained in that R script inside our analysis file.\n\n\nWhere do the tests go?\nThe tests for the function should live in tests/testthat/test-FUNCTION_NAME.R, and the code to reproducibly generate helper data for the tests lives in tests/testthat/helper-FUNCTION_NAME.R. The test suite can be run via testthat::test_dir(\"tests/testthat\"). testthat::test_dir(\"tests/testthat\") first runs any files that begin with helper* and then any files that begin with test*.\n\n\nConvenience functions for setting this up\nSeveral usethis R package functions can be used to setup the file and directory structure needed for this: - usethis::use_r(\"FUNCTION_NAME\") can be used to create the R script file the function should live in, inside the R directory - usethis::use_testthat() can be used to create the necessary test directories to use testthat’s automated test suite execution function (testthat::test_dir(\"tests/testthat\")) - usethis::use_test(\"FUNCTION_NAME\") can be used to create the test file for each function\nNote: tests/testthat/helper-FUNCTION_NAME.R needs to be created manually, as there is no usethis function to automate this.",
    "crumbs": [
      "data-testing.html",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Introduction to testing code for data science</span>"
    ]
  },
  {
    "objectID": "lectures/140-intro-to-testing-code.html#reproducibly-generating-test-data",
    "href": "lectures/140-intro-to-testing-code.html#reproducibly-generating-test-data",
    "title": "14  Introduction to testing code for data science",
    "section": "14.3 Reproducibly generating test data",
    "text": "14.3 Reproducibly generating test data\nAs highlighted above, where at all possible, we should use code to generate reproducible, simple and tractable helper data for our tests. When using the testthat R package in R to automate the running of the test suite, the convention is to put such code in a file named helper-FUNCTION_NAME.R which should live in the tests/testthat directory.",
    "crumbs": [
      "data-testing.html",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Introduction to testing code for data science</span>"
    ]
  },
  {
    "objectID": "lectures/140-intro-to-testing-code.html#common-types-of-test-levels-in-data-science",
    "href": "lectures/140-intro-to-testing-code.html#common-types-of-test-levels-in-data-science",
    "title": "14  Introduction to testing code for data science",
    "section": "14.4 Common types of test levels in data science",
    "text": "14.4 Common types of test levels in data science\n\nUnit tests - exercise individual components, usually methods or functions, in isolation. This kind of testing is usually quick to write and the tests incur low maintenance effort since they touch such small parts of the system. They typically ensure that the unit fulfills its contract making test failures more straightforward to understand. This is the kind of tests we wrote for our example for count_classes above.\nIntegration tests - exercise groups of components to ensure that their contained units interact correctly together. Integration tests touch much larger pieces of the system and are more prone to spurious failure. Since these tests validate many different units in concert, identifying the root-cause of a specific failure can be difficult. In data science, this might be testing whether several functions that call each other, or run in sequence, work as expected (e.g., tests for a tidymodel’s workflow function)\n\nSource: CPSC 310 class notes from Reid Holmes, UBC",
    "crumbs": [
      "data-testing.html",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Introduction to testing code for data science</span>"
    ]
  },
  {
    "objectID": "lectures/140-intro-to-testing-code.html#test-driven-development-tdd-and-testability",
    "href": "lectures/140-intro-to-testing-code.html#test-driven-development-tdd-and-testability",
    "title": "14  Introduction to testing code for data science",
    "section": "14.5 Test-driven development (TDD) and testability",
    "text": "14.5 Test-driven development (TDD) and testability\nTest-driven development (TDD): a process for developing software, whereby you write test cases that assess the software requirements before you write the software that implements the software requirements/functionality. Hey! We just did that in the workflow above!\nSource: Wikipedia\nTestability is defined as the degree to which a system or component facilitates the establishment of test objectives and the execution of tests to determine whether those objectives have been achieved.\nIn order to be successful, a test needs to be able to execute the code you wish to test, in a way that can trigger a defect that will propagate an incorrect result to a program point where it can be checked against the expected behaviour. From this we can derive four high-level properties required for effective test writing and execution. These are controllability, observability, isolateablilty, and automatability.\n\ncontrollability: the code under test needs to be able to be programmatically controlled\nobservability: the outcome of the code under test needs to be able to be verified\nisolateablilty: the code under test needs to be able to be validated on its own\nautomatability: the tests should be able to be executed automatically\n\nSource: CPSC 310 & CPSC 410 class notes from Reid Holmes, UBC]\nDiscussion: Does test-driven development afford testability? How might it do so? Let’s discuss controllability, observability, isolateablilty, and automatability in our case study of test-driven development of count_classes.",
    "crumbs": [
      "data-testing.html",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Introduction to testing code for data science</span>"
    ]
  },
  {
    "objectID": "lectures/140-intro-to-testing-code.html#observability-of-unit-outputs-in-data-science",
    "href": "lectures/140-intro-to-testing-code.html#observability-of-unit-outputs-in-data-science",
    "title": "14  Introduction to testing code for data science",
    "section": "14.6 Observability of unit outputs in data science",
    "text": "14.6 Observability of unit outputs in data science\nObservability is defined as the extent to which the response of the code under test (here our functions) to a test can be verified.\nQuestions we should ask when trying to understand how observable our tests are: - What do we have to do to identify pass/fail? - How expensive is it to do this? - Can we extract the result from the code under test? - Do we know enough to identify pass/fail?\nSource: CPSC 410 class notes from Reid Holmes, UBC\nThese questions are easier to answer and address for code that creates simpler data science objects such as data frames, as in the example above. However, when our code under test does something more complex, such as create a plot object, these questions are harder to answer, or can be answered less fully…\nLet’s talk about how we might test code to create plots!\n\n14.6.1 Visual regression testing\nWhen we use certain data visualization libraries, we might think that we can test all code that generates data visualizations similar to code that generates more traditional data objects, such as data frames.\nFor example, when we create a scatter plot object with ggplot2, we can easily observe many of it’s values and attributes. We show an example below:\n\noptions(repr.plot.width = 4, repr.plot.height = 4)\n\n\ncars_ggplot_scatter &lt;- ggplot2::ggplot(mtcars, ggplot2::aes(hp, mpg)) +\n    ggplot2::geom_point()\n\ncars_ggplot_scatter\n\n\n\n\n\n\n\n\n\ncars_ggplot_scatter$layers[[1]]$geom\n\n&lt;ggproto object: Class GeomPoint, Geom, gg&gt;\n    aesthetics: function\n    default_aes: uneval\n    draw_group: function\n    draw_key: function\n    draw_layer: function\n    draw_panel: function\n    extra_params: na.rm\n    handle_na: function\n    non_missing_aes: size shape colour\n    optional_aes: \n    parameters: function\n    rename_size: FALSE\n    required_aes: x y\n    setup_data: function\n    setup_params: function\n    use_defaults: function\n    super:  &lt;ggproto object: Class Geom, gg&gt;\n\n\n\ncars_ggplot_scatter$mapping$x\n\n&lt;quosure&gt;\nexpr: ^hp\nenv:  global\n\n\nAnd so we could write some tests for a function that created a ggplot2 object like so:\n\n#' scatter2d\n#'\n#' A short-cut function for creating 2 dimensional scatterplots via ggplot2.\n#'\n#' @param data data.frame or tibble\n#' @param x unquoted column name to plot on the x-axis from data data.frame or tibble\n#' @param y unquoted column name to plot on the y-axis from data data.frame or tibble\n#'\n#' @return\n#' @export\n#'\n#' @examples\n#' scatter2d(mtcars, hp, mpg)\nscatter2d &lt;- function(data, x, y) {\n    ggplot2::ggplot(data, ggplot2::aes(x = {{x}}, y = {{y}})) +\n        ggplot2::geom_point()\n}\n\nhelper_data &lt;- dplyr::tibble(x_vals = c(2, 4, 6),\n                   y_vals = c(2, 4, 6))\n\nhelper_plot2d &lt;- scatter2d(helper_data, x_vals, y_vals)\n\ntest_that('Plot should use geom_point and map x to x-axis, and y to y-axis.', {\n    expect_true(\"GeomPoint\" %in% c(class(helper_plot2d$layers[[1]]$geom)))\n    expect_true(\"x_vals\"  == rlang::get_expr(helper_plot2d$mapping$x))\n    expect_true(\"y_vals\" == rlang::get_expr(helper_plot2d$mapping$y))\n})\n\nTest passed 😸\n\n\nHowever, when we create a similar plot object using base R, we do not get an object back at all…\n\ncars_scatter &lt;- plot(mtcars$hp, mtcars$mpg)\n\n\n\n\n\n\n\n\n\ntypeof(cars_scatter)\n\n[1] \"NULL\"\n\nclass(cars_scatter)\n\n[1] \"NULL\"\n\n\nSo as you can see, testing plot objects can be more challenging. In the cases of several commonly used plotting functions and package in R and Python, the objects created are not rich objects with attributes that can be easily accessed (or accessed at all). Plotting packages likeggplot2 (R) and altair (Python) which do create rich objects with observable values and attributes appear to be exceptions, rather than the rule. Thus, regression testing against an image generated by the plotting function is often the “best we can do”, or because of this history, what is commonly done.\n\n\n\n\n\n\nRegression Testing\n\n\n\nRegression testing is defined as tests that check that recent changes to the code base do not break already implemented features.\n\n\nThus, once a desired plot is generated from the plotting function, visual regression tests can be used to ensure that further code refactoring does not change the plot function. Tools for this exist for R in the vdiffr package. Matplotlib uses visual regression testing as well, you can see the docs for examples of this here.\n\nVisual regression testing with vdiffr\nSay you have a function that creates a nicely formatted scatter plot using ggplot2, such as the one shown below:\n\npretty_scatter &lt;- function(.data, x_axis_col, y_axis_col) {\n    ggplot2::ggplot(data = .data,\n                    ggplot2::aes(x = {{ x_axis_col }}, y = {{ y_axis_col }})) +\n        ggplot2::geom_point(alpha = 0.8, colour = \"steelblue\", size = 3) +\n        ggplot2::theme_bw() +\n        ggplot2::theme(text = ggplot2::element_text(size = 14))\n}\n\nlibrary(palmerpenguins)\nlibrary(ggplot2)\npenguins_scatter &lt;- pretty_scatter(penguins, bill_length_mm, bill_depth_mm) +\n    labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\")\npenguins_scatter\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nWhat is so pretty about this scatter plot? Compared to the default settings of a scatter plot created in ggplot2 this scatter plot has a white instead of grey background, has blue points instead of black, has larger points, and they points have a bit of transparency so you can see some overlapping data points.\n\nNow, say that you want to write tests to make sure that as you further develop and refactor your data visualization code, you do not break it or change the plot (because you have decided you are happy with what it looks like). You can use the vdiffr visual regression testing package to do this. First, you need to abstract the function to an R script that lives in R. For this case, we would create a file called R/pretty_scatter.R that houses the pretty_scatter function shown above.\nThen you need to setup a tests directory and test file in which to house your tests that works with the testthat framework (we recommend using usethis::use_testthat() and usethis::use_test(\"FUNCTION_NAME\") to do this).\nFinally, add an expectation with vdiffr::expect_doppelganger to your test_that statement:\nlibrary(palmerpenguins)\nlibrary(ggplot2)\nlibrary(vdiffr)\nsource(\"../../R/pretty_scatter.R\")\n\npenguins_scatter &lt;- pretty_scatter(penguins, bill_length_mm, bill_depth_mm) +\n    labs(x = \"Bill length (mm)\", y = \"Bill depth (mm)\")\npenguins_scatter\n\ntest_that(\"refactoring our code should not change our plot\", {\n    expect_doppelganger(\"pretty scatter\", penguins_scatter)\n})\nThen when you run testthat::test_dir(\"tests/testthat\") to run your test suite for the first time, it will take a snapshot of the figure created in your test for that visualization and save it to tests/testthat/_snaps/EXPECT_DOPPELGANGER_TITLE.svg. Then as you refactor your code, you and run testthat::test_dir(\"tests/testthat\") it will compare a new snapshot of the figure with the existing one. If they differ, the tests will fail. You can then run testthat::snapshot_review() to get an interactive viewer which will let you compare the two data visualizations and allow you to either choose to accept the new snapshot (if you wish to include the changes to the data visualization as part of your code revision and refactoring) or you can stop the app and revert/fix some of your code changes so that the data visualization is not unintentionally changed.\nBelow we show an example of running testthat::snapshot_review() after we made our tests fail by removing alpha = 0.8 from our pretty_scatter function source:\n\n\n\nvdiffr demo\nIn this GitHub repository, we have created a vdiffr demo based on the case above: https://github.com/ttimbers/vdiffr-demo\nTo get experience and practice using this, we recommend forking this, and then cloning it so that you can try using this, and building off it locally.\n\n\n\n\n\n\nExercise 1\n\n\n\n\nInside RStudio, run testthat::test_dir(\"tests/testthat\") to ensure you can get the tests to pass as they exist in the demo.\nChange something about the code in R/pretty_scatter.R that will change what the plot looks like (text size, point colour, type of geom used, etc).\nRun testthat::test_dir(\"tests/testthat\") and see if the tests fail. If they do, run testthat::snapshot_review() to explore the differences in the two image snapshots. You may be prompted to install a couple R packages to get this working.\nAdd another plot function to the project and create a test for it using testthat and vdiffr.\n\n\n\n\n\n\n14.6.2 Example tests for altair plots (using object attributes not visual regression testing)\nHere’s a function that creates a scatter plot:\ndef scatter(df, x_axis, y_axis):\n    chart = alt.Chart(df).mark_line().encode(\n        alt.X(x_axis + ':Q',\n            scale=alt.Scale(zero=False),\n              axis=alt.Axis(tickMinStep=1)\n        ),\n        y=y_axis\n    )\n    return chart\nHere’s some small data to test it on:\nsmall_data = pd.DataFrame({\n        'year': np.array([1901, 1902, 1903, 1904, 1905]),\n        'measure' : np.array([25, 25, 50, 50, 100])\n    })\nsmall_data\nHere’s the plot:\nsmall_scatter = scatter(small_data, 'year', 'measure')\nsmall_scatter\n\nHere’s a unit test for the scatter function:\ndef test_scatter():\n    assert small_scatter.encoding.x.field == 'year', 'x_axis should be mapped to the x axis'\n    assert small_scatter.encoding.y.field == 'measure', 'y_axis should be mapped to the y axis'\n    assert small_scatter.mark == 'line', 'mark should be a line'\n    assert small_scatter.encoding.x.scale.zero == False, \"x-axis should not start at 0\"\n    assert small_scatter.encoding.x.axis.tickMinStep == 1, \"x-axis small tick step should be 1\"",
    "crumbs": [
      "data-testing.html",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Introduction to testing code for data science</span>"
    ]
  },
  {
    "objectID": "lectures/140-intro-to-testing-code.html#evaluate-test-suite-quality",
    "href": "lectures/140-intro-to-testing-code.html#evaluate-test-suite-quality",
    "title": "14  Introduction to testing code for data science",
    "section": "14.7 Evaluate test suite quality",
    "text": "14.7 Evaluate test suite quality\nWe really want to know how well the code performs its expected tasks and how robustly it responds to unexpected inputs and situations. Answering this complex and qualitative question involves balancing tests value (e.g., protection against regressions and bugs introduced by refactoring) and tests cost (e.g., time for creation and upkeep, time to run the tests). This is hard to do, and takes a lot of time. Also, as hinted in the previous sentence, it is not very quantifiable.\nCode coverage is a way to more simply assess code quality - it is useful to answer the question does the test suite validate all the code? This is an assessment that also attempts to be more quantitative and is popular because it can often be calculated automatically. However it does not factor time or test value, so we have to take it with a grain of salt.\nSource: CPSC 310 class notes from Reid Holmes, UBC\n\n14.8 Black box & white box testing\nUp until now, we have primarily been focused on deriving tests when thinking about the code specifications - what is the functionality of the code, and does it do what we expect. For example, most of the tests we wrote for the count_classes function would fall under the guise of black box testing. After we follow our TDD-inspired workflow for writing functions and tests for data science, we may want to look inside our code and do some testing of how it is implemented. This would fall under the umbrella of white box testing and code coverage can be very helpful here to identify portions of code that is not being tested well, or at all.\nBlack box testing: is a method of software testing that examines the functionality of an application without peering into its internal structures or workings. This method of test can be applied virtually to every level of software testing: unit, integration, system and acceptance. It is sometimes referred to as specification-based testing.\nWhite box testing: is a method of software testing that tests internal structures or workings of an application, as opposed to its functionality (i.e. black-box testing). In white-box testing an internal perspective of the system, as well as programming skills, are used to design test cases.\nSource: Wikipedia",
    "crumbs": [
      "data-testing.html",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Introduction to testing code for data science</span>"
    ]
  },
  {
    "objectID": "lectures/140-intro-to-testing-code.html#coverage",
    "href": "lectures/140-intro-to-testing-code.html#coverage",
    "title": "14  Introduction to testing code for data science",
    "section": "14.9 Coverage",
    "text": "14.9 Coverage\nDefinition: Proportion of system being executed by the test suite.\n\nusually reported as a percentage: \\[Coverage = \\frac{covered}{(covered + uncovered)} * 100\\]\n\n\n14.9.1 Coverage metrics:\nThere are many, but here are the ones our automated tools in this course will calculate for you:\n\n\n\n\n\n\n\n\nMetric\nDescription\nDependent upon control flow\n\n\n\n\nline\nlines of code that tests execute\nNo\n\n\nbranch\nnumber of branches (independent code segments) that tests execute\nYes\n\n\n\n\n\n14.9.2 What exactly is a branch?\n\nmy_function &lt;- function(x) {\n  # Branch 1\n  if (condition_met) {\n        y = function_a(x)\n        z = function_b(y)\n  }\n  # Branch 2\n  else {\n    y = function_b(x)\n    z = function_c(y)\n  }\n  z\n}\n\nAdapted from: http://www.ncover.com/blog/code-coverage-metrics-branch-coverage/",
    "crumbs": [
      "data-testing.html",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Introduction to testing code for data science</span>"
    ]
  },
  {
    "objectID": "lectures/140-intro-to-testing-code.html#how-are-line-and-branch-coverage-different",
    "href": "lectures/140-intro-to-testing-code.html#how-are-line-and-branch-coverage-different",
    "title": "14  Introduction to testing code for data science",
    "section": "14.10 How are line and branch coverage different?",
    "text": "14.10 How are line and branch coverage different?\nConsider the same example we just saw and the unit test below, let’s manually calculate the coverage using line and branch coverage metrics:\n1  |  my_function &lt;- function(x) {\n2  |  # Branch 1\n3  |    if (x == \"pony\") {\n4  |      y = function_a(x)\n5  |      z = function_b(y)\n6  |    }\n7  |    # Branch 2\n8  |    else {\n9  |      y = function_b(x)\n10 |      z = function_c(y)\n11 |    }\n12 |    z\n13 |  }\n14 |\n15 |  test_that(\"ponies are actually unicorns\", {\n16 |    expect_equal(my_function(\"pony\"), (\"Actually a unicorn\"))\n17 |  })\nNote: function definitions are not counted as lines when calculating coverage\n\nUsing branch as the coverage metric:\nBranch 1 (lines 3-5) is covered by the test, because if (x == \"pony\") evaluates to true. Therefore we have one branch covered, and one branch uncovered (lines 8-10), resulting in 50% branch coverage when we plug these numbers into our formula.\n\\(Coverage = \\frac{covered}{(covered + uncovered)} * 100\\)\n\\(Coverage = \\frac{1}{(1 + 1)} * 100\\)\n\\(Coverage = 50\\%\\)\n\n\nUsing line as the coverage metric:\nLines 3-5 and 12 are executed, and lines 8-10 are not (function definitions are not typically counted in line coverage). There fore we have 4 lines covered, and 3 lines uncovered, resulting in 57% line coverage when we plug these numbers into our formula.\n\\(Coverage = \\frac{covered}{(covered + uncovered)} * 100\\)\n\\(Coverage = \\frac{4}{(4 + 3)} * 100\\)\n\\(Coverage = 57\\%\\)\nIn this case, both metrics give us relatively similar estimates of code coverage.\n\n\n14.10.1 But wait, line coverage can be misleading…\nLet’s alter our function and re-calculate line and branch coverage:\n1  |  my_function &lt;- function(x) {\n2  |  # Branch 1\n3  |    if (x == \"pony\") {\n4  |      y = function_a(x)\n5  |      z = function_b(y)\n6  |      print(z)\n7  |      print(\"some important message\")\n8  |      print(\"another important message\")\n9  |      print(\"a less important message\")\n10 |      print(\"just makin' stuff up here...\")\n11 |      print(\"out of things to say...\")\n12 |      print(\"how creative can I be...\")\n13 |      print(\"I guess not very...\")\n14 |    }\n15 |    # Branch 2\n16 |    else {\n17 |      y = function_b(x)\n18 |      z = function_c(y)\n19 |    }\n20 |    z\n21 |  }\n22 |\n23 |  test_that(\"ponies are actually unicorns\", {\n24 |    expect_equal(my_function(\"pony\"), (\"Actually a unicorn\"))\n25 |  })\n\nUsing branch as the coverage metric:\nBranch 1 (lines 3-13) is covered by the test, because if (x == \"pony\") evaluates to true. Therefore we have one branch covered, and one branch uncovered (lines 16-18), resulting in 50% branch coverage when we plug these numbers into our formula.\n\\(Coverage = \\frac{covered}{(covered + uncovered)} * 100\\)\n\\(Coverage = \\frac{1}{(1 + 1)} * 100\\)\n\\(Coverage = 50\\%\\)\n\n\nUsing line as the coverage metric:\nLines 3-13 and 20 are executed, and lines 16-18 are not (function definitions are not typically counted in line coverage). There fore we have 12 lines covered, and 3 lines uncovered, resulting in 57% line coverage when we plug these numbers into our formula.\n\\(Coverage = \\frac{covered}{(covered + uncovered)} * 100\\)\n\\(Coverage = \\frac{12}{(12 + 3)} * 100\\)\n\\(Coverage = 80\\%\\)\nIn this case, the different metrics give us very different estimates of code coverage! 🤯\n\n\n\n14.10.2 Take home message:\nUse branch coverage when you can, especially if your code uses control flow!",
    "crumbs": [
      "data-testing.html",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Introduction to testing code for data science</span>"
    ]
  },
  {
    "objectID": "lectures/140-intro-to-testing-code.html#calculating-coverage-in-r",
    "href": "lectures/140-intro-to-testing-code.html#calculating-coverage-in-r",
    "title": "14  Introduction to testing code for data science",
    "section": "14.11 Calculating coverage in R",
    "text": "14.11 Calculating coverage in R\nWe use the covr R package to do this.\nInstall via R console:\ninstall.packages(\"covr\")\nTo calculate line coverage and have it show in the viewer pane in RStudio:\ncovr::report()\nCurrently covr does not have the functionality to calculate branch coverage. Thus this is up to you in R to calculate this by hand if you really want to know.\n\nWhy has this not been implemented? It has been in an now unsupported package (see here), but its implementation was too complicated for others to understand. Automating the calculation of branch coverage is non-trivial, and this is a perfect demonstration of that.",
    "crumbs": [
      "data-testing.html",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Introduction to testing code for data science</span>"
    ]
  },
  {
    "objectID": "lectures/140-intro-to-testing-code.html#calculating-coverage-in-python",
    "href": "lectures/140-intro-to-testing-code.html#calculating-coverage-in-python",
    "title": "14  Introduction to testing code for data science",
    "section": "14.12 Calculating coverage in Python",
    "text": "14.12 Calculating coverage in Python\nWe use the plugin tool pytest-cov to do this.\nInstall as a package via conda:\nconda install pytest-cov\n\n14.12.1 Calculating coverage in Python\nTo calculate line coverage and print it to the terminal:\npytest --cov=&lt;directory&gt;\nTo calculate line coverage and print it to the terminal:\npytest --cov-branch --cov=&lt;directory&gt;\n\n\n14.12.2 How does coverage in Python actually count line coverage?\n\nthe output from poetry run pytest --cov=src gives a table that looks like this:\n\n---------- coverage: platform darwin, python 3.7.6-final-0 -----------\nName                  Stmts   Miss  Cover\n-----------------------------------------\nbig_abs/big_abs.py        8      2    75%\n-----------------------------------------\nTOTAL                     9      2    78%\nIn the column labelled as “Stmts”, coverage is calculating all possible line jumps that could have been executed (these line jumps are sometimes called arcs). This is essentially covered + uncovered lines of code.\n\n\n\n\n\n\nNote\n\n\n\nThis leads coverage to count two statements on one line that are separated by a “;” (e.g., print(“hello”); print(“there”)) as one statement, as well as calculating a single statement that is spread across two lines as one statement.\n\n\nIn the column labelled as “Miss”, this is the number of line jumps not executed by the tests. So our covered lines of code is “Stmts” - “Miss”.\nThe coverage percentage in this scenario is calculated by: \\[Coverage = \\frac{(Stmts - Miss)}{Stmts}\\] \\[Coverage = \\frac{8 - 2}{8} * 100 = 75\\%\\]\n\n\n14.12.3 How does coverage in Python actually branch coverage?\n\nthe output from poetry run pytest --cov-branch --cov=src gives a table that looks like this:\n\n---------- coverage: platform darwin, python 3.7.6-final-0 -----------\nName                  Stmts   Miss Branch BrPart  Cover\n-------------------------------------------------------\nbig_abs/big_abs.py        8      2      6      3    64%\n-------------------------------------------------------\nTOTAL                     9      2      6      3    67%\nIn the column labelled as “Branch”, coverage is actually counting the number of possible jumps from branch points. This is essentially covered + uncovered branches of code.\n\n\n\n\n\n\nNote\n\n\n\nBecause coverage is using line jumps to count branches, each if inherently has an else even if its not explicitly written in the code.\n\n\nIn the column labelled as “BrPart”, this is the number of of possible jumps from branch points executed by the tests. This is essentially our covered branches of code.\nThe branch coverage percentage in this tool is calculated by:\n\\[Coverage = \\frac{(Stmts\\:executed + BrPart)}{(Stmts + Branch)}\\]\n\\[Coverage = \\frac{((Stmts - Miss) + BrPart)}{(Stmts + Branch)}\\]\n\n\n\n\n\n\nNote\n\n\n\nYou can see this formula actually includes both line and branch coverage in this calculation.\n\n\nSo for big_abs/big_abs.py 64% was calculated from: \\[Coverage = \\frac{((8 - 2) + 3)}{(8 + 6)} * 100 = 64\\%\\]",
    "crumbs": [
      "data-testing.html",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Introduction to testing code for data science</span>"
    ]
  },
  {
    "objectID": "lectures/140-intro-to-testing-code.html#testing-in-python-resources",
    "href": "lectures/140-intro-to-testing-code.html#testing-in-python-resources",
    "title": "14  Introduction to testing code for data science",
    "section": "14.13 Testing in Python resources",
    "text": "14.13 Testing in Python resources\n\ntesting in Python with Pytest (from the Python packages book)\nPytest documentation\nTesting Software (from the Research Software Engineering with Python book)",
    "crumbs": [
      "data-testing.html",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Introduction to testing code for data science</span>"
    ]
  },
  {
    "objectID": "lectures/140-intro-to-testing-code.html#attribution",
    "href": "lectures/140-intro-to-testing-code.html#attribution",
    "title": "14  Introduction to testing code for data science",
    "section": "14.14 Attribution:",
    "text": "14.14 Attribution:\n\nAdvanced R by Hadley Wickham\nThe Tidynomicon by Greg Wilson\nCPSC 310 and CPSC 410 class notes by Reid Holmes, UBC",
    "crumbs": [
      "data-testing.html",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Introduction to testing code for data science</span>"
    ]
  },
  {
    "objectID": "lectures/150-scripts.html",
    "href": "lectures/150-scripts.html",
    "title": "15  Non-interactive scripts",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "automation.html",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Non-interactive scripts</span>"
    ]
  },
  {
    "objectID": "lectures/150-scripts.html#learning-objectives",
    "href": "lectures/150-scripts.html#learning-objectives",
    "title": "15  Non-interactive scripts",
    "section": "",
    "text": "Explain when it is optimal to work in a read-eval-print-loop (REPL) framework and when it is optimal to shift to using non-interactive scripts.\nBe able to create simple scripts in R that can take input and be executed from the command line.\nDecide when to move from using command line arguments to pass variables into a script to passing variables in via a configuration file, and create scripts that can read configuration files.",
    "crumbs": [
      "automation.html",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Non-interactive scripts</span>"
    ]
  },
  {
    "objectID": "lectures/150-scripts.html#read-eval-print-loop-repl-framework-i.e.-interactive-mode-versus-scripts",
    "href": "lectures/150-scripts.html#read-eval-print-loop-repl-framework-i.e.-interactive-mode-versus-scripts",
    "title": "15  Non-interactive scripts",
    "section": "15.1 Read-eval-print-loop (REPL) framework (i.e., interactive mode) versus Scripts",
    "text": "15.1 Read-eval-print-loop (REPL) framework (i.e., interactive mode) versus Scripts\n\nUp until now, we have primarily been using R and Python in an Read-eval-print-loop (REPL) framework (i.e., interactive mode)\nRead-eval-print-loop (REPL) framework (i.e., interactive mode) is when we run our code in the console in R/Python, or in cells/chunks in the RStudio/Juptyer notebooks\nA Read-eval-print-loop (REPL) framework (i.e., interactive mode) is very useful for:\n\nsolving small problems\ndeveloping code that will be knit to an analytic report\ndeveloping code that will be run as a script (i.e., in “batch” mode)",
    "crumbs": [
      "automation.html",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Non-interactive scripts</span>"
    ]
  },
  {
    "objectID": "lectures/150-scripts.html#what-is-a-script",
    "href": "lectures/150-scripts.html#what-is-a-script",
    "title": "15  Non-interactive scripts",
    "section": "15.2 What is a script?",
    "text": "15.2 What is a script?\nAn R/Python script is simply a plain text file containing (almost) the same commands that you would enter into R/Python’s console or in cells/chunks in the RStudio/Juptyer notebooks. We often run these from top to bottom from the command line/unix shell.\n\n15.2.1 Why write scripts?\n\nEfficiency!\nAutomation!\nReusable!\nRecord of what you have done!\n\nIt also makes your report files a lot cleaner!!!\nYou can find the code repository for this lesson here: https://github.com/DSCI-310/2024-02-13-scripts",
    "crumbs": [
      "automation.html",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Non-interactive scripts</span>"
    ]
  },
  {
    "objectID": "lectures/150-scripts.html#scripts-in-r",
    "href": "lectures/150-scripts.html#scripts-in-r",
    "title": "15  Non-interactive scripts",
    "section": "15.3 Scripts in R",
    "text": "15.3 Scripts in R\nLet’s start with a small, simple example to demonstrate how we write and run scripts in R (it is very similar in Python and we will get to this later in the lesson).\nOur script will be called print_mean_hp.R, and it will calculate the mean horsepower of the cars from the built-in R data frame mtcars.\nWe will develop this script inside RStudio, make sure it works, and then run it from the command line/terminal/Git bash.\n\n15.3.1 Our first R script\n# author: Tiffany Timbers\n# date: 2020-01-15\n#\n# This script calculates the mean horsepower of the cars from the built-in\n# R data frame `mtcars`. This script takes no arguments.\n#\n# Usage: Rscript print_mean_hp.R\n\nmean_hp &lt;- mean(mtcars$hp)\nprint(mean_hp)\n\n\n15.3.2 Running our first R script\nTo run our R script, we need to open the command line/terminal/Git bash, and either navigate to the directory that houses the script OR point to it when we call it. We will do the former.\nThen to run the R script, we use the Rscript command, followed by the name/path to the file:\nRscript print_mean_hp.R\nThe output should be:\n[1] 146.6875\n\n\n15.3.3 A couple notes about scripts\n\nIf you want something to be output to the command line/terminal/Git bash, you should explicitly ask for it to be print. This is not an absolute requirement in R, but it is in Python!\nSimilar with figures, they need to be saved! You will never see a figure created in a script unless you write it to a file.\nFrom a reproducibility perspective, if we want input from the user, usually we will design the scripts to take command line arguments, and not use keyboard/user prompts.\n\n\n\n15.3.4 Script structure and organization\nAlthough not necessary in R or Python, it is still good practice and advised to organize the code in your script into related sections. This practice keeps your code readable and organized. Below we outline how we typically organize R scripts:\n\n\n15.3.5 Example R script organization:\n# documentation comments\n\n# import libraries/packages\n\n# parse/define command line arguments here\n\n# define main function\nmain &lt;- function(){\n    # code for \"guts\" of script goes here\n}\n\n# code for other functions & tests goes here\n\n# call main function\nmain() # pass any command line args to main here\n\n\n15.3.6 R script example\nHere we write a script called quick_titanic_fare_mean.R which reads in the titanic dataset (original source: https://biostat.app.vumc.org/wiki/Main/DataSets) and calculates the mean for the fare (ticket price) variable.\nOur script has two functions, a function we defined to calculate the standard error of the mean (such a function does not exist in R) and a main function which runs the “body” of our code.\n# author: Tiffany Timbers\n# date: 2020-01-15\n#\n# This script calculates the mean for the fare (ticket price)\n# from titanic.csv. This script takes no arguments.\n#\n# Usage: quick_titanic_fare_mean.R\n\nlibrary(tidyverse)\n\nmain &lt;- function() {\ndata &lt;- read_csv('data/titanic.csv')\n  out &lt;- data %&gt;%\n         pull(fare) %&gt;%\n         mean(na.rm = TRUE)\n  print(out)\n}\n\nmain()\n\n\n15.3.7 Saving things from scripts\nAbove we just printed the mean to the terminal. That is was done because the purpose of that script was to have a very simple illustration of how to create and run scripts in R. However, in practice, we typically want to save our analysis artifacts (figures, tables, values, etc) to disc so that we can load them into other files (e.g., our final reports to communicate our analysis findings).\nBelow we show an example of how we would use readr::write_csv to save the mean value we calculated to a .csv file:\n# author: Tiffany Timbers\n# date: 2020-01-15\n#\n# This script calculates the mean horsepower of the cars from the built-in\n# R data frame `mtcars` and saves it to `results/mean_hp_col.csv`.\n# This script takes no arguments.\n#\n# Usage: Rscript print_mean_hp.R\n\nlibrary(readr)\n\nmain &lt;- function() {\n  mean_hp &lt;- mean(mtcars$hp)\n  mean_hp &lt;- data.frame(value = mean_hp)\n  write_csv(mean_hp, \"results/mean_hp_col.csv\")\n}\n\nmain()\n\n\n\n\n\n\nNote\n\n\n\nIn this script we are saving the file to the results directory. There needs to be a results directory created before this script would work.",
    "crumbs": [
      "automation.html",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Non-interactive scripts</span>"
    ]
  },
  {
    "objectID": "lectures/150-scripts.html#using-command-line-arguments-in-r",
    "href": "lectures/150-scripts.html#using-command-line-arguments-in-r",
    "title": "15  Non-interactive scripts",
    "section": "15.4 Using command line arguments in R",
    "text": "15.4 Using command line arguments in R\nLet’s make our script more flexible, and let us specify what column variable we want to calculate the mean for when we call the script.\nTo do this, we use the docopt R package. This will allow us to collect the text we enter at the command line when we call the script, and make it available to us when we run the script.\nWhen we run docopt it takes the text we entered at the command line and gives it to us as a named list of the text provided after the script name. The names of the items in the list come from the documentation. Whitespace at the command line is what is used to parse the text into separate items in the vector.\n# author: Tiffany Timbers\n# date: 2020-01-15\n\n\"This script calculates the mean for a specified column\nfrom titanic.csv.\n\nUsage: quick_titanic_col_mean.R &lt;var&gt;\n\" -&gt; doc\n\n\nlibrary(tidyverse)\nlibrary(docopt)\n\nopt &lt;- docopt(doc)\n\nmain &lt;- function(var) {\n\n  # read in data\n  data &lt;- read_csv('data/titanic.csv')\n\n  # print out statistic of variable of interest\n  out &lt;- data |&gt;\n    pull(!!var) |&gt;\n    mean(na.rm = TRUE)\n  print(out)\n}\n\nmain(opt$var)\n\n\n\n\n\n\nNote\n\n\n\nNote: we use !! in front of opt$col because all command line arguments are passed into R as strings, and are thus quoted. However, pull is a function from the tidyverse that expects an unquoted column name of a data frame. !! does this unquoting. This is similar to {{ that we saw before with functions (which quotes and unquotes values when they are passed into functions). However here we use !! as we have no indirection and just need to perform unquoting.\n\n\nAnd we would run a script like this from the command line as follows:\nRscript src/quick_titanic_col_mean.R fare\nLet’s make our script even more flexible, and let us specify that dataset as well (we could then use it more generally on other files, such as the Gapminder .csv’s).\n# author: Tiffany Timbers\n# date: 2020-01-15\n\n\"This script calculates the mean for a specified column\nfrom titanic.csv.\n\nUsage: quick_titanic_col_mean.R &lt;file_path&gt; &lt;var&gt;\n\" -&gt; doc\n\n\nlibrary(tidyverse)\nlibrary(docopt)\n\nopt &lt;- docopt(doc)\n\nmain &lt;- function(file_path, var) {\n\n  # read in data\n  data &lt;- read_csv(file_path)\n\n  # print out statistic of variable of interest\n  out &lt;- data |&gt;\n    pull(!!var) |&gt;\n    mean(na.rm = TRUE)\n  print(out)\n}\n\nmain(opt$file_path, opt$var)\nNow we would run a script like this from the command line as follows:\nRscript src/quick_csv_col_mean.R data/titanic.csv fare\n\n15.4.1 Positional arguments vs options\nIn the examples above, we used docopt to specify positional arguments. This means that the order matters! If we change the order of the values of the arguments at the command line, our script will likely throw an error, because it will try to perform the wrong operations on the wrong values.\nAnother downside to positional arguments, is that without good documentation, they can be less readable. And certainly the call to the script to is less readable. We can instead give the arguments names using --ARGUMENT_NAME syntax. We call these “options”. Below is the same script but specified using options as opposed to positional arguments:\n# author: Tiffany Timbers\n# date: 2020-01-15\n\n\"This script calculates the mean for a specified column\nfrom titanic.csv.\n\nUsage: quick_csv_col_mean.R --file_path=&lt;file_path&gt; --var=&lt;var&gt;\n\nOptions:\n--file_path=&lt;file_path&gt;   Path to the data file\n--var=&lt;var&gt;               Unquoted column name of the numerical vector for which to calculate the mean\n\" -&gt; doc\n\n\nlibrary(tidyverse)\nlibrary(docopt)\n\nopt &lt;- docopt(doc)\n\nmain &lt;- function(file_path, var) {\n\n  # read in data\n  data &lt;- read_csv(file_path)\n\n  # print out statistic of variable of interest\n  out &lt;- data |&gt;\n    pull(!!var) |&gt;\n    mean(na.rm = TRUE)\n  print(out)\n}\n\nmain(opt$file_path, opt$var)\nAnd we would run a script like this that uses options like this:\nRscript src/quick_csv_col_mean.R --file_path=data/titanic.csv --col=fare\nor like this:\nRscript src/quick_csv_col_mean.R --col=fare --file_path=data/titanic.csv\nbecause we gave the arguments names, and thus their position no longer matters!\n\n\n15.4.2 Some tips for RStudio IDE\n\nTo indent a block of text, highlight and use tab\nTo fix indenting in general to R code standards, use ⌘/ctrl&gt; + shift + i\nTo get multiple cursors, hold alt/option and highlight lines using cursor\nTo get multiple cursors to the beginning of the line, use control A\nTo get multiple cursors to the end of the line, use control E",
    "crumbs": [
      "automation.html",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Non-interactive scripts</span>"
    ]
  },
  {
    "objectID": "lectures/150-scripts.html#scripts-in-python",
    "href": "lectures/150-scripts.html#scripts-in-python",
    "title": "15  Non-interactive scripts",
    "section": "15.5 Scripts in Python",
    "text": "15.5 Scripts in Python\n\n15.5.1 Example Python script organization:\n# documentation comments\n\n# import libraries/packages\n\n# parse/define command line arguments here\n\n# define main function\ndef main():\n    # code for \"guts\" of script goes here\n\n# code for other functions & tests goes here\n\n# call main function\nif __name__ == \"__main__\":\n    main() # pass any command line args to main here\n\n\n\n\n\n\nNote\n\n\n\nYou can see that R and Python scripts should have roughly the same style. There is the difference of if __name__ == \"__main__\": in Python scripts, and R does not really have an equivalent. The benefit of some control flow around main, as is done in Python, is so that you could import or source the other functions in the script without running the main function.\n\n\n\n\n\n\n\n\nTip\n\n\n\nIt is still worthwhile however to have a main function in your R scripts, as it helps with organization and readability.\n\n\n\n\n15.5.2 Using command line arguments in Python\nAlthough docopt for Python exists, it is not currently being supported by an active development community. Thus, we will use the click Python package instead. It is widely used, has a healthy and active development community, and excellent functionality.\nBelow is an example of using click for a simple Python script:\nimport click\n\n@click.command()\n@click.argument('num1', type=int)\n@click.argument('num2', type=int)\ndef main(num1, num2):\n    \"\"\"Simple program that adds two numbers.\"\"\"\n    result = num1 + num2\n    click.echo(f\"The sum of {num1} and {num2} is {result}\")\n\nif __name__ == '__main__':\n    main()\nRunning this script via:\npython sum.py 5 7\nWould result in:\nThe sum of 5 and 7 is 12\n\n\n\n\n\n\nNote\n\n\n\nWe do not need to pass the variables into main(). The click decorators take care of that for us! How nice!!!\n\n\n\n\n15.5.3 Positional arguments vs options in Python\nIf we instead wanted to use options in the script above, we swap the argument method for the option method and add -- to the prefix of our options:\nimport click\n\n@click.command()\n@click.option('--num1', type=int)\n@click.option('--num2', type=int)\ndef main(num1, num2):\n    \"\"\"Simple program that adds two numbers.\"\"\"\n    result = num1 + num2\n    click.echo(f\"The sum of {num1} and {num2} is {result}\")\n\nif __name__ == '__main__':\n    main()\nRunning this script, we now add the names of the options as shown below via:\npython sum.py --num1=5 --num2=7\nWould result in:\nThe sum of 5 and 7 is 12",
    "crumbs": [
      "automation.html",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Non-interactive scripts</span>"
    ]
  },
  {
    "objectID": "lectures/150-scripts.html#saving-objects-from-scripts",
    "href": "lectures/150-scripts.html#saving-objects-from-scripts",
    "title": "15  Non-interactive scripts",
    "section": "15.6 Saving objects from scripts",
    "text": "15.6 Saving objects from scripts\n\n15.6.1 Tables\nAs mentioned already, it is important you save results from your scripts so that you can import them into your reports (or other other data products). For data frame objects that will be presented as tables, writing the objects to a .csv file through readr (in R) or pandas (in Python) is great.\n\n\n15.6.2 Figures\nFor figures, saving images as .png is also a good choice. Although the downstream usage of the figure can sometimes change this recommendation. For a brief but more thorough discussion of this topic, see the “Saving the visualization” chapter from Data Science: An Introduction by Timbers, Campbell & Lee (2020).\n\n\n15.6.3 Model objects\nModel objects that are trained/fit in one script, and then need to be used again later in another script can and should be saved as binary files. In R, the format is .RDS and we use the functions saveRDS() and readRDS() to do this. In python, the format is .pickle and we use the functions pickle.dump() and pickle.load() from the pickle package.\n\nexample of saving a model using saveRDS()\nsaveRDS(final_knn_model, \"final_knn_model.rds\")\n\n\nexample of loading a saved model using readRDS()\nfinal_knn_model &lt;- readRDS(\"final_knn_model.rds\")\n\n\nexample of saving a model using pickle.dump()\n\nfor very simple objects (like preprocessor)\nimport pickle\npickle.dump(knn_preprocessor, open(\"knn_preprocessor.pickle\", \"wb\"))\n\n\nfor more complex objects (like a fit model)\nimport pickle\nwith open(\"knn_fit.pickle\", 'wb') as f:\n    pickle.dump(knn_fit, f)\n\n\n\nexample of loading a saved model using pickle.load()\n\nfor very simple objects (like preprocessor)\nimport pickle\nknn_preprocessor = pickle.load(open(\"knn_preprocessor.pickle\", \"rb\"))\n\n\nfor more complex objects (like a fit model)\nimport pickle\nwith open(\"knn_fit.pickle\", 'rb') as f:\n        knn_fit = pickle.load(f)",
    "crumbs": [
      "automation.html",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Non-interactive scripts</span>"
    ]
  },
  {
    "objectID": "lectures/160-reproducible-reports.html",
    "href": "lectures/160-reproducible-reports.html",
    "title": "16  Reproducible reports",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "automation.html",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Reproducible reports</span>"
    ]
  },
  {
    "objectID": "lectures/160-reproducible-reports.html#learning-objectives",
    "href": "lectures/160-reproducible-reports.html#learning-objectives",
    "title": "16  Reproducible reports",
    "section": "",
    "text": "Discuss the advantages and disadvantages of using literate code documents (e.g., Quarto, Jupyter, R Markdown) for writing analytic reports compared to What You See Is What You Get (WYSIWYG) editors (e.g., Word, Pages)\nConvert .ipynb and .Rmd files to Quarto .qmd files\nExecute and render literate code documents\nGenerate tables of contents, label and number figures and tables, and format bibliographies in a reproducible and automatted manner",
    "crumbs": [
      "automation.html",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Reproducible reports</span>"
    ]
  },
  {
    "objectID": "lectures/160-reproducible-reports.html#reproducible-reports-vs-what-you-see-is-what-you-get-wysiwyg-editors",
    "href": "lectures/160-reproducible-reports.html#reproducible-reports-vs-what-you-see-is-what-you-get-wysiwyg-editors",
    "title": "16  Reproducible reports",
    "section": "16.1 Reproducible reports vs What You See Is What You Get (WYSIWYG) editors",
    "text": "16.1 Reproducible reports vs What You See Is What You Get (WYSIWYG) editors\nReproducible reports are reports where automation is used to update a report when changes to data or data analysis methods lead to changes in data analysis artifacts (e.g., figures, tables, inline text values). This automation is usually specified and controlled by code. In the field of data science, the most common implementations of this are:\n\nQuarto\nR Markdown\nJupyter\nLaTeX\n\n\n\n\n\n\n\nNote\n\n\n\nR Markdown and Jupyter are not completely separable from LaTeX, as they both wrap LaTeX when the desired output is PDF.\n\n\nMost implementations of reproducible reports involve a process called rendering, where a document containing code and/or markup language used to specify the text formatting, document layout, figure and table placement and numbering, bibliography formatting, etc, is converted to an output more suitable for consumption (e.g., beautifully formatted PDF or html document) by some software.\n\nThis contrasts from What You See Is What You Get (WYSIWYG) software or editors (e.g., Microsoft Word, Google Docs, etc), where the document that is being edited looks exactly like the final document - there is no rendering process. WYSIWYG reports are typically easier to get started with and use. However, they are usually quite limited in their ability to automatically update a report when changes to data or data analysis methods lead to changes in data analysis artifacts. This makes them less reproducible, and can lead to errors when repeated manual updating of data analysis artifacts is needed when analysis is iterated on during development.",
    "crumbs": [
      "automation.html",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Reproducible reports</span>"
    ]
  },
  {
    "objectID": "lectures/160-reproducible-reports.html#introduction-to-quarto",
    "href": "lectures/160-reproducible-reports.html#introduction-to-quarto",
    "title": "16  Reproducible reports",
    "section": "16.2 Introduction to Quarto",
    "text": "16.2 Introduction to Quarto\nQuarto is one implementation of a reproducible reporting tool. It is very user friendly and has become very powerful - allowing a great deal of control over formatting with the ability to render to many different outputs.\nIncluding: - PDF - html - Word - Powerpoint presentation - many more!\nIt is a mix of markdown primarily used for narrative text, and code chunks where code can be executed in an engine. It works very well with either R or Python as the coding engine. It has many more advanced features for customizing document outputs compared to Jupyter notebooks on their own, which is why we recommend shifting to this tool when the audience of your analysis moves beyond you and your data science team.\nQuarto also can very easily convert between different kinds of reproducible report documents, making it easy to shift from working in an Jupyter notebook to this different reproducible report tool. For example,\nquarto convert your_notebook.ipynb\nThere is a wonderful online guide for getting to know Quarto, we link to it below. In these notes, we will generally introduce this tool, and demonstrate how to:\n\nAnatomy of a Quarto document\nCreate document sections and a table of contents.\nAdd citations and a bibliography.\nFormat figures and figure captions, as well as automatically number them and cross reference them in the narrative text.\nFormat tables and table descriptions, as well as automatically number them and cross reference them in the narrative text.\nExecute code inline in the report narrative, so that the text will be automatically updated with the correct value when the report is rendered.\nSet the global and local code chunk options so that no code is viewable in the rendered report, just the code outputs where needed (e.g., figures and tables).\n\nQuarto can do all this and so much more, and so if you are interested in learning more be sure to refer to the Quarto Guide.\n\n\n\n\n\n\nGet to know Quarto\n\n\n\nLet’s get to know Quarto! Open RStudio and create a new Quarto document, choosing HTML as the output format. Look at the source document that is created, where is the narrative text written? Where is the code written? How does this differ from Jupyter?\n\n\n\n\n\n\n\n\nRender your first document\n\n\n\nThere are two ways to render a document from the qmd source to the desired output. One is using a button in RStudio - the “Render” button that looks like this:\n\nTry clicking that button and see what happens!\nAnother way you can do this is through code! Try running this in the terminal (replacing \"FILEPATH/FILE.qmd\" with the file path to where you saved this Quarto document:\nquarto render your_report.qmd --to html\n\n\n\n\n\n\n\n\nCheckout the new visual markdown editor\n\n\n\nRStudio has implemented a new feature for working with Quarto to make it more similar to working with Jupyter - it is called the visual markdown editor. Checkout this feature by clicking the visual markdown editor button when you have an R Markdown file open in the editor. The button looks like this:\n\n\n\n\n16.2.1 Quarto in VS Code\nYou can also use Quarto in VS Code with R or Python. If you decide to use this editor, it is highly recommended that you use the VS Code Quarto extension. This will allow you to preview the rendered document, similar to how this can be done in RStudio. In VS Code, to preview the rendered document you click “Preview” (instead of “Render” as in RStudio), which is located at the top right-hand side of the document you are working on.",
    "crumbs": [
      "automation.html",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Reproducible reports</span>"
    ]
  },
  {
    "objectID": "lectures/160-reproducible-reports.html#anatomy-of-a-quarto-document",
    "href": "lectures/160-reproducible-reports.html#anatomy-of-a-quarto-document",
    "title": "16  Reproducible reports",
    "section": "16.3 Anatomy of a Quarto document",
    "text": "16.3 Anatomy of a Quarto document\nAt a basic level, a Quarto document has three parts:\n\nYAML front matter\nMarkdown text\nCode chunks\n\n\n\n16.3.1 YAML front matter\nThe YAML front matter controls global settings for the report (document output types, code display defaults, table of contents presence and depth), as well as houses document metadata (e.g., author, date, etc). Below is an example YAML front matter which sets the:\n\nTitle\nAuthor\nDate of report rendering\nDocument render format to HTML\nDocument have a references section and the source file for the references\nSource code to be hidden in the rendered report\nRStudio editor to show the source code view instead of the visual markdown view\n\n---\ntitle: \"Historical Horse Population in Canada\"\nauthor: \"Tiffany Timbers\"\ndate: Sys.Date()\nformat: html\nbibliography: references.bib\nexecute:\n  echo: false\neditor: visual\n---\n\n\n16.3.2 Markdown text\nFollowing the yaml frontmatter section, by default, the rest of the document is considered Markdown text. If no markdown syntax is used, the text will be rendered as unformatted text. Here is a table of very common markdown formatting that you might find useful:\n\n\n\nDescription\nMarkdown syntax\nOutput\n\n\n\n\nBold text\n**Bold text**\nBold text\n\n\nItalic text\n*Italic text*\nItalic text\n\n\nBold Italic text\n***Bold Italic text***\nBold Italic text\n\n\nverbatim code (not executed)\n`verbatim code`\nverbatim code\n\n\nInline math\n$E = mc^{2}$\n\\(E = mc^{2}\\)\n\n\nDisplay math\n$$E = mc^{2}$$\n\\[E = mc^{2}\\]\n\n\n\nSee the Quarto Markdown docs for additional formatting that you might need to do: - Quarto Markdown basics\n\n\n16.3.3 Code chunks\nJust like Jupyter notebooks, Quarto has code cells, although they’re more commonly referred to as code “chunks” or “blocks”. These are based off fenced Markdown code blocks and always start and end with 3 backticks (```), just like in Markdown. Unique to Quarto is that the leading three backticks are followed by curly braces containing the language engine you want to run, which for r looks like this {r}. For Python, they would look like {python}. Additional metadata can be included, for example a name to reference the code chunk:\n```{r my-first-code-chunk}\nx &lt;- 5\nx\n```\nAll code cells are run when you render the entire document (like pressing “Run all” in JupyterLab). By default, the code in the chunk and the code output will be included in your rendered document. You can also run the code by clicking the green play button on the right-hand side of the code chunk.",
    "crumbs": [
      "automation.html",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Reproducible reports</span>"
    ]
  },
  {
    "objectID": "lectures/160-reproducible-reports.html#document-sections-and-a-table-of-contents",
    "href": "lectures/160-reproducible-reports.html#document-sections-and-a-table-of-contents",
    "title": "16  Reproducible reports",
    "section": "16.4 Document sections and a table of contents",
    "text": "16.4 Document sections and a table of contents\nBy using Markdown formatted headers, we can create document sections that we can link to, as well as easily create linkable table of contents. This is especially important in longer documents, that without such features are hard to navigate.\nMarkdown headers are specified by staring the line with one or more # characters.\nA single # results in a first level header, two #’s results in a second level header, three #'s results in a third level header, and so on.\nFor example:\n# First level header\n## Second level header\nWe can have up to header level 6, ######\nFor example we can have H3 and H4 tags, and will render like this\n### Header 3 markdown\n#### Header 4 Markdown\n\n16.4.1 Header 3 markdown\n\nHeader 4 Markdown\n\n\n\n\n\n\nNote\n\n\n\nWe are rending the H3 and H4 tags because this book is written in Quarto, and having it render the H1 and H2 tags will interfere with the book’s rendering and table of contents.\nWe’ve also disabled numbering for this book beyond H4.",
    "crumbs": [
      "automation.html",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Reproducible reports</span>"
    ]
  },
  {
    "objectID": "lectures/160-reproducible-reports.html#specifying-the-table-of-contents-in-the-yaml-front-matter",
    "href": "lectures/160-reproducible-reports.html#specifying-the-table-of-contents-in-the-yaml-front-matter",
    "title": "16  Reproducible reports",
    "section": "16.5 Specifying the table of contents in the YAML front matter",
    "text": "16.5 Specifying the table of contents in the YAML front matter\nOnce the headers have been created in the document, the YAML front matter can be edited to specify that you would like a table of contents, and to what depth of header level you would like to include in it. This is done by setting the toc key to true, and the header depth the the level you want it use in the table contents using the toc-depth key (e.g., setting that to 2 would include first and second level headers in the table of contents).\nThese need to be set under each document type you want to render to, for example to create a table of contents with depth 3 for HTML documents you would add this to the YAML front matter:\nformat:\n  html:\n    toc: true\n    toc-depth: 3\nYou can render more than one document type from a .qmd file, below is the example of YAML front matter for a .qmd file that renders to both HTML and PDF and set the table of contents depth to 2 for both of these:\n---\ntitle: \"Historical Horse Population in Canada\"\nauthor: \"Tiffany Timbers\"\ndate: Sys.Date()\nformat:\n    html:\n        toc: true\n        toc-depth: 2\n    pdf:\n        toc: true\n        toc-depth: 2\neditor: visual\n---",
    "crumbs": [
      "automation.html",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Reproducible reports</span>"
    ]
  },
  {
    "objectID": "lectures/160-reproducible-reports.html#citations-and-a-bibliographies",
    "href": "lectures/160-reproducible-reports.html#citations-and-a-bibliographies",
    "title": "16  Reproducible reports",
    "section": "16.6 Citations and a bibliographies",
    "text": "16.6 Citations and a bibliographies\nDO NOT format references by hand, you will drive yourself nuts, especially the more references you collect! Instead use Quarto bibliography/citiation functionality. Below is a brief introduction, full docs here.\n\nSteps to citing in Quarto\n\nAdd bibliography: FILENAME.bib to the YAML front matter\nAt the very end of the .qmd file add a header with the word “References” (e.g., ## References)\nCreate FILENAME.bib (this is a plain text file) and place citations in there using bibtex citation format (see examples here).\nIn the .qmd mardown text, use @label (corresponds to the first string inside the {} in the .bib file for the reference of interest) to do a in-text citation.\n\n\n\nGetting bibtex citations\nBibtex citations for code (R & Python programming languages and their respective packages) can be obtained from the sources: - citatation() function in R (no argument will give you the citation for R, adding a package name as a string to the function call will give you the citation for that package) - For Python, I have found I have had to do this somewhat manually from package GitHub repos or docs pages (some packages exist to do this, but none seem to work flawlessly out of box). See my demo .bib file here for an example: https://github.com/ttimbers/breast_cancer_predictor/blob/master/doc/breast_cancer_refs.bib\nBibtex citations for papers can usually be found via Google scholar:\n\n\n1. Search for the paper on Google Scholar\n\nvisit https://scholar.google.com/\n\n\n\n2. Use Google Scholar to get the citation\n\nClick on the quotation icon under the paper:\n\n\n\nClick on the BibTex link on the pop-up box:\n\n\n\nCopy the BibTeX citation to your .bib file\n\n@article{ahmad2007k,\n  title={A k-mean clustering algorithm for mixed numeric and categorical data},\n  author={Ahmad, Amir and Dey, Lipika},\n  journal={Data \\& Knowledge Engineering},\n  volume={63},\n  number={2},\n  pages={503--527},\n  year={2007},\n  publisher={Elsevier}\n}",
    "crumbs": [
      "automation.html",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Reproducible reports</span>"
    ]
  },
  {
    "objectID": "lectures/160-reproducible-reports.html#figures",
    "href": "lectures/160-reproducible-reports.html#figures",
    "title": "16  Reproducible reports",
    "section": "16.7 Figures",
    "text": "16.7 Figures\nFigures can be added by using Markdown figure syntax in the Markdown section of the Quarto documents. For example, the syntax shown below ![](../img/banff.png) will embed the figure in the rendered document:\n\nUsually, we want to do a few things to our figures in reports to make them more understandable and easier to refer to and discuss. These include: - adding figure captions - give them figure numbers so we can cross reference them in the text - adjust their size so they are just big enough to understand, but also small enough to read the surrounding report text\nIn particular, with the assigning figure numbers, we want to do this in a automated way so that if you change the order that figures show up in your report during the editing process, you do not have to manually renumber them.\nThe syntax below takes our Markdown figure and gives it a caption (via adding the caption between the [ ]), gives it a label so that Quarto will do automated numbering of the figure and so you can cross reference it in the text (via #fig-some-name), and specifies the size of the figure to be 50% (via width=50%).\n![The Banff International Research Station campus in February 2024.](../img/banff.png){#fig-banff width=50%}\n\n\n\n\n\n\nFigure 16.1: The Banff International Research Station campus in February 2024.\n\n\n\nTo cross reference the figure in the narrative of the report, we write @fig-banff when we want to refer to it. That will change to Figure 1 in the rendered report, if the figure named fig-banff is the first figure embedded in the report. This works in both R and Python.",
    "crumbs": [
      "automation.html",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Reproducible reports</span>"
    ]
  },
  {
    "objectID": "lectures/160-reproducible-reports.html#tables",
    "href": "lectures/160-reproducible-reports.html#tables",
    "title": "16  Reproducible reports",
    "section": "16.8 Tables",
    "text": "16.8 Tables\nThere are several ways to add tables to Quarto documents, but the most consistent way that works across many data set sizes is to use code from the programming language you are working with to load your data into the Quarto document as a data frame object and then use the appropriate functions from your programming language to display them as a nicely formatted table. This should happen within a code chunk, specifying the code engine you are using (either R or Python). In R, that combination is usually a function from the readr package in combination with the knitr::kable function. In python, that combination is usually a reading data method from the pandas package (e.g., pd.read_csv) in combination with the Markdown function from the IPython package.\nR example:\n```{r}\ntable_to_display &lt;- readr::read_csv(\"my_data.csv\")\nknitr::kable( table_to_display)\n```\nPython example:\n```{python}\nimport pandas as pd\ntable_to_display = pd.read_csv(\"my_data.csv\")\nMarkdown(table_to_display.to_markdown(index = False))\n```\nAs with figures, with tables we usually want to do a few things to our figures in reports to make them more understandable and easier to refer to and discuss. These include: - adding table descriptions - give them table numbers so we can cross reference them in the text\nSimilar to with the assigning figure numbers, when assigning table numbers, we want to do this in a automated way so that if you change the order that tables show up in your report during the editing process, you do not have to manually renumber them.\nSince we loaded the table data into the report using R or Python code in a code chunk, we will this time use code chunk formatting options to add the table descriptions and to gives it a label so that Quarto will do automated numbering of the tables and allow us to cross refernece them in the text (via #tab-some_name).\nR example:\n```{r}\n#| label: tbl-my-data\n#| tbl-cap: Some relevant description about the data in my table.\n\ntable_to_display &lt;- readr::read_csv(\"my_data.csv\")\nknitr::kable( table_to_display)\n```\n\nNote that we only showed the example in one language because the code chunk options for this are the same in both R and Python.\n\nTo cross reference the table in the narrative of the report, we write @tbl-my-data when we want to refer to it. That will change to Table 1 in the rendered report, if the table named tbl-my-data is the first table embedded in the report. This works in both R and Python.",
    "crumbs": [
      "automation.html",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Reproducible reports</span>"
    ]
  },
  {
    "objectID": "lectures/160-reproducible-reports.html#inline-code",
    "href": "lectures/160-reproducible-reports.html#inline-code",
    "title": "16  Reproducible reports",
    "section": "16.9 Inline code",
    "text": "16.9 Inline code\nIf you are wanting to include a value your code calculates in the narrative of your Quarto report, you ideally should not hard-code this value, as it may change if you change something in your analysis. Instead you want to use inline code to embed the appropriate value when your render your document. You can do this via\nYou can refer to values stored in code inside your markdown text using the {language_engine} code_object surrounded by backticks. For example in R you would include this in your narrative text to embed the sample mean inside the report:\n```{r}\nsample_mean = 5\n```\n\nThe radius of the circle is `{r} sample_mean`\nIn Python, you would replace the r with python:\n```{python}\nsample_mean = 5\n```\n\nThe radius of the circle is `{python} sample_mean`\n\n\n\n\n\n\nNote\n\n\n\nThe value for this must be a vector of length one in R, or a scalar in Python.",
    "crumbs": [
      "automation.html",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Reproducible reports</span>"
    ]
  },
  {
    "objectID": "lectures/160-reproducible-reports.html#code-chunk-execution-options",
    "href": "lectures/160-reproducible-reports.html#code-chunk-execution-options",
    "title": "16  Reproducible reports",
    "section": "16.10 Code chunk execution options",
    "text": "16.10 Code chunk execution options\nThere are many code chunk options that you can set. These options let you customize chunk behavior, including whether a chunk is evaluated, whether to include the output in the rendered document, etc.\nA short list of code chunk options is shown below, but you can find an extensive list in the Quarto docs: - Quarto Execution Options\n\nSource: https://quarto.org/docs/computations/execution-options.html#output-options\nYou can set the chunk options at either a global level (once set they will be applied to all code chunks in the .qmd file) or locally for a specific chunk (these will override the global chunk options if they are contradictory).\nGlobal options are set in the YAML front matter at the top of the document. Setting the code chunk options so that the source code (echo) is hidden in the rendered document and that warnings (warning are suppressed) looks like this:\nexecute:\n  echo: false\n  warning: false\nLocal chunk options are set by adding the options at the top of the code chunk. To not display warnings in a single code chunk we would use the warning: FALSE code chunk as follows:\n```{r}\n#| warning: false\n\ncor( c( 1 , 1 ), c( 2 , 3 ) )\n```",
    "crumbs": [
      "automation.html",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Reproducible reports</span>"
    ]
  },
  {
    "objectID": "lectures/160-reproducible-reports.html#a-helpful-hint-for-successfully-working-with-quarto-documents",
    "href": "lectures/160-reproducible-reports.html#a-helpful-hint-for-successfully-working-with-quarto-documents",
    "title": "16  Reproducible reports",
    "section": "16.11 A helpful hint for successfully working with Quarto documents",
    "text": "16.11 A helpful hint for successfully working with Quarto documents\nGiven that you need to render the entire document to see your Markdown and LaTeX rendered, it is important to render often as you make changes. If you make an error in a LaTeX equation for example, it will stop the rendering process and you will not get to see the rendered document. So by rendering often you will know where the last changes you made are and then will be able to easily identify and fix your errors.",
    "crumbs": [
      "automation.html",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Reproducible reports</span>"
    ]
  },
  {
    "objectID": "lectures/160-reproducible-reports.html#quarto-reports-in-subdirectories",
    "href": "lectures/160-reproducible-reports.html#quarto-reports-in-subdirectories",
    "title": "16  Reproducible reports",
    "section": "16.12 Quarto reports in subdirectories",
    "text": "16.12 Quarto reports in subdirectories\nWhen working with a more complex project, it is a good practice to split our files up into subdirectories. This means that our Quarto documents often end up in a directory called reports or analysis or something similar. This best practice can lead to headaches when rendering the document, as the relative paths for loading in data artifacts (figures and tables) can change depending on where the document is executed from.\nIn this course we will adopt the strategy that Quarto documents will be executed from the subdirectory that live in (e.g., reports or analysis or something similar). This is actually Quarto’s default when using the quarto render ... command. To ensure that the preview works correctly with this setup, however, we need to add an empty _quarto.yml file to our project root directory. This can be done by running touch _quarto.yml in the terminal in your project root. This document can be used to set many kinds of Quarto configurations (see all here), including saving the rendered documents to different directories (which can be useful if you plan to serve up your report as a nice, human readable HTML document on the web as documented here).",
    "crumbs": [
      "automation.html",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Reproducible reports</span>"
    ]
  },
  {
    "objectID": "lectures/160-reproducible-reports.html#strategies-for-collaborative-writing-with-quarto",
    "href": "lectures/160-reproducible-reports.html#strategies-for-collaborative-writing-with-quarto",
    "title": "16  Reproducible reports",
    "section": "16.13 Strategies for collaborative writing with Quarto",
    "text": "16.13 Strategies for collaborative writing with Quarto\nAlthough Quarto is wonderful for incorporating code, and code-generated research artifacts in a reproducible manner, it is not as user-friendly for collaborative writing as other tools (e.g., Google docs). In particular, the lack of real-time collaboration and possibilities for merge conflicts that cannot be automatically resolved are the greatest challenges. Thus, to make collaborative writing efficient and effective with Quarto, we need to take a thoughtful approach to using it for this purpose. Clear communication helps, in particular with avoiding merge conflicts, however we can do more than that to make this work even better. We outline below, two strategies we can incorporate, which function to help reduce the number of merge conflicts that cannot be automatically resolved, and create a cohesive document, even with multiple authors.\n\n16.13.1 Child documents\nWhen multiple authors are editing a Quarto document at the same time in collaborative writing, there ends up being a good chance that a merge conflict will occur, and potentially one that cannot be automatically resolved. One way to avoid this is using communication to assign different authors to work on different sections of the document - reducing the chance that they will be editing the same lines at the same time. With this strategy however, this still can happen with sections that are adjacent to each other as Git uses the line numbers, not section headers to decide which changes are overappling changes.\nThus, an even better strategy is the use of child documents. Child documents are documents that are sourced into the main (parent) document when the document is rendered (you get one single document rendered, which includes all the content from the parent and child documents). A good idea in collaborative writing is to split each section into a separate child document. That way, each person working on a section is working on a separate document - minimizing greatly the number of potential merge conflicts.\nThe Includes syntax is the way to incorporate a child document into a parent document in Quarto. Below we show an example parent document (breast_cancer_predictor_report.qmd) which uses child documents to split the sections of the report into separate documents.\n---\ntitle: \"Predicting breast cancer from digitized images of breast mass\"\nauthor: \"Tiffany A. Timbers, Joel Ostblom & Melissa Lee\"\nformat:\n    pdf\n        toc: true\n        toc-depth: 2\nbibliography: references.bib\nexecute:\n  echo: false\n  warning: false\n---\n\n## Abstract\n\n{{&lt; include _abstract.qmd &gt;}}\n\n## Introduction\n\n{{&lt; include _introduction.qmd &gt;}}\n\n## Methods\n\n{{&lt; include _methods.qmd &gt;}}\n\n## Results & Discussion\n\n{{&lt; include _results-and-discussion.qmd &gt;}}\n\n## References\nThis document would live in a project structure something like this (only zooming in on the reports sub-directory:\nproject/\n├── data/\n├── reports/\n│   ├── _abstract.qmd\n│   ├── _introduction.qmd\n│   ├── _methods.qmd\n│   ├── _results-and-discussion.qmd\n│   └── breast_cancer_predictor_report.qmd\n├── src/\n├── doc/\n├── README.md\n└── Dockerfile\nAnd when rendered, (by running quarto render reports/breast_cancer_predicotr_report.qmd) a single PDF document would be rendered from the 5 separate .qmd files.\n\n\n16.13.2 Smoothing\nOne downside to splitting collaborative writing into sections, is the the resulting document reads like it was written by several authors (because it was)! Additionally, the transitions between different sections of the report are often quite disjoint and abrupt (Stout, 2022). This is really undesirable as it makes it more difficult for the reader to understand the report. To counteract this effect of drafting a manuscript collaboratively by sections, we need to employ a smoothing process to blend the writing styles from the various authors, so that in the end, it will read as one consistently styled document, similar to documents written by a single author (Stout, 2022).\nSmoothing is a step, after the initial draft is generated by separate authors, where the authors trade section assignments and edit and revise a different section than which they initially drafted. It is also advisable after this step of smoothing, to do another step, where the entire manuscript is read by at least 2-3 authors where they pay particular attention to section transitions. This extra work, results in a more cohesive, clear and easy to read document, and should be used in any collaborative writing project (Stout, 2022).",
    "crumbs": [
      "automation.html",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Reproducible reports</span>"
    ]
  },
  {
    "objectID": "lectures/160-reproducible-reports.html#references",
    "href": "lectures/160-reproducible-reports.html#references",
    "title": "16  Reproducible reports",
    "section": "16.14 References",
    "text": "16.14 References\nSara Stoudt (2022) Collaborative Writing Workflows in the Data-Driven Classroom: A Conversation Starter, Journal of Statistics and Data Science Education, 30:3, 282-288, DOI: 10.1080/26939169.2022.2082602",
    "crumbs": [
      "automation.html",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Reproducible reports</span>"
    ]
  },
  {
    "objectID": "lectures/170-pipelines-scripts.html",
    "href": "lectures/170-pipelines-scripts.html",
    "title": "17  Data analysis pipelines with scripts",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "automation.html",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Data analysis pipelines with scripts</span>"
    ]
  },
  {
    "objectID": "lectures/170-pipelines-scripts.html#learning-objectives",
    "href": "lectures/170-pipelines-scripts.html#learning-objectives",
    "title": "17  Data analysis pipelines with scripts",
    "section": "",
    "text": "Justify why analysis pipelines should be used over a single long script in data analysis projects, specifically in regards to how this affects reproducibility, maintainability and future derivatives of the work.",
    "crumbs": [
      "automation.html",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Data analysis pipelines with scripts</span>"
    ]
  },
  {
    "objectID": "lectures/170-pipelines-scripts.html#data-analysis-pipelines",
    "href": "lectures/170-pipelines-scripts.html#data-analysis-pipelines",
    "title": "17  Data analysis pipelines with scripts",
    "section": "17.1 Data analysis pipelines",
    "text": "17.1 Data analysis pipelines\n\nAs analysis grows in length and complexity, one literate code document generally is not enough\nTo improve code report readability (and code reproducibility and modularity) it is better to abstract at least parts of the code away (e.g, to scripts)\nThese scripts save figures and tables that will be imported into the final report\n\n\n\n\n\n\n\n\nBuilding a Data Analysis pipeline using a Shell script tutorial\n\n\n\nAdapted from Software Carpentry\nTo illustrate how to make a data analysis pipeline using a shell script to drive other scripts, we will work through an example together. Here are some set-up instructions so that we can do this together:\n\nSet-up instructions\n\nClick the green “Use this template” button from this GitHub repository to obtain a copy of it for yourself (do not fork it).\nClone this repository to your computer.\n\n\n\nLet’s do some analysis!\nSuppose we have a script, wordcount.py, that reads in a text file, counts the words in this text file, and outputs a data file:\npython scripts/wordcount.py \\\n    --input_file=data/isles.txt \\\n    --output_file=results/isles.dat\nIf we view the first 5 rows of the data file using head,\nhead -5 results/isles.dat\nwe can see that the file consists of one row per word. Each row shows the word itself, the number of occurrences of that word, and the number of occurrences as a percentage of the total number of words in the text file.\nthe 3822 6.7371760973\nof 2460 4.33632998414\nand 1723 3.03719372466\nto 1479 2.60708619778\na 1308 2.30565838181\nSuppose we also have a script, plotcount.py, that reads in a data file and save a plot of the 10 most frequently occurring words:\npython scripts/plotcount.py \\\n    --input_file=results/isles.dat \\\n    --output_file=results/figure/isles.png\nTogether these scripts implement a data analysis pipeline:\n\nRead a data file.\nPerform an analysis on this data file.\nWrite the analysis results to a new file.\nPlot a graph of the analysis results.\n\nTo document how we’d like the analysis to be run, so we (and others) have a record and can replicate it, we will build a shell script called run_all.sh. Let’s work to try to build this pipeline so it does all that!\n# run_all.sh\n# Tiffany Timbers, Nov 2017\n#\n# This driver script completes the textual analysis of\n# 3 novels and creates figures on the 10 most frequently\n# occuring words from each of the 3 novels. This script\n# takes no arguments.\n#\n# Usage: bash run_all.sh\n\n# perform wordcout on novels\npython scripts/wordcount.py \\\n    --input_file=data/isles.txt \\\n    --output_file=results/isles.dat\n\n# create plots\npython scripts/plotcount.py \\\n    --input_file=results/isles.dat \\\n    --output_file=results/figure/isles.png\nWe actually have 4 books that we want to analyze, and then put the figures in a report.\n\nRead a data file.\nPerform an analysis on this data file.\nWrite the analysis results to a new file.\nPlot a graph of the analysis results.\nSave the graph as an image, so we can put it in a paper.\n\nLet’s extend our shell script to do that!\n# run_all.sh\n# Tiffany Timbers, Nov 2018\n\n# This driver script completes the textual analysis of\n# 3 novels and creates figures on the 10 most frequently\n# occurring words from each of the 3 novels. This script\n# takes no arguments.\n\n# example usage:\n# bash run_all.sh\n\n# count the words\npython scripts/wordcount.py \\\n    --input_file=data/isles.txt \\\n    --output_file=results/isles.dat\npython scripts/wordcount.py \\\n    --input_file=data/abyss.txt \\\n    --output_file=results/abyss.dat\npython scripts/wordcount.py \\\n    --input_file=data/last.txt \\\n    --output_file=results/last.dat\npython scripts/wordcount.py \\\n    --input_file=data/sierra.txt \\\n    --output_file=results/sierra.dat\n\n# create the plots\npython scripts/plotcount.py \\\n    --input_file=results/isles.dat \\\n    --output_file=results/figure/isles.png\npython scripts/plotcount.py \\\n    --input_file=results/abyss.dat \\\n    --output_file=results/figure/abyss.png\npython scripts/plotcount.py \\\n    --input_file=results/last.dat \\\n    --output_file=results/figure/last.png\npython scripts/plotcount.py \\\n    --input_file=results/sierra.dat \\\n    --output_file=results/figure/sierra.png\n\n# write the report\nquarto render report/count_report.qmd\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nFrom the breast cancer prediction example analysis repo, here is a data analysis pipeline using a shell script:\n# run_all.sh\n# Tiffany Timbers, Jan 2020\n\n# download data\npython src/download_data.py --out_type=feather --url=https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wpbc.data --out_file=data/raw/wdbc.feather\n\n# run eda report\nRscript -e \"rmarkdown::render('src/breast_cancer_eda.Rmd')\"\n\n# pre-process data\nRscript src/pre_process_wisc.r --input=data/raw/wdbc.feather --out_dir=data/processed\n\n# create exploratory data analysis figure and write to file\nRscript src/eda_wisc.r --train=data/processed/training.feather --out_dir=results\n\n# tune model\nRscript src/fit_breast_cancer_predict_model.r --train=data/processed/training.feather --out_dir=results\n\n# test model\nRscript src/breast_cancer_test_results.r --test=data/processed/test.feather --out_dir=results\n\n# render final report\nRscript -e \"rmarkdown::render('doc/breast_cancer_predict_report.Rmd', output_format = 'github_document')\"",
    "crumbs": [
      "automation.html",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Data analysis pipelines with scripts</span>"
    ]
  },
  {
    "objectID": "lectures/170-pipelines-scripts.html#discussion",
    "href": "lectures/170-pipelines-scripts.html#discussion",
    "title": "17  Data analysis pipelines with scripts",
    "section": "17.2 Discussion",
    "text": "17.2 Discussion\n\nWhat are the advantages to using a data analysis pipeline?\nHow “good” is a shell script as a data analysis pipeline? What might not be ideal about this?",
    "crumbs": [
      "automation.html",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Data analysis pipelines with scripts</span>"
    ]
  },
  {
    "objectID": "lectures/180-pipelines-make.html",
    "href": "lectures/180-pipelines-make.html",
    "title": "18  Data analysis pipelines with GNU Make",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "automation.html",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data analysis pipelines with GNU Make</span>"
    ]
  },
  {
    "objectID": "lectures/180-pipelines-make.html#learning-objectives",
    "href": "lectures/180-pipelines-make.html#learning-objectives",
    "title": "18  Data analysis pipelines with GNU Make",
    "section": "",
    "text": "Write a simple automated analysis pipeline using workflow tool (e.g., GNU Make)\nDiscuss the advantage of using software that has a dependency graph for analysis pipelines compared to software that does not.",
    "crumbs": [
      "automation.html",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data analysis pipelines with GNU Make</span>"
    ]
  },
  {
    "objectID": "lectures/180-pipelines-make.html#gnu-make-as-a-data-analysis-pipeline-tool",
    "href": "lectures/180-pipelines-make.html#gnu-make-as-a-data-analysis-pipeline-tool",
    "title": "18  Data analysis pipelines with GNU Make",
    "section": "18.1 GNU Make as a data analysis pipeline tool",
    "text": "18.1 GNU Make as a data analysis pipeline tool\nWe previously built a data analysis pipeline by using a shell script (we called it run_all.sh) to piece together and create a record of all the scripts and arguments we used in our analysis. That is a step in the right direction, but there were a few unsatisfactory things about this strategy:\n\nIt takes time to manually erase all intermediate and final files generated by analysis to do a complete test to see that everything is working from top to bottom\nIt runs every step every time. This can be problematic if some steps take a long time and you have only changed other, smaller parts of the analysis\n\nThus, to improve on this we are going to use the build and automation tool, Make, to make a smarter data analysis pipeline.",
    "crumbs": [
      "automation.html",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data analysis pipelines with GNU Make</span>"
    ]
  },
  {
    "objectID": "lectures/180-pipelines-make.html#makefile-structure",
    "href": "lectures/180-pipelines-make.html#makefile-structure",
    "title": "18  Data analysis pipelines with GNU Make",
    "section": "18.2 Makefile Structure",
    "text": "18.2 Makefile Structure\nEach block of code in a Makefile is called a rule, it looks something like this:\nfile_to_create.png : data_it_depends_on.dat script_it_depends_on.py\n    python script_it_depends_on.py data_it_depends_on.dat file_to_create.png\n\nfile_to_create.png is a target, a file to be created, or built.\ndata_it_depends_on.dat and script_it_depends_on.py are dependencies, files which are needed to build or update the target. Targets can have zero or more dependencies.\n: separates targets from dependencies.\npython script_it_depends_on.py data_it_depends_on.dat file_to_create.png is an action, a command to run to build or update the target using the dependencies. Targets can have zero or more actions. Actions are indented using the TAB character, not 8 spaces.\nTogether, the target, dependencies, and actions form a rule.",
    "crumbs": [
      "automation.html",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data analysis pipelines with GNU Make</span>"
    ]
  },
  {
    "objectID": "lectures/180-pipelines-make.html#structure-if-you-have-multiple-targets-from-a-scripts",
    "href": "lectures/180-pipelines-make.html#structure-if-you-have-multiple-targets-from-a-scripts",
    "title": "18  Data analysis pipelines with GNU Make",
    "section": "18.3 Structure if you have multiple targets from a scripts",
    "text": "18.3 Structure if you have multiple targets from a scripts\nfile_to_create_1.png file_to_create_2.png : data_it_depends_on.dat script_it_depends_on.py\n    python script_it_depends_on.py data_it_depends_on.dat file_to_create\n\n\n\n\n\n\nBuilding a Data Analysis pipeline using Make, a tutorial\n\n\n\nadapted from Software Carpentry\n\nSet-up instructions\n\nClick the green “Use this template” button from this GitHub repository to obtain a copy of it for yourself (do not fork it).\nClone this repository to your computer.\n\nGood reference: http://swcarpentry.github.io/make-novice/reference\nCreate a file, called Makefile, with the following content:\n# Count words.\nresults/isles.dat : data/isles.txt src/wordcount.py\n    python scripts/wordcount.py \\\n        --input_file=data/isles.txt \\\n        --output_file=results/isles.dat\nThis is a simple build file, which for GNU Make is called a Makefile - a file executed by GNU Make. Let us go through each line in turn:\n\n# denotes a comment. Any text from # to the end of the line is ignored by Make.\nresults/isles.dat is a target, a file to be created, or built.\ndata/isles.txt and scripts/wordcount.py are dependencies, a file that is needed to build or update the target. Targets can have zero or more dependencies.\n: separates targets from dependencies.\npython scripts/wordcount.py --input_file=data/isles.txt --output_file=results/isles.dat is an action, a command to run to build or update the target using the dependencies. Targets can have zero or more actions.\nActions are indented using the TAB character, not 8 spaces. This is a legacy of Make’s 1970’s origins.\nTogether, the target, dependencies, and actions form a rule.\n\nOur rule above describes how to build the target results/isles.dat using the action python scripts/wordcount.py and the dependency data/isles.txt.\nBy default, Make looks for a Makefile, called Makefile, and we can run Make as follows:\n$ make results/isles.dat\nMake prints out the actions it executes:\npython scripts/wordcount.py --input_file=data/isles.txt --output_file=results/isles.dat\nIf we see,\nMakefile:3: *** missing separator.  Stop.\nthen we have used a space instead of a TAB characters to indent one of our actions.\nWe don’t have to call our Makefile Makefile. However, if we call it something else we need to tell Make where to find it. This we can do using -f flag. For example:\n$ make -f Makefile results/isles.dat\nAs we have re-run our Makefile, Make now informs us that:\nmake: `results/isles.dat' is up to date.\nThis is because our target, results/isles.dat, has now been created, and Make will not create it again. To see how this works, let’s pretend to update one of the text files. Rather than opening the file in an editor, we can use the shell touch command to update its timestamp (which would happen if we did edit the file):\n$ touch data/isles.txt\nIf we compare the timestamps of data/isles.txt and results/isles.dat,\n$ ls -l data/isles.txt results/isles.dat\nthen we see that results/isles.dat, the target, is now older thandata/isles.txt, its dependency:\n-rw-r--r--    1 mjj      Administ   323972 Jun 12 10:35 books/isles.txt\n-rw-r--r--    1 mjj      Administ   182273 Jun 12 09:58 isles.dat\nIf we run Make again,\n$ make results/isles.dat\nthen it recreates results/isles.dat:\npython src/wordcount.py data/isles.txt results/isles.dat\nWhen it is asked to build a target, Make checks the ‘last modification time’ of both the target and its dependencies. If any dependency has been updated since the target, then the actions are re-run to update the target.\nWe may want to remove all our data files so we can explicitly recreate them all. We can introduce a new target, and associated rule, clean:\n# Count words.\nresults/isles.dat : data/isles.txt src/wordcount.py\n    python scripts/wordcount.py \\\n        --input_file=data/isles.txt \\\n        --output_file=results/isles.dat\n\nclean :\n    rm -f results/isles.dat\nThis is an example of a rule that has no dependencies. clean has no dependencies on any .dat file as it makes no sense to create these just to remove them. We just want to remove the data files whether or not they exist. If we run Make and specify this target,\n$ make clean\nthen we get:\nrm -f *.dat\nThere is no actual thing built called clean. Rather, it is a short-hand that we can use to execute a useful sequence of actions.\nLet’s add another rule to the end of Makefile:\nresults/isles.dat : data/isles.txt scripts/wordcount.py\n    python scripts/wordcount.py \\\n        --input_file=data/isles.txt \\\n        --output_file=results/isles.dat\n\nresults/figure/isles.png : results/isles.dat scripts/plotcount.py\n    python scripts/plotcount.py \\\n        --input_file=results/isles.dat \\\n        --output_file=results/figure/isles.png\n\nclean :\n    rm -f results/isles.dat\n    rm -f results/figure/isles.png\nthe new target isles.png depends on the target isles.dat. So to make both, we can simply type:\n$ make results/figure/isles.png\n$ ls\nLet’s add another book:\nresults/isles.dat : data/isles.txt scripts/wordcount.py\n    python scripts/wordcount.py \\\n        --input_file=data/isles.txt \\\n        --output_file=results/isles.dat\n\nresults/abyss.dat : data/abyss.txt scripts/wordcount.py\npython scripts/wordcount.py \\\n    --input_file=data/abyss.txt \\\n    --output_file=results/abyss.dat\n\nresults/figure/isles.png : results/isles.dat scripts/plotcount.py\n    python scripts/plotcount.py \\\n        --input_file=results/isles.dat \\\n        --output_file=results/figure/isles.png\n\nresults/figure/abyss.png : results/abyss.dat scripts/plotcount.py\n    python scripts/plotcount.py \\\n        --input_file=results/abyss.dat \\\n        --output_file=results/figure/abyss.png\n\nclean :\n    rm -f results/isles.dat \\\n        results/abyss.dat\n    rm -f results/figure/isles.png \\\n        results/figure/abyss.png\nTo run all of the commands, we need to type make  for each one:\n$ make results/figure/isles.png\n$ make results/figure/abyss.png\nOR we can add a target all to the very top of the Makefile which will build the last of the dependencies.\nall: results/figure/isles.png results/figure/abyss.png",
    "crumbs": [
      "automation.html",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data analysis pipelines with GNU Make</span>"
    ]
  },
  {
    "objectID": "lectures/180-pipelines-make.html#finish-off-the-makefile",
    "href": "lectures/180-pipelines-make.html#finish-off-the-makefile",
    "title": "18  Data analysis pipelines with GNU Make",
    "section": "18.4 Finish off the Makefile!",
    "text": "18.4 Finish off the Makefile!\nSince we will also combine the figures into a report in the end, we will change our all target to being the rendered report file, and add a target for the rendered report file at the end:\n# Makefile\n# Tiffany Timbers, Nov 2018\n\n# This driver script completes the textual analysis of\n# 3 novels and creates figures on the 10 most frequently\n# occuring words from each of the 3 novels. This script\n# takes no arguments.\n\n# example usage:\n# make all\n\nall : report/count_report.html\n\n# count the words\nresults/isles.dat : data/isles.txt scripts/wordcount.py\n    python scripts/wordcount.py \\\n        --input_file=data/isles.txt \\\n        --output_file=results/isles.dat\n\nresults/abyss.dat : data/abyss.txt scripts/wordcount.py\npython scripts/wordcount.py \\\n    --input_file=data/abyss.txt \\\n    --output_file=results/abyss.dat\n\nresults/last.dat : data/last.txt scripts/wordcount.py\n    python scripts/wordcount.py \\\n        --input_file=data/last.txt \\\n        --output_file=results/last.dat\n\nresults/sierra.dat : data/sierra.txt scripts/wordcount.py\n    python scripts/wordcount.py \\\n        --input_file=data/sierra.txt \\\n        --output_file=results/sierra.dat\n\n# create the plots\nresults/figure/isles.png : results/isles.dat scripts/plotcount.py\n    python scripts/plotcount.py \\\n        --input_file=results/isles.dat \\\n        --output_file=results/figure/isles.png\n\nresults/figure/abyss.png : results/abyss.dat scripts/plotcount.py\n    python scripts/plotcount.py \\\n        --input_file=results/abyss.dat \\\n        --output_file=results/figure/abyss.png\n\nresults/figure/last.png : results/last.dat scripts/plotcount.py\n    python scripts/plotcount.py \\\n        --input_file=results/last.dat \\\n        --output_file=results/figure/last.png\n\nresults/figure/sierra.png : results/sierra.dat scripts/plotcount.py\n    python scripts/plotcount.py \\\n        --input_file=results/sierra.dat \\\n        --output_file=results/figure/sierra.png\n\n# write the report\nreport/count_report.html : report/count_report.qmd \\\nresults/figure/isles.png \\\nresults/figure/abyss.png \\\nresults/figure/last.png \\\nresults/figure/sierra.png\n    quarto render report/count_report.qmd\n\nclean :\n    rm -f results/isles.dat \\\n        results/abyss.dat \\\n        results/last.dat \\\n        results/sierra.dat\n    rm -f results/figure/isles.png \\\n        results/figure/abyss.png \\\n        results/figure/last.png \\\n        results/figure/sierra.png\n    rm -rf report/count_report.html",
    "crumbs": [
      "automation.html",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data analysis pipelines with GNU Make</span>"
    ]
  },
  {
    "objectID": "lectures/180-pipelines-make.html#improving-the-makefile",
    "href": "lectures/180-pipelines-make.html#improving-the-makefile",
    "title": "18  Data analysis pipelines with GNU Make",
    "section": "18.5 Improving the Makefile",
    "text": "18.5 Improving the Makefile\nAdding PHONY targets to run parts of the analysis (e.g., just counting the words, or just making the figures) can be very useful when iterating and refining a particular part of your analysis. Similarly, adding other PHONY targets to clean up parts of the analysis can help with this as well.\nIn the version of the Makefile below we do just that, creating the following PHONY targets: - dats (counts words and saves to .dat files) - figs (creates figures) - clean-dats (cleans up .dat files) - clean-figs(cleans up figure files)\n# Makefile\n# Tiffany Timbers, Nov 2018\n\n# This driver script completes the textual analysis of\n# 3 novels and creates figures on the 10 most frequently\n# occuring words from each of the 3 novels. This script\n# takes no arguments.\n\n# example usage:\n# make all\n\n# run entire analysis\nall: report/count_report.html\n\n# count words\ndats: results/isles.dat \\\nresults/abyss.dat \\\nresults/last.dat \\\nresults/sierra.dat\n\nresults/isles.dat : scripts/wordcount.py data/isles.txt\n    python scripts/wordcount.py \\\n        --input_file=data/isles.txt \\\n        --output_file=results/isles.dat\nresults/abyss.dat : scripts/wordcount.py data/abyss.txt\n    python scripts/wordcount.py \\\n        --input_file=data/abyss.txt \\\n        --output_file=results/abyss.dat\nresults/last.dat : scripts/wordcount.py data/last.txt\n    python scripts/wordcount.py \\\n        --input_file=data/last.txt \\\n        --output_file=results/last.dat\nresults/sierra.dat : scripts/wordcount.py data/sierra.txt\n    python scripts/wordcount.py \\\n        --input_file=data/sierra.txt \\\n        --output_file=results/sierra.dat\n\n# plot\nfigs : results/figure/isles.png \\\n    results/figure/abyss.png \\\n    results/figure/last.png \\\n    results/figure/sierra.png\n\nresults/figure/isles.png : scripts/plotcount.py results/isles.dat\n    python scripts/plotcount.py \\\n        --input_file=results/isles.dat \\\n        --output_file=results/figure/isles.png\nresults/figure/abyss.png : scripts/plotcount.py results/abyss.dat\n    python scripts/plotcount.py \\\n        --input_file=results/abyss.dat \\\n        --output_file=results/figure/abyss.png\nresults/figure/last.png : scripts/plotcount.py results/last.dat\n    python scripts/plotcount.py \\\n        --input_file=results/last.dat \\\n        --output_file=results/figure/last.png\nresults/figure/sierra.png : scripts/plotcount.py results/sierra.dat\n    python scripts/plotcount.py \\\n        --input_file=results/sierra.dat \\\n        --output_file=results/figure/sierra.png\n\n# write the report\nreport/count_report.html : report/count_report.qmd figs\n    quarto render report/count_report.qmd\n\nclean-dats :\n    rm -f results/isles.dat \\\n        results/abyss.dat \\\n        results/last.dat \\\n        results/sierra.dat\n\nclean-figs :\n    rm -f results/figure/isles.png \\\n    results/figure/abyss.png \\\n    results/figure/last.png \\\n    results/figure/sierra.png\n\nclean-all : clean-dats \\\n    clean-figs\n    rm -f report/count_report.html\n    rm -rf report/count_report_files",
    "crumbs": [
      "automation.html",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data analysis pipelines with GNU Make</span>"
    ]
  },
  {
    "objectID": "lectures/180-pipelines-make.html#pattern-matching-and-variables-in-a-makefile",
    "href": "lectures/180-pipelines-make.html#pattern-matching-and-variables-in-a-makefile",
    "title": "18  Data analysis pipelines with GNU Make",
    "section": "18.6 Pattern matching and variables in a Makefile",
    "text": "18.6 Pattern matching and variables in a Makefile\nIt is possible to DRY out a Makefile and use variables.\nUsing wild cards and pattern matching in a makefile is possible, but the syntax is not very readable. So if you choose to do this proceed with caution. Example of how to do this are here: http://swcarpentry.github.io/make-novice/05-patterns/index.html\nAs for variables in a Makefile, in most cases we actually do not want to do this. The reason is that we want this file to be a record of what we did to run our analysis (e.g., what files were used, what settings were used, etc). If you start using variables with your Makefile, then you are shifting the problem of recording how your analysis was done to another file. There needs to be some file in your repo that captures what variables were called so that you can replicate your analysis. Examples of using variables in a Makefile are here: http://swcarpentry.github.io/make-novice/06-variables/index.html",
    "crumbs": [
      "automation.html",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data analysis pipelines with GNU Make</span>"
    ]
  },
  {
    "objectID": "lectures/190-packaging-and-documenting.html",
    "href": "lectures/190-packaging-and-documenting.html",
    "title": "19  Packaging and documenting code",
    "section": "",
    "text": "Learning Objectives",
    "crumbs": [
      "packaging-ci-cd-publish.html",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Packaging and documenting code</span>"
    ]
  },
  {
    "objectID": "lectures/190-packaging-and-documenting.html#learning-objectives",
    "href": "lectures/190-packaging-and-documenting.html#learning-objectives",
    "title": "19  Packaging and documenting code",
    "section": "",
    "text": "Explain the circumstances in which one should consider creating a package for their code\nUse available package development tools (e.g., usethis and devtools in R) to create a small, simple software package\nBe able to distinguish between essential software package elements (required to make a minimal software package) and those that are not essential but act to improve the user and developer experiences.\nDefine what a namespace is.\nExplain the role of the code within the following software package files:\n\n\nPython\n\npyproject.toml\n__init__.py\ndocs/*\n.github/workflows/ci-cd.yml\n\nR\n\nDESCRIPTION\nNAMESPACE\nman/*.Rd\n\n\n\nExplain at a high level how the Cookiecutter project template tool works.\nCompare and contrast how the Cookiecutter project template tool sets up a software package structure to how devtools and usethis set up a software package.\nGenerate well formatted function and package-level documentation for software packages using available tools (e.g., Roxygen2 and pkgdown in R, or numpy-style dosctrings and Sphinx in Python)",
    "crumbs": [
      "packaging-ci-cd-publish.html",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Packaging and documenting code</span>"
    ]
  },
  {
    "objectID": "lectures/190-packaging-and-documenting.html#when-to-start-writing-an-r-or-python-package",
    "href": "lectures/190-packaging-and-documenting.html#when-to-start-writing-an-r-or-python-package",
    "title": "19  Packaging and documenting code",
    "section": "19.1 When to start writing an R or Python package",
    "text": "19.1 When to start writing an R or Python package\nWhen your DRY radar goes off:\n\nIt depends on your level of patience for violating the DRY principle, for me the magic number is about 3. If I repeat writing code within a single script or notebook, on the third try hair starts to stand up on the back of my neck and I am irritated sufficiently to either write a function or a loop. The same goes with copying files containing functions that I would like to re-use. The third time that I do this, I am convinced that I will likely want to use this code again on another project and thus I should package it to make my life easier.\n\nWhen you think your code has general usefulness others could benefit from:\n\nAnother reason to package your code is that you think that it is a general enough solution that other people would find it useful. Doing this can be viewed as an act of altruism that also provides you professional gain by putting your data science skills on display (and padding your CV!).\n\nWhen you want to share data:\n\nThe R package ecosystem has found great use in packaging data in addition to Software. Packaging data is beyond the scope of this course, but you should know that it is widely done in R and that you can do it if you want or need to.\n\n\n\n\n\n\n\nOptional video\n\n\n\nI recommend watching a video from rstudio::conf 2020, called RMarkdown Driven Development by Emily Riederer, because I think it is such a nice narrative on how analysis code can evolve into software packages. Despite this talk being focused on R, it really applies to any data science language, and you could easily re-present this as “Jupyter Driven Development” or “Quarto Driven Development”.\nRMarkdown Driven Development by Emily Riederer (video)",
    "crumbs": [
      "packaging-ci-cd-publish.html",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Packaging and documenting code</span>"
    ]
  },
  {
    "objectID": "lectures/190-packaging-and-documenting.html#tour-de-packages",
    "href": "lectures/190-packaging-and-documenting.html#tour-de-packages",
    "title": "19  Packaging and documenting code",
    "section": "19.2 Tour de packages",
    "text": "19.2 Tour de packages\nWe will spend some time now exploring GitHub repositories of some Python and R packages to get more familiar with their internals. As we do this, fill out the missing data in the table below (which tries to match up the corresponding R and Python package key files and directories):\n\n\n\n\n\n\n\n\nDescription\nPython\nR\n\n\n\n\nOnline package repository or index\n\n\n\n\nDirectory for user-facing package functions\n\n\n\n\nDirectory for tests for user-facing package functions\n\n\n\n\nDirectory for documentation\n\n\n\n\nPackage metadata\n\n\n\n\nFile(s) that define the Namespace\n\n\n\n\nTool(s) for easy package creation\n\n\n\n\n\n\nNote: at the end of lecture I will review the answers to this table.",
    "crumbs": [
      "packaging-ci-cd-publish.html",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Packaging and documenting code</span>"
    ]
  },
  {
    "objectID": "lectures/190-packaging-and-documenting.html#tour-de-r-packages",
    "href": "lectures/190-packaging-and-documenting.html#tour-de-r-packages",
    "title": "19  Packaging and documenting code",
    "section": "19.3 Tour de R packages",
    "text": "19.3 Tour de R packages\nSome R packages you may want to explore:\n\nfoofactors - the end product of the Whole Game Chapter from the R Packages book.\nconvertempr - a simple example package I built as a demo for this course\nbroom - a tidyverse R package\ntidyhydat - a reviewed ROpenSci R package\n\n\n19.3.1 usethis and the evolution of R packaging\nWhat constitutes an R package or its configuration files, has not changed in a long time. However the tools commonly used to build them have. Currently, the most straight forward, and easy way to build R packages now involves the use of two developer packages called usethis and devtools.\n\nThese packages automate repetitive tasks that arise during project setup and development. It prevents you from having to do things like manually create boiler plate file and directory structures needed for building your R package structure, as well as simplifies the checking, installation and building of your package from source code.\nPackages created via usethis and devtools can be shared so that they can be installed via source from GitHub, or as package binaries on CRAN. For a package to be shared on CRAN, there is a check and gatekeeping system (in contrast to PyPI). We will learn more about this in later parts of the course.\n\nFun fact! Jenny Bryan, a past UBC Statistics Professor UBC MDS founder, is the maintainer for the usethis R package!\n\n\n\n19.3.2 Learn more by building a toy R package\nFor individual assignment 5, you will build a toy R package using a tutorial Jenny Bryan put together: The Whole Game\nAfter building the toy package, read Package structure and state to deepen your understanding of what packaging means in R and what packages actually are.",
    "crumbs": [
      "packaging-ci-cd-publish.html",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Packaging and documenting code</span>"
    ]
  },
  {
    "objectID": "lectures/190-packaging-and-documenting.html#tour-de-python-packages",
    "href": "lectures/190-packaging-and-documenting.html#tour-de-python-packages",
    "title": "19  Packaging and documenting code",
    "section": "19.4 Tour de Python packages",
    "text": "19.4 Tour de Python packages\nSome Python packages you may want to explore:\n\npycounts_tb - the end product of How to Package a Python from the Python Packages book\nconvertempPy - a simple example package I built as a demo for this course\nPyWebCAT - a Pythonic way to interface with the NOAA National Ocean Service Web Camera Applications Testbed (WebCAT)\nlaserembeddings - a Python package (uses the Poetry approach to creating Python packages)\npandera - a reviewed PyOpenSci Python package (uses more traditional approach to creating Python packages)\n\n\n19.4.1 Poetry and the evolution of Python packaging\nIf you want to package your Python code and distribute for ease of use by others on the Python Packaging Index (PyPI) you need to convert it to a standard format called Wheel.\nPreviously, to do this it was standard to have 4 configuration files in your package repository:\n\nsetup.py\nrequirements.txt\nsetup.cfg\nMANIFEST.in\n\nIn 2016, a new PEP (518) was made. This PEP defined a new configuration format, pyproject.toml, so that now a single file can be used in place of those previous four. This file must have at least two sections, [build-system] and [tool].\nThis new single configuration file has inspired some new tools to be created in the package building and management ecosystem in Python. One of the most recent and simplest to use is Poetry (created in 2018). When used to build packages, Poetry roughly does two things:\n\nUses the pyproject.toml to manage and solve package configurations via the Poetry commands init, add, config, etc\nCreates a lock file (poetry.lock) which automatically creates and activates a virtual environment (if none are activated) where the Poetry commands such as install, build, run, etc are executed.\n\nCookiecutter templates are also useful to help setup the biolerplate project and directory structure needed for packages. We will use these in this course as well.\n\n\n19.4.2 Learn more by building a toy Python package\nFor an optional/bonus part of invidividual assignment 5, you can choose to build a toy Python package using a tutorial we have put together for you: How to package a Python\nAfter building the toy package, read Package structure and state to deepen your understanding of what packaging means in Python and what packages actually are.",
    "crumbs": [
      "packaging-ci-cd-publish.html",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Packaging and documenting code</span>"
    ]
  },
  {
    "objectID": "lectures/190-packaging-and-documenting.html#key-package-things-for-python-and-r-packages",
    "href": "lectures/190-packaging-and-documenting.html#key-package-things-for-python-and-r-packages",
    "title": "19  Packaging and documenting code",
    "section": "19.5 Key package things for Python and R packages",
    "text": "19.5 Key package things for Python and R packages\n\n\n\n\n\n\n\n\nDescription\nPython\nR\n\n\n\n\nOnline package repository or index\nPyPI\nCRAN\n\n\nDirectory for user-facing package functions\nPackage directory (a named directory within the project root that contians .py files and a __init__.py file)\nR directory\n\n\nDirectory for tests for user-facing package functions\ntests\ntests/testthat\n\n\nDirectory for documentation\ndocs\nman and vignettes\n\n\nPackage metadata\npyproject.toml\nDESCRIPTION\n\n\nFile(s) that define the Namespace\n__init__.py\nNAMESPACE\n\n\nTool(s) for easy package creation\nCookiecutter & Poetry\nusethis & devtools",
    "crumbs": [
      "packaging-ci-cd-publish.html",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Packaging and documenting code</span>"
    ]
  },
  {
    "objectID": "lectures/190-packaging-and-documenting.html#what-is-a-minimal-viable-software-package",
    "href": "lectures/190-packaging-and-documenting.html#what-is-a-minimal-viable-software-package",
    "title": "19  Packaging and documenting code",
    "section": "19.6 What is a minimal viable software package?",
    "text": "19.6 What is a minimal viable software package?\nIn this course, we are learning to build software packages in R and Python to a high standard, with regards to: - code robustness - documentation - collaboration strategies\nThis is really great, but that means there are a LOT of files to track and learn about. What can be helpful is to know and be able to identify which files are essential to make a locally installable software package.\n\n19.6.1 Essential Python package files\nUsing the project layout we recommend for this course, here is a Python package structure with only the most essential files.\npkg\n├── pyproject.toml\n└── src\n    └── pkg\n        ├── __init__.py\n        └── module.py\nWhat do each of these do?\n\npyproject.toml stores all the metadata and install instructions for the package.\nThe presence of the src directory defines the code that will form the installable version of your package (meaning that users only get this code when they install the package, and that developers must install the package before running their tests).\n__init__.py tells Python that a directory is a package. They can be left empty, or they can also be used to add objects to the package’s namespace, provide documentation, and/or run other initialization code.\nmodule.py (this one can be named something else!) contains the functions you would like to share with your package users.\n\n\nGetting to know a pyproject.toml file’s contents\nHere is an example pyproject.toml file:\n[tool.poetry]                                          ┐\nname = \"pycounts_tt_2024\"                              │\nversion = \"0.1.0\"                                      │ Package metadata\ndescription = \"Calculate word counts in a text file!\"  │\nauthors = [\"Tiffany Timbers\"]                          │\nlicense = \"MIT\"                                        │\nreadme = \"README.md\"                                   ┘\n\n[tool.poetry.dependencies]                             ┐\npython = \"&gt;=3.9\"                                       │ User function dependencies\nmatplotlib = \"&gt;=3.8.2\"                                 ┘\n\n[tool.poetry.dev-dependencies]                         ┐\n                                                       │\n[tool.poetry.group.dev.dependencies]                   │\npytest = \"&gt;=7.4.4\"                                     │\npytest-cov = \"&gt;=4.1.0\"                                 │ Developer dependencies\njupyter = \"&gt;=1.0.0\"                                    │\nmyst-nb = {version = \"^1.0.0\", python = \"^3.9\"}        │\nsphinx-autoapi = \"^3.0.0\"                              │\nsphinx-rtd-theme = \"^2.0.0\"                            ┘\n\n[build-system]                                         ┐\nrequires = [\"poetry-core&gt;=1.0.0\"]                      │ Package build dependencies\nbuild-backend = \"poetry.core.masonry.api\"              ┘\nAlmost everything inside this file is customizable based on your package. Even the build system! pyproject.toml can also be used with other build tools such as setuptools and flit! If you are interested in learning more, checkout the Python Packaging Tools guide from PyOpenSci.\n\n\nGetting to know a __init__.py file’s contents\nAs mentioned earlier, this file can serve it’s essential package function of telling Python that this directory is a package by just existing and being empty. It can however serve other purposes. Let’s discuss the code that exists in the py-pkgs-cookiecutter template:\nThe py-pkgs-cookiecutter we used to create our pycounts package (Section 3.2.2) already populated our __init__.py file with this code for us:\n# read version from installed package\nfrom importlib.metadata import version\n__version__ = version(\"pycounts\")\nBecause this code is in __init__.py is executed when the package is imported (this is the case for all __init__.py files, we can ask for the package version with Python code like this:\nimport pycounts\npycounts.__version__\n0.1.0\nIn the code in __init__.py we get the version from the package metadata, that is derived from pyproject.toml. This allows us to only have to update the version of our package in one place when we bump to a new version.\nWhat other kinds of interesting things can we do with __init__.py? Well we can use it to control the import behavior of a package. For example, there are currently only two main functions that users will commonly use from our pycounts package: pycounts.count_words() and plotting.plot_words(). Users have to type the full path to these functions to import them:\nfrom pycounts.pycounts import count_words\nfrom pycounts.plotting import plot_words\nIf we import those core functions in pycounts’s __init__.py file, which would bind them to the package namespace, we can make our user’s experience of loading them easier!\n# read version from installed package\nfrom importlib.metadata import version\n__version__ = version(__name__)\n\n# populate package namespace\nfrom pycounts.pycounts import count_words\nfrom pycounts.plotting import plot_words\nThe functions are now bound to the pycounts namespace, so users can access them like this:\nimport pycounts\npycounts.count_words\n&lt;function count_words&gt;\n\nWhat is a namespace again? A “namespace” is a set of names that are used to identify and refer to objects of various kinds (source: Wikipedia).\n\n\n\n19.6.2 docs/* and .github/workflows/ci-cd.yml\nWe will cover both these topics in greater detail later (docs/* in the documentation chapter/lecture and .github/workflows/ci-cd.yml in the continuous integration and deployment chapter/lecture), but for now it’s helpful to get a bit better of an idea of these files and what they are doing.\n\ndocs/*\nTo keep things organized, to and to make the package documentation easier to automate, we keep the documentation files in the docs directory and use a Makefile (or make.bat on Windows) therein to automate their rendering.\ndocs\n   ├── changelog.md\n   ├── conduct.md\n   ├── conf.py\n   ├── contributing.md\n   ├── example.ipynb\n   ├── index.md\n   ├── make.bat\n   └── requirements.txt (deprecated)\nNote that many of these files also commonly exist in the root of projects and people expect to find them there!!! These include:\n\nchangelog.md\nconduct.md\ncontributing.md\n\nSo what we have done is pointed the doc/*.md’s of these files to the contents these files in the project root. And at rendering, their content will be used instead! If you look at the raw source of one of these files (e.g., conduct.md) you will see:\n\nThis helps keep our docs up to date by not having two places to update them!\nconf.py is a configuration file that specifies style and formatting for the documentation. In this course we will not be modifying it, but if you wanted you can! To do this, please first read the docs for Sphinx configuration here.\nThe requirements.txt file in the pypkgs-cookiecutter template is now deprecated and will be removed in a future version. All dependencies are now managed by pyproject.toml.\n\n\n.github/workflows/ci-cd.yml\nThis file contains automation code for continuous integration and delivery of our Python software package, reading the steps in this file you will see that it does the following:\n\nContinuous integration tasks:\n\n\ninstalls your package\nruns the test suite\ncalculates test coverage\n\n\nContinous deployment tasks:\n\n\nbumps the package software version, as well as prepares and creates a GitHub release of the package\nbuilds the package\npublishes it to the package repository (i.e., TestPyPI or PyPI, or both)\n\nWhen we first start building our packages, the GitHub Actions workflows for most of these things will fail. That is to be expected! As you complete the tasks for setting each of these up (e.g., writing tests and package code, setting up authentication to PyPI using GitHub secrets, etc) then the GitHub Actions workflows should start passing. This is a consequence of the philosophy of the pypkgs-cookiecutter template tool.\n\n\n\n19.6.3 Essential R package files\nUsing the project layout we recommend for this course, here is a R package structure with only the most essential files.\npkg\n├── DESCRIPTION\n├── man\n│   ├── functionA.Rd\n│   └── functionB.Rd\n├── NAMESPACE\n└── R\n    └── functions.R\nWhat do each of these do?\n\nDESCRIPTION stores all the metadata and dependency installation instructions for the package.\nThe man directory contains the function documentation in .Rd files (one per function). The contents are from these are generated from the ROxygen2 function documentation in the R/*.R files.\nUnsurprisingly, the NAMESPACE file is important in defining your package’s namespace.\nR is the directory for the *.R files which contain your exported (i.e., user-facing) functions.\nfunctions.R (this one can be named something else!) contains the functions you would like to share with your package users.\n\n\nGetting to know a DESCRIPTION file’s contents\nHere is an example DESCRIPTION file:\nPackage: foofactors                                                         ┐\nTitle: Make Factors Less Aggravating                                        │\nVersion: 0.0.0.9000                                                         │\nAuthors@R:                                                                  │\n    person(given = \"Tiffany\",                                               │ Package metadata\n           family = \"Timbers\",                                              │\n           role = c(\"aut\", \"cre\"),                                          │\n           email = \"tiffany.timbers@gmail.com\",                             │\n           comment = c(ORCID = \"0000-0002-2667-376X\"))                      │\nDescription: Factors have driven people to extreme measures, like ordering  │\n    custom conference ribbons and laptop stickers to express how HELLNO we  │\n    feel about stringsAsFactors. And yet, sometimes you need them. Can they │\n    be made less maddening? Let's find out.                                 │\nLicense: MIT + file LICENSE                                                 │\nEncoding: UTF-8                                                             │\nLazyData: true                                                              ┘\nRoxygen: list(markdown = TRUE)                                              ┐\nRoxygenNote: 7.0.2                                                          │ Developer dependencies\nSuggests:                                                                   │\n    testthat (&gt;= 2.1.0),                                                    │\n    covr                                                                    ┘\nImports:                                                                    ┐ User function dependencies\n    forcats                                                                 ┘\nURL: https://github.com/ttimbers/foofactors                                 ┐ More package metadata\nBugReports: https://github.com/ttimbers/foofactors/issues                   ┘\nThis is equivalent to the pyproject.toml file in Python packages. Again, almost everything in it is customizable based on your package’s specifics.\n\n\nman/*Rd files\nThe man directory contains the function documentation in .Rd files (one per function). These can be created from the function’s roxygen2 documentation using devtools::document. They use a custom syntax that is loosely based on LaTeX, which can be rendered to different formats for sharing with the package users.\nFor example, this roxygen2 documention in R/fbind.R:\n#' Bind two factors\n#'\n#' Create a new factor from two existing factors, where the new factor's levels\n#' are the union of the levels of the input factors.\n#'\n#' @param a factor\n#' @param b factor\n#'\n#' @return factor\n#' @export\n#' @examples\n#' fbind(iris$Species[c(1, 51, 101)], PlantGrowth$group[c(1, 11, 21)])\ngives this syntax (loosely based on LaTeX) in man/bind.Rd:\n% Generated by roxygen2: do not edit by hand\n% Please edit documentation in R/fcount.R\n\\name{fcount}\n\\alias{fcount}\n\\title{Make a sorted frequency table for a factor}\n\\usage{\nfcount(x)\n}\n\\arguments{\n\\item{x}{factor}\n}\n\\value{\nA tibble\n}\n\\description{\nMake a sorted frequency table for a factor\n}\n\\examples{\nfcount(iris$Species)\n}\n\n\nNAMESPACE file\nIn it are commands applied to R objects. Common commands include indicating that a function should be exported from your package, and/or that a another package should be imported to be used internally. The contents of this file are best to be created automatically using devtools to pull this information from the package function’s roxygen2 documentation (exports) and the DESCRIPTION file (imports).\nHere’s an example NAMESPACE file:\n# Generated by roxygen2: do not edit by hand\n\nexport(compare)\nexport(expect_equal)\nimport(rlang)\nimportFrom(brio,readLines)\n\n\n\n19.6.4 Cookiecutter project templates\nCookiecutter is a general Python tool for repeatedly generating a desired file and directory structure.\nMany many many Python Cookiecutter templates already exist, this search on GitHub (linked to from the Cookiecutter documentation) identifies 9000 repositories!!!\n\nhttps://github.com/search?q=cookiecutter&amp%3Btype=Repositories&type=repositories\n\nHow does it work? It uses the concept of code templating and uses the Jinja templating engine. Special syntax (involving {{) is used to specify parts of the code that should be “filled in” with user input (from a function call if interacting directly with the Jinja API, or the keyboard if using Cookiecutter). The input is then used to replace the placeholder text in the code (denoted by the special syntax involving {{). This allows template code to be automatically filled in and reused again and again!\n\n\n19.6.5 Exercise:\nLet’s explore the pypkgs-cookiecutter to get to know how the Cookiecutter project template tool works in the case of pypkgs-cookiecutter, and in general!\n\nGo to: https://github.com/py-pkgs/py-pkgs-cookiecutter\nLooks at: https://github.com/py-pkgs/py-pkgs-cookiecutter/blob/main/cookiecutter.json\nLook for all the places you see { cookiecutter.__SOMETHING }\n\n\n\n19.6.6 Differences in package building tool philosophy\n\niClicker question:\nSetting the programming language differences aside (and the technical issues with the pypkgs-Cookiecutter at the beginning of last week), which approach to package building did you prefer?\nA. Adding package components step-by-step as needed, like you did when using devtools/usethis to create an R package.\nB. Setting up the project template in one fell swoop (i.e., once at the beginning of the project), like you did when using pypkgs-Cookiecutter to create an R package.\n\n\nIn class disucssion\n\nWhat are the advantages and disadvantages of the devtools/usethis software package building approach in R?\n\n\nAdvantages:\n\nNot overwhelming for beginners, since you just add one thing at a time.\nAllows you to redo the one thing you made a mistake on.\n\nDisadvantages:\n\nEasy to forget to add or do somethings, as you need to run a different command each time.\nSomewhat inefficient as a command needs to be run each time you want to add a new feature to the package.\n\n\n\nWhat are the advantages and disadvantages of the pypkgs-Cookiecutter software package building approach in Python?\n\n\nAdvantages:\n\nMinimizes forgetting to dd or do somethings, because you just have one setup step at the beginning.\nEfficient because you just have one setup step at the beginning.\n\nDisadvantages:\n\nCan be overwhelming for beginners, since you generate a large number of files at one time.\nTakes a lot of time to when you make a mistake (e.g., typo in package name) because you need to manually got back and fix things.\nSets everything up at the beginning, including the things you are not yet ready for (e.g., Continuous integration and deployment workflows on GitHub actions) and there are some annoying consequences of this (e.g., notifications about failed checks).",
    "crumbs": [
      "packaging-ci-cd-publish.html",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Packaging and documenting code</span>"
    ]
  },
  {
    "objectID": "lectures/190-packaging-and-documenting.html#dealing-with-other-package-dependencies-in-your-package",
    "href": "lectures/190-packaging-and-documenting.html#dealing-with-other-package-dependencies-in-your-package",
    "title": "19  Packaging and documenting code",
    "section": "19.7 Dealing with other package dependencies in your package",
    "text": "19.7 Dealing with other package dependencies in your package\n\n19.7.1 Dealing with package dependencies in R\n\nWhen we write code in our package that uses functions from other packages we need to import those functions from the namespace of their packages.\nIn R, we do this via use_package, which adds that package to the “Imports” section of DESCRIPTION\nWe also need to refer to that package in our function code, there are two ways to do this:\n\nrefer the function by package::fun_name (e.g., dplyer::filter) whenever you use the function in your code\nadd the function to your packages namespace so that you can just refer to it as your normally would. To do this add @importFrom &lt;package_name&gt; &lt;function_or_operator&gt; to the Roxygen documentation of the function that you would like to use the function from the other package in and then use document() to update the DESCRIPTION and NAMESPACE file.\n\n\nIt is recommended to use method 1 (pkg::fun_name) because it is more explicit on what external functions your package code depends on (making it easier to read for collaborators, including future you). The trade off is that it’s a little more work to write.\n\n\n19.7.2 The pipe, %&gt;%, a special dependency\n\nGiven the ubiquity of the pipe operator, %&gt;%, in R, there is a function that automates exposing to your entire package: usethis::use_pipe()\nNote, in general, the tidyverse team does not recommend using the pipe in packages unless they are personal, low use packages, or “pro” packages with a lot of data wrangling because:\n\nIn most cases it is really an unnecessary dependency\nIt is not that readable for folks who do not typically use the pipe\nmakes debugging more challenging\n\n\nSo, should you use the pipe in your package? The answer is, it depends on your package’s scope, aims and goals. Also, this is probably your first package, so it doesn’t have to be perfect. If using the pipe helps you get the job done this time around, go for it. Just know that if you aim to ever build a widely used package, you probably do not want to depend on it.\n\nNote: with the advent of the base R pipe |&gt;, you can now use that pipe and no longer have to add it as a dependency!",
    "crumbs": [
      "packaging-ci-cd-publish.html",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Packaging and documenting code</span>"
    ]
  },
  {
    "objectID": "lectures/190-packaging-and-documenting.html#package-documentation-for-r",
    "href": "lectures/190-packaging-and-documenting.html#package-documentation-for-r",
    "title": "19  Packaging and documenting code",
    "section": "19.8 Package documentation for R",
    "text": "19.8 Package documentation for R\nThere are several levels of documentation possible for R packages: - code-level documentation (Roxygen-style comments) - vignettes - package websites (via pkgdown)\n\n19.8.1 Code-level documentation (Roxygen-style comments)\n\nWe learned the basics of how to write Roxygen-style comments in DSCI 511\nIn the package context, there are Namespace tags you should know about:\n\n@export - this should be added to all package functions you want your user to know about\n@NoRd - this should be added to helper/internal helper functions that you don’t want your user to know about\n\n\n\n\n19.8.2 Vignettes\nIt is common for packages to have vignettes (think demos with narratives) showing how to use the package in a more real-world scenario than the documentation examples show. Think of your vignette as a demonstration of how someone would use your function to solve a problem.\n\nIt should demonstrate how the individual functions in your package work, as well as how they can be integrated together.\nTo create a template for your vignette, run: usethis::use_vignette(\"package_name-vignette\")\nAdd content to that file and knit it when done.\n\nAs an example, here’s the dplyr vignette: https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html\n\n\n19.8.3 Package websites (via pkgdown)\n\nVignettes are very helpful, however they are not that discoverable by others, websites are a much easier way to share your package with others.\nThe pkgdown R package let’s you build a beautiful website for your R package in 4 steps!\n\nTurn on GitHub pages in your package repository, setting main branch / docs folder as the source.\nInstall pkgdown: `install.packages(“pkgdown”)\nRun pkgdown::build_site() from the root of your project, and commit and push the changes made by this.\nPush your code, including the docs directory to GitHub.com\n\n\nIn addition to the beautiful website, pkgdown automatically links to your vignette under the articles section of the website!!! 🎉🎉🎉\n\nNote you can also configure a GitHub Actions workflow to automate the rebuilding of the pkgdown site anytime changes are pushed to your package’s GitHub repository. We will discuss this later in the course under the topic of continuous deployment.",
    "crumbs": [
      "packaging-ci-cd-publish.html",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Packaging and documenting code</span>"
    ]
  },
  {
    "objectID": "lectures/200-continuous-integration.html",
    "href": "lectures/200-continuous-integration.html",
    "title": "20  Automated testing and continuous integration",
    "section": "",
    "text": "20.1 Topic learning objectives\nBy the end of this topic, students should be able to:",
    "crumbs": [
      "packaging-ci-cd-publish.html",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Automated testing and continuous integration</span>"
    ]
  },
  {
    "objectID": "lectures/200-continuous-integration.html#topic-learning-objectives",
    "href": "lectures/200-continuous-integration.html#topic-learning-objectives",
    "title": "20  Automated testing and continuous integration",
    "section": "",
    "text": "Argue the costs and benefits of using automated test infrastructure\nUse a testing framework (e.g., testhat) to automate the running of a project’s entire test suite\nUse a continuous integration (e.g., GitHub Actions) to automate the running of the test suite using the testing framework\nIncorporate non-code-functionality tests such as style/format checkers into a testing pipeline",
    "crumbs": [
      "packaging-ci-cd-publish.html",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Automated testing and continuous integration</span>"
    ]
  },
  {
    "objectID": "lectures/200-continuous-integration.html#continuous-integration-ci",
    "href": "lectures/200-continuous-integration.html#continuous-integration-ci",
    "title": "20  Automated testing and continuous integration",
    "section": "20.2 Continuous Integration (CI)",
    "text": "20.2 Continuous Integration (CI)\nDefined as the practice of frequently integrating code (e.g., several times a day) changes from contributors to a shared repository. Often the submission of code to the shared repository is combined with automated testing (and other things, such as style checking) to increase code dependability and quality.\n\n20.2.1 Why use CI + automated testing\n\ndetects errors sooner\nreduces the amount of code to be examined when debugging\nfacilitates merging\nensures new code additions do not introduce errors",
    "crumbs": [
      "packaging-ci-cd-publish.html",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Automated testing and continuous integration</span>"
    ]
  },
  {
    "objectID": "lectures/200-continuous-integration.html#github-actions",
    "href": "lectures/200-continuous-integration.html#github-actions",
    "title": "20  Automated testing and continuous integration",
    "section": "20.3 GitHub actions",
    "text": "20.3 GitHub actions\nA tool and service for automating software development tasks, located in the same place where you already store your code.\n\n20.3.1 Key concepts:\nActions: Individual tasks you want to perform.\nWorkflow: A collection of actions (specified together in one file).\nEvent: Something that triggers the running of a workflow.\nRunner: A machine that can run the Github Action(s).\nJob: A set of steps executed on the same runner.\nStep: A set of commands or actions which a job executes.\n\n\n20.3.2 Examples of GitHub Actions\nYou have already interacted with GitHub Actions in this class! We used it to:\n\nGenerate the issues in the dsci-310-homework repo upon push to the “create” branch: https://github.com/UBC-DSCI/dsci-310-homework/blob/master/.github/workflows/create_issues.yml\nGenerate a pull request in the review-my-pull-request repo upon push to the “pr” branch: : https://github.com/ttimbers/review-my-pull-request/blob/master/.github/workflows/pr.yml\n\n\n\n20.3.3 Exercise: Getting to know GitHub Actions workflows\nWe are going to each create our own GitHub Actions workflow. This workflow is a very simple toy example where we run some echo shell commands to print things to the runner’s terminal.\n\nSteps:\n\nCreate a new public GitHub.com repository with a README.\nClick on the “Actions” tab\nClick on the first “Set up this workflow” button\n\nClick on the two green commit buttons to add this workflow file\nGo to the “Actions” tab and look at the build logs by following these instructions:\nClick on the message associated with the event that created the action:\n\nClick on the build link:\n\nClick on the arrow inside the build logs to expand a section and see the output of the action:\n\n\n\n\n\n20.3.4 GitHub Actions workflow file:\nA YAML file that lives in the .github/workflows directory or your repository which speciies your workflow.\n# This is a basic workflow to help you get started with Actions\n\nname: CI\n\n# Controls when the workflow will run\non:\n  # Triggers the workflow on push or pull request events but only for the main branch\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\n  # Allows you to run this workflow manually from the Actions tab\n  workflow_dispatch:\n\n# A workflow run is made up of one or more jobs that can run sequentially or in parallel\njobs:\n  # This workflow contains a single job called \"build\"\n  build:\n    # The type of runner that the job will run on\n    runs-on: ubuntu-latest\n\n    # Steps represent a sequence of tasks that will be executed as part of the job\n    steps:\n      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it\n      - uses: actions/checkout@v2\n\n      # Runs a single command using the runners shell\n      - name: Run a one-line script\n        run: echo Hello, world!\n\n      # Runs a set of commands using the runners shell\n      - name: Run a multi-line script\n        run: |\n          echo Add other actions to build,\n          echo test, and deploy your project.\nThe file above has: - 3 possible event triggers (push to main, pull request to main, or manual dispatch through the Actions tab) - runs on an Ubuntu OS - one job called “build” - 3 steps - the type of runner is ubuntu - the first step uses an action, and the following two steps run commands\n\n\n20.3.5 Commands vs actions\nSteps can consist commands or actions. Let’s spend some time to discuss what each of these are and how they differ.\n\n\n20.3.6 Commands\nSteps that use commands look like the one shown below. They consist of a name and a run parameter. The commands listed after run are run in the runner’s shell:\n- name: Run a one-line script\n      run: echo Hello, world!\nAs shown in the file above, we can run multiple commands in a step using the | character:\n- name: Run a multi-line script\n      run: |\n        echo Add other actions to build,\n        echo test, and deploy your project.\n\n\n20.3.7 Actions\nSteps that use actions look like the one shown below (which builds and publishes Docker containers). They always have a uses parameter, and often also have name and with parameters. The uses parameter specifies which action to use, and the with parameters provide arguments to those actions. The @master at the name of the uses line, specifies whether to use the version at the head of the actions default branch, or a specific version (e.g., @v2).\n- name: Publish to Registry\n      uses: elgohr/Publish-Docker-Github-Action@master\n      with:\n        name: myDocker/repository\n        username: ${{ secrets.DOCKER_USERNAME }}\n        password: ${{ secrets.DOCKER_PASSWORD }}\nActions commonly perform one task in a workflow. There are two ways to build actions, either using JavaScript or by creating a Docker container that runs a shell script. For the latter such actions are defined by: - a Dockerfile - a shell script to run inside the Docker container\n\nIn this course we will use actions built by others, but not build our own. That is beyond the scope of this course. However, if you are intersted in learning more, I point you to the documentation below.\n\n\nOptional:\nFor example, for the action above see its: - Dockerfile - endpoint.sh script - GitHub repo\nRead the docs here to learn how to build your own Docker container GitHub action: https://help.github.com/en/actions/building-actions/creating-a-docker-container-action\nRead the docs here to learn how to build your own JavaScript GitHub action: https://help.github.com/en/actions/building-actions/creating-a-javascript-action",
    "crumbs": [
      "packaging-ci-cd-publish.html",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Automated testing and continuous integration</span>"
    ]
  },
  {
    "objectID": "lectures/200-continuous-integration.html#case-study-a-simplified-version-of-the-r-check-release.yaml-workflow",
    "href": "lectures/200-continuous-integration.html#case-study-a-simplified-version-of-the-r-check-release.yaml-workflow",
    "title": "20  Automated testing and continuous integration",
    "section": "20.4 Case study: a simplified version of the R check-release.yaml workflow:",
    "text": "20.4 Case study: a simplified version of the R check-release.yaml workflow:\n\nLet’s break down a simplified and well annotated version of the R check-release.yaml workflow file (full workflow here) to better understand a real use case of GitHub Actions.\n\non: [push, pull_request]\n\nname: R-CMD-check\n\njobs:\n  R-CMD-check:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout files from GitHub version control repository\n        uses: actions/checkout@v2\n\n      - name: Setup R\n        uses: r-lib/actions/setup-r@v2\n\n      - name: Install R packages\n        uses: r-lib/actions/setup-r-dependencies@v2\n        with:\n          extra-packages: any::rcmdcheck\n          needs: check\n\n      - name: Checks if R package can be installed\n        uses: r-lib/actions/check-r-package@v2\n\n20.4.1 Exercise: Orientating ourselves with the check-release.yaml workflow\nLet’s answer the following questions to start better understanding the check-release.yaml workflow.\n\nHow many jobs are there?\nHow many steps are there?\nWhat which steps are actions and which are commands\nWhat is the type of runner\nWhat events trigger this workflow?\n\n\n\n20.4.2 Exercise: Adding the check-release.yaml workflow to your R package repository\nThere are two ways you can do this. Manually via the GitHub interface:\n\nGo to the actions tab for your GitHub repository\nClick on “set up a workflow yourself”\nDelete the template action provided to you, and paste the check-release.yaml file above into the text editor. Rename the file check-release.yaml.\nClick “Start commit”, enter a commit message and then click “Commit”.\n\nOr using the usethis::use_github_action package convenience function:\n\nIn the R console run usethis::use_github_action(\"check-release\")\nPut the new check-release.yaml file in .github/workflows under local version control and push the commit to GitHub.",
    "crumbs": [
      "packaging-ci-cd-publish.html",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Automated testing and continuous integration</span>"
    ]
  },
  {
    "objectID": "lectures/200-continuous-integration.html#storing-and-use-github-actions-credentials-safely-via-github-secrets",
    "href": "lectures/200-continuous-integration.html#storing-and-use-github-actions-credentials-safely-via-github-secrets",
    "title": "20  Automated testing and continuous integration",
    "section": "20.5 Storing and use GitHub Actions credentials safely via GitHub Secrets",
    "text": "20.5 Storing and use GitHub Actions credentials safely via GitHub Secrets\nSome of the tasks we want to do in our workflows require authentication. However, the whole point of this is to automate this process - so how can we do that without sharing our authentication tokens, usernames or passwords in our workflow files?\nGitHub Secrets is the solution to this!\nGitHub Secrets are encrypted environment variables that are used only with GitHub Actions, and specified on a repository-by-repository basis. They can be accessed in a workflow file via: ${{ secrets.SECRET_NAME }}\nSee GitHub’s help docs for how to do this: https://help.github.com/en/actions/configuring-and-managing-workflows/creating-and-storing-encrypted-secrets",
    "crumbs": [
      "packaging-ci-cd-publish.html",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Automated testing and continuous integration</span>"
    ]
  },
  {
    "objectID": "lectures/200-continuous-integration.html#authenticating-with-the-github_token",
    "href": "lectures/200-continuous-integration.html#authenticating-with-the-github_token",
    "title": "20  Automated testing and continuous integration",
    "section": "20.6 Authenticating with the GITHUB_TOKEN",
    "text": "20.6 Authenticating with the GITHUB_TOKEN\nWhat if you need to do Git/GitHub things in your workflow? Like checkout your files to run the tests? Create a release? Open an issue? To help with this GitHub automatically (i.e., you do not need to create this secret) creates a secret named GITHUB_TOKEN that you can access and use in your workflow. You access this token in your workflow file via:\n${{ secrets.GITHUB_TOKEN }}",
    "crumbs": [
      "packaging-ci-cd-publish.html",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Automated testing and continuous integration</span>"
    ]
  },
  {
    "objectID": "lectures/200-continuous-integration.html#creating-and-accessing-environment-variables-in-github-actions",
    "href": "lectures/200-continuous-integration.html#creating-and-accessing-environment-variables-in-github-actions",
    "title": "20  Automated testing and continuous integration",
    "section": "20.7 Creating and accessing environment variables in GitHub Actions",
    "text": "20.7 Creating and accessing environment variables in GitHub Actions\nSometimes our commands or actions need environment variables. In both of these scenarios, we create environment variables and access them within a step via:\nsteps:\n  - name: Hello env vars\n    run: echo $VARIABLE_NAME1 $VARIABLE_NAME2\n    env:\n      VARIABLE_NAME1: &lt;variable_value1&gt;\n      VARIABLE_NAME2: &lt;variable_value2&gt;",
    "crumbs": [
      "packaging-ci-cd-publish.html",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Automated testing and continuous integration</span>"
    ]
  },
  {
    "objectID": "lectures/200-continuous-integration.html#matrix-workflows",
    "href": "lectures/200-continuous-integration.html#matrix-workflows",
    "title": "20  Automated testing and continuous integration",
    "section": "20.8 Matrix workflows",
    "text": "20.8 Matrix workflows\nWe don’t want our software to just work on one operating system, or just one version of Python or R. Ideally it is compatible with the three major operating systems as well as a couple versions of the programming language it was written it.\nHow do we ensure this? Well, we could have several GitHub Action workflows, each of which runs the job on a different version of Python, on a different operating system. However, there would be a lot of redundancy in those workflows, with the only differences between them being the operating system of the runner and the version of Python.\nA more efficient way to do this with GitHub Actions workflows is to use matrix workflows. In these workflows, we use a matrix variable, which we specify as:\nstrategy:\n  matrix:\n    &lt;variable_name&gt;: [&lt;value1&gt;, &lt;value2&gt;]\nwhich we can refer to in the workflow steps as:\n${{ matrix.&lt;variable_name&gt; }}\nWhen we do this, GitHub Actions runs multiple jobs, one for each of the values in the matrix variable.\n\n20.8.1 Exercise: in English, what does this workflow file do?\nNow that we have some understanding of GitHub Actions workflows, let’s use that knowledge to write in English what each of the steps do in this more complicated version of the workflow shown above.\non:\n  push:\n    branches: [main, master]\n  pull_request:\n    branches: [main, master]\n\nname: R-CMD-check\n\njobs:\n  R-CMD-check:\n    runs-on: ${{ matrix.config.os }}\n\n    name: ${{ matrix.config.os }} (${{ matrix.config.r }})\n\n    strategy:\n      matrix:\n        config:\n          - {os: windows-latest,  r: 'devel'}\n          - {os: windows-latest, r: 'release'}\n          - {os: ubuntu-latest,   r: 'devel'}\n          - {os: ubuntu-latest,   r: 'release'}\n\n    steps:\n      - name: Checkout files from GitHub version control repository\n        uses: actions/checkout@v2\n\n      - name: Setup R\n        uses: r-lib/actions/setup-r@v2\n\n      - name: Install R packages\n        uses: r-lib/actions/setup-r-dependencies@v2\n        with:\n          extra-packages: any::rcmdcheck\n          needs: check\n\n      - name: Checks if R package can be installed\n        uses: r-lib/actions/check-r-package@v2\n\nSteps in English:\nFILL IN DURING CLASS\n\n\nHow many jobs are run? What does each do?\nFILL IN DURING CLASS",
    "crumbs": [
      "packaging-ci-cd-publish.html",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Automated testing and continuous integration</span>"
    ]
  },
  {
    "objectID": "lectures/200-continuous-integration.html#setting-up-github-actions-workflows-to-check-tests-and-test-coverage",
    "href": "lectures/200-continuous-integration.html#setting-up-github-actions-workflows-to-check-tests-and-test-coverage",
    "title": "20  Automated testing and continuous integration",
    "section": "20.9 Setting up GitHub Actions workflows to check tests and test coverage",
    "text": "20.9 Setting up GitHub Actions workflows to check tests and test coverage\n\nAdd the covr package as a suggested dependendency to your package via: usethis::use_package(\"covr\", type = \"Suggests\")\nAdd a GitHub Actions workflows that runs a comprehensive build check across the major operating systems and runs the test suite and calculates coverage via: usethis::use_github_action_check_standard() and usethis::use_github_action(\"test-coverage.yaml\")\nLink your R package GitHub repo with codecov.io\nCopy the codecov.io token for that repo from codecov.io and add that as a GitHub Secret named CODECOV_TOKEN\nAdd the codecov.io badge markdown syntax to your README.Rmd and knit to render the README.md file.\nPush your local changes to GitHub and sit back and watch the magic happen ✨",
    "crumbs": [
      "packaging-ci-cd-publish.html",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Automated testing and continuous integration</span>"
    ]
  },
  {
    "objectID": "lectures/200-continuous-integration.html#additional-resources",
    "href": "lectures/200-continuous-integration.html#additional-resources",
    "title": "20  Automated testing and continuous integration",
    "section": "20.10 Additional resources:",
    "text": "20.10 Additional resources:\n\n20.10.1 Github actions with R book\n\nhttps://ropenscilabs.github.io/actions_sandbox/\n\n\n\n20.10.2 GitHub Actions for the R community\n\nhttps://github.com/r-lib/actions\n\n\n\n20.10.3 GitHub Actions with Python\n\nhttps://py-pkgs.org/08-ci-cd#introduction-to-github-actions\n\n\n\n20.10.4 Curated list of awesome actions to use on GitHub 🎉\n\nhttps://github.com/sdras/awesome-actions",
    "crumbs": [
      "packaging-ci-cd-publish.html",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Automated testing and continuous integration</span>"
    ]
  },
  {
    "objectID": "lectures/210-deploy-and-publish.html",
    "href": "lectures/210-deploy-and-publish.html",
    "title": "21  Deploying and publishing packages",
    "section": "",
    "text": "21.1 Topic learning objectives\nBy the end of this topic, students should be able to:",
    "crumbs": [
      "packaging-ci-cd-publish.html",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Deploying and publishing packages</span>"
    ]
  },
  {
    "objectID": "lectures/210-deploy-and-publish.html#topic-learning-objectives",
    "href": "lectures/210-deploy-and-publish.html#topic-learning-objectives",
    "title": "21  Deploying and publishing packages",
    "section": "",
    "text": "Define continuous deployment and argue the costs and benefits of continuous deployment\nExplain semantic versioning, and define what constitutes patch, minor, major and breaking changes\nUse continuous deployment (e.g., GitHub Actions) to automate deployment of software packages, or components of software packages (e.g., documentation)\nPublish software packages to development and production package repositories (e.g., GitHub and CRAN)",
    "crumbs": [
      "packaging-ci-cd-publish.html",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Deploying and publishing packages</span>"
    ]
  },
  {
    "objectID": "lectures/210-deploy-and-publish.html#continuous-deployment-cd",
    "href": "lectures/210-deploy-and-publish.html#continuous-deployment-cd",
    "title": "21  Deploying and publishing packages",
    "section": "21.2 Continuous Deployment (CD)",
    "text": "21.2 Continuous Deployment (CD)\nDefined as the practice of automating the deployment of software that has successfully run through your test-suite.\nFor example, upon merging a pull request to master, an automation process builds the Python package and publishes to PyPI without further human intervention.\n\n21.2.1 Why use CD?\n\nlittle to no effort in deploying new version of the software allows new features to be rolled out quickly and frequently\nalso allows for quick implementation and release of bug fixes\ndeployment can be done by many contributors, not just one or two people with a high level of Software Engineering expertise\n\n\n\n21.2.2 Why use CD?\nPerhaps this story is more convincing:\nThe company, let’s call them ABC Corp, had 16 instances of the same software, each as a different white label hosted on separate Linux machines in their data center. What I ended up watching (for 3 hours) was how the client remotely connected to each machine individually and did a “capistrano deploy”. For those unfamiliar, Capistrano is essentially a scripting tool which allows for remote execution of various tasks. The deployment process involved running multiple commands on each machine and then doing manual testing to make sure it worked.\nThe best part was that this developer and one other were the only two in the whole company who knew how to run the deployment, meaning they were forbidden from going on vacation at the same time. And if one of them was sick, the other had the responsibility all for themselves. This deployment process was done once every two weeks.\nSource: Tylor Borgeson\nInfrequent & manual deployment makes me feel like this when it comes time to do it:\n\nand so it can become a viscious cycle of delaying deployment because its hard, and then making it harder to do deployment because a lot of changes have been made since the last deployment…\nSo to avoid this, we are going to do continuous deployment when we can! And where we can’t, we will automate as much as we can up until the point where we need to manually step in.",
    "crumbs": [
      "packaging-ci-cd-publish.html",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Deploying and publishing packages</span>"
    ]
  },
  {
    "objectID": "lectures/210-deploy-and-publish.html#examples-of-cd-being-used-for-data-science",
    "href": "lectures/210-deploy-and-publish.html#examples-of-cd-being-used-for-data-science",
    "title": "21  Deploying and publishing packages",
    "section": "21.3 Examples of CD being used for data science",
    "text": "21.3 Examples of CD being used for data science\n\nPython packages (but not R, more on this later…)\nPackage documentation (e.g., pkgdown websites for R, ReadtheDocs websites for Python)\nBooks and websites (e.g., jupyter-book, bookdown, distill websites, etc)",
    "crumbs": [
      "packaging-ci-cd-publish.html",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Deploying and publishing packages</span>"
    ]
  },
  {
    "objectID": "lectures/210-deploy-and-publish.html#conditionals-for-when-to-run-the-job",
    "href": "lectures/210-deploy-and-publish.html#conditionals-for-when-to-run-the-job",
    "title": "21  Deploying and publishing packages",
    "section": "21.4 Conditionals for when to run the job",
    "text": "21.4 Conditionals for when to run the job\nWe only want our cd job to run if certain conditions are true, these are:\n\nif the ci job passes\nif this is a commit to the main branch\n\nWe can accomplish this in our cd job be writing a conditional using the needs and if keywords at the top of the job, right after we set the permissions:\ncd:\n    permissions:\n      id-token: write\n      contents: write\n\n    # Only run this job if the \"ci\" job passes\n    needs: ci\n\n    # Only run this job if new work is pushed to \"main\"\n    if: github.event_name == 'push' && github.ref == 'refs/heads/main'\n\n21.4.1 Exercise: read the cd job of ci-cd.yml\nTo make sure we understand what is happening in our workflow that performs CD, let’s convert each step to a human-readable explanation:\n\nSets up Python on the runner\nCheckout our repository files from GitHub and put them on the runner\n…\n…\n…\n…\n…\n\n\nNote: I filled in the steps we went over last class, so you can just fill in the new stuff\n\n\n\n21.4.2 How can we automate version bumping?\nLet’s look at the first step that works towards accomplishing this:\n      - name: Use Python Semantic Release to prepare release\n        id: release\n        uses: python-semantic-release/python-semantic-release@v8.3.0\n        with:\n          github_token: ${{ secrets.GITHUB_TOKEN }}\nPython semantic-release is a Python tool which parses commit messages looking for keywords to indicate how to bump the version. It bumps the version in the pyproject.toml file.\nTo understand how it works so that we can use it, we need to understand semantic versioning and how to write conventional commit messages.\nLet’s unpack each of these on its own.",
    "crumbs": [
      "packaging-ci-cd-publish.html",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Deploying and publishing packages</span>"
    ]
  },
  {
    "objectID": "lectures/210-deploy-and-publish.html#semantic-versioning",
    "href": "lectures/210-deploy-and-publish.html#semantic-versioning",
    "title": "21  Deploying and publishing packages",
    "section": "21.5 Semantic versioning",
    "text": "21.5 Semantic versioning\n\nWhen we make changes and publish new versions of our packages, we should tag these with a version number so that we and others can view and use older versions of the package if needed.\nThese version numbers should also communicate something about how the underlying code has changed from one version to the next.\nSemantic versioning is an agreed upon “code” by developers that gives meaning to version number changes, so developers and users can make meaningful predictions about how code changes between versions from looking solely at the version numbers.\nSemantic versioning assumes version 1.0.0 defines the API, and the changes going forward use that as a starting reference.",
    "crumbs": [
      "packaging-ci-cd-publish.html",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Deploying and publishing packages</span>"
    ]
  },
  {
    "objectID": "lectures/210-deploy-and-publish.html#semantic-versioning-1",
    "href": "lectures/210-deploy-and-publish.html#semantic-versioning-1",
    "title": "21  Deploying and publishing packages",
    "section": "21.6 Semantic versioning",
    "text": "21.6 Semantic versioning\nGiven a version number MAJOR.MINOR.PATCH (e.g., 2.3.1), increment the:\n\nMAJOR version when you make incompatible API changes (often called breaking changes 💥)\nMINOR version when you add functionality in a backwards compatible manner ✨↩︎️\nPATCH version when you make backwards compatible bug fixes 🐞\n\nSource: https://semver.org/\n\n21.6.1 Semantic versioning case study\nCase 1: In June 2009, Python bumped versions from 3.0.1, some changes in the new release included: - Addition of an ordered dictionary type - A pure Python reference implementation of the import statement - New syntax for nested with statements\nCase 2: In Dec 2017, Python bumped versions from 3.6.3, some changes in the new release included:\n\nFixed several issues in printing tracebacks (PyTraceBack_Print()).\nFix the interactive interpreter looping endlessly when no memory.\nFixed an assertion failure in Python parser in case of a bad unicodedata.normalize()\n\nCase 3: In Feb 2008, Python bumped versions from 2.7.17, some changes in the new release included: - print became a function - integer division resulted in creation of a float, instead of an integer - Some well-known APIs no longer returned lists (e.g., dict.keys, dict.values, map)\n\n\n21.6.2 Exercise: name that semantic version release\nReading the three cases posted above, think about whether each should be a major, minor or patch version bump. Answer the chat when prompted.",
    "crumbs": [
      "packaging-ci-cd-publish.html",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Deploying and publishing packages</span>"
    ]
  },
  {
    "objectID": "lectures/210-deploy-and-publish.html#conventional-commit-messages",
    "href": "lectures/210-deploy-and-publish.html#conventional-commit-messages",
    "title": "21  Deploying and publishing packages",
    "section": "21.7 Conventional commit messages",
    "text": "21.7 Conventional commit messages\nPython Semantic Release by default uses a parser that works on the conventional (or Angular) commit message style, which is:\n&lt;type&gt;(optional scope): succinct description of the change\n\n(optional body: the motivation for the change and contrast this with previous behavior)\n\n(optional footer: note BREAKING CHANGES here, as well as any issues to be closed)\nHow to affect semantic versioning with conventional commit messages: - a commit with the type fix leads to a patch version bump - a commit with the type feat leads to a minor version bump - a commit with a body or footer that starts with BREAKING CHANGE: - these can be of any type (Note: currently Python Semantic release is broken for detecting these on commits with Windows line endings, wich the GitHub pen tool commits also use. The workaround fix is to use ! after feat, for example: feat!: This describes the new feature and breaking changes in addition to BREAKING CHANGES: ... in the footer.)\n\nNote - commit types other than fix and feat are allowed. Recommeneded ones include docs, style, refactor, test, ci and others. However, only fix and feat result in version bumps using Python Semantic Release.\n\n\n21.7.1 An example of a conventional commit message\ngit commit -m \"feat(function_x): added the ability to initialize a project even if a pyproject.toml file exists\"\nWhat kind of version bump would this result in?\n\n\n21.7.2 Another example of a conventional commit message\ngit commit -m \"feat!: change to use of `%&gt;%` to add new layers to ggplot objects\n\nBREAKING CHANGE: `+` operator will no longer work for adding new layers to ggplot objects after this release\"\nWhat kind of version bump would this result in?\n\n\n21.7.3 Some practical notes for usage in your packages:\n\nYou must add the following to the tool section of your pyproject.toml file for this to work (note: the pypkgs-cookiecutter adds this table if you choose to add ci-cd when you set it up):\n[tool.semantic_release]\nversion_toml = [\n    \"pyproject.toml:tool.poetry.version\",\n]                                                    # version location\nbranch = \"main\"                                      # branch to make releases of\nchangelog_file = \"CHANGELOG.md\"                      # changelog file\nbuild_command = \"pip install poetry && poetry build\" # build dists\nVersions will not be bumped if conventional commits are not used.\n\n\n\n21.7.4 Some practical notes for usage in your packages:\n\nAutomated version bumping can only work (as currently implemented in our cookiecutter) with versions in the pyproject.toml metadata (line 3). If you add a version elsewhere, it will not get bumped unless you specify the location the the [tool.semantic_release] table in pyproject.toml.\nIf you have been working with main branch protection, you will need to change something to use ci.yml work for continuous deployment. The reason for this, is that this workflow (which bumps versions and deploy the package) is triggered to run after the pull request is merged to main. Therefore, when we bump the versions in the pyproject.toml file we need to push these changes to the main branch - however this is problematic given that we have set-up main branch protection!\n\nWhat are we to do about #2?\n\nSolution 1:\nRemove main branch protection. This is not the most idealistic solution, however it is a simple and practical one.\n\n\nPossible solution 2:\n(I say possible because this has yet to be formally documented by PSR, and is still just reported in an issue: https://github.com/python-semantic-release/python-semantic-release/issues/311. I have tested it and it works for me, see example here: https://github.com/ttimbers/pycounts_tt_2024/blob/main/.github/workflows/ci-cd.yml)\n\nCreate a new GitHub PAT (see these docs) with the repo scope.\nUnder “Secrets”, click on “Settings and variables” &gt; “Actions”, and then under “Repository secrets” click “New repository secret” to create a new repository secret named RELEASE_TOKEN and the new GitHub PAT with repo scope as its value.\nEdit the to cd job steps shown below (from ci-cd.yml) use this token:\n\n...\n    - uses: actions/checkout@v3\n      with:\n        fetch-depth: 0\n        token: ${{ secrets.RELEASE_TOKEN }}\n...\n\n    - name: Use Python Semantic Release to prepare release\n      id: release\n      uses: python-semantic-release/python-semantic-release@v8.3.0\n      with:\n        github_token: ${{ secrets.RELEASE_TOKEN }}\n...\n\n\n\n21.7.5 Demo of Continous Deployment!\n\nhttps://github.com/ttimbers/pycounts_tt_2024\n\n\n\n21.7.6 What about CD with R packages\n\nThis is not a common practice (yet!). One reason for this could be that CRAN has a policy where they only want to see updates every 1-2 months.\nSemantic versioning is used in Tidyverse R packages, but creating versions is done manually",
    "crumbs": [
      "packaging-ci-cd-publish.html",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Deploying and publishing packages</span>"
    ]
  },
  {
    "objectID": "lectures/210-deploy-and-publish.html#publishing-your-r-package",
    "href": "lectures/210-deploy-and-publish.html#publishing-your-r-package",
    "title": "21  Deploying and publishing packages",
    "section": "21.8 Publishing your R package",
    "text": "21.8 Publishing your R package\n\n21.8.1 Level 1: publishing on GitHub\nFor this course, we will only publish your package on GitHub, not CRAN. For this to work, you need to push your package code to GitHub and provide users these instructions to download, build and install your package:\n# install.packages(\"devtools\")\ndevtools::install_github(\"ttimbers/convertempr\")\nThis is where almost all R packages start out publishing, and continue publishing development versions between releases.",
    "crumbs": [
      "packaging-ci-cd-publish.html",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Deploying and publishing packages</span>"
    ]
  },
  {
    "objectID": "lectures/210-deploy-and-publish.html#level-2-publishing-on-cran",
    "href": "lectures/210-deploy-and-publish.html#level-2-publishing-on-cran",
    "title": "21  Deploying and publishing packages",
    "section": "21.9 Level 2: publishing on CRAN",
    "text": "21.9 Level 2: publishing on CRAN\n\nCRAN (founded in 1997) stands for the “Comprehensive R Archive Network”\nit is a collection of sites which host identical copies of:\n\nR distribution(s)\nthe contributed extensions (i.e., packages)\ndocumentation for R\nbinaries (i.e., packages)\n\nas of 2012, there were 85 official ‘daily’ mirrors\n\nSource: Hornik, K (2012). The Comprehensive R Archive Network. Wiley interdisciplinary reviews. Computational statistics. 4(4): 394-398. doi:10.1002/wics.1212\n\n21.9.1 Binary vs source distributions, what’s the difference?\nBinary distributions are pre-compiled (computer readable), whereas source distributions have to be compiled before they are installed.\nPrecompiled binaries are often different for each operating system (e.g., Windows vs Mac)\n\n\n21.9.2 Number of packages hosted by CRAN over history\n\nSource: “Reproducibility and Replicability in a Fast-Paced Methodological World” by Sacha Epskamp\n\n\n21.9.3 What does it mean to be a CRAN package:\nA stamp of authenticity: - passed quality control of the check utility\nEase of installation: - can be installed by users via install.packages (it’s actually the default!) - binaries available for Windows & Mac OS’s\nDiscoverability: - listed as a package on CRAN\nHOWEVER - CRAN makes no assertions about the package’s usability, or the efficiency and correctness of the computations it performs\n\n\n21.9.4 How to submit a package to CRAN\n\nPick a version number.\nRun and document R CMD check.\nCheck that you’re aligned with CRAN policies.\nUpdate README.md and NEWS.md.\nSubmit the package to CRAN.\nPrepare for the next version by updating version numbers.\nPublicise the new version.\n\nSource: Chapter 18 Releasing a package - R packages book by Hadley Wickham & Jenny Bryan\n\n\n21.9.5 Notes on submitting to CRAN\n\nCRAN is staffed by volunteers, all of whom have other full-time jobs\nA typical week has over 100 submissions and only three volunteers to process them all.\nThe less work you make for them the more likely you are to have a pleasant submission experience…\n\n\n\n21.9.6 Notes on submitting to CRAN (cont’d)\nTechnical things:\n\nYour package must pass R CMD check with the current development version of R (R-devel)\nit must work on at least two platforms (CRAN uses the following 4 platforms: Windows, Mac OS X, Linux and Solaris) - use GitHub Actions to ensure this before submitting to CRAN!\n\nIf you decide to submit a package to CRAN follow the detailed instructions in Chapter 18 Releasing a package fromt the R packages book by Hadley Wickham & Jenny Bryan to do so. If you submit your package to rOpenSci, they will help you get everything in order for submission to CRAN as well!\nCRAN policies: https://cran.r-project.org/web/packages/policies.html\nMost common problems (from the R packages book):\n\nThe maintainer’s e-mail address must be stable, if they can’t get in touch with you they will remove your package from CRAN.\nYou must have clearly identified the copyright holders in DESCRIPTION: if you have included external source code, you must ensure that the license is compatible.\nDo not make external changes without explicit user permission. Don’t write to the file system, change options, install packages, quit R, send information over the internet, open external software, etc.\nDo not submit updates too frequently. The policy suggests a new version once every 1-2 months at most.\n\n\n\n21.9.7 If your submission fails:\nRead section 18.6.1 “On failure” from Chapter 18 Releasing a package - R packages book by Hadley Wickham & Jenny Bryan*\nTL;DR - Breathe, don’t argue, fix what is needed and re-submit.",
    "crumbs": [
      "packaging-ci-cd-publish.html",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Deploying and publishing packages</span>"
    ]
  },
  {
    "objectID": "lectures/210-deploy-and-publish.html#publishing-your-python-package",
    "href": "lectures/210-deploy-and-publish.html#publishing-your-python-package",
    "title": "21  Deploying and publishing packages",
    "section": "21.10 Publishing your Python package",
    "text": "21.10 Publishing your Python package\n\n21.10.1 Level 1: GitHub\nPackages can be installed from GitHub via pip:\npip install git+https://github.com/USERNAME/REPOSITORY.git\n\n\n21.10.2 Level 2: PyPI\nPackages can be installed from PyPI via:\npip install PACKAGE_NAME\n\nshould be pronounced like “pie pea eye”\nalso known as the Cheese Shop (a reference to the Monty Python’s Flying Circus sketch “Cheese Shop”)\n\n\nBecause level 2 is so easy, it is the most commonly used method.\n\n\nfrom IPython.display import YouTubeVideo\nYouTubeVideo('zB8pbUW5n1g')\n\n\n        \n        \n\n\nDon’t get the joke? I didn’t either without historical context. When PyPI was first launched it didn’t have many Python packages on it - similar to a cheese shop with no cheese 😆\n\n\n21.10.3 the Cheese Shop (er, PyPI)\n\nPyPI (founded in 2002) stands for the “Python Package Index”\nhosts Python packages of two different forms:\n\nsdists (source distributions)\nprecompiled “wheels (binaries)\n\nheavily cached and distributed\ncurrently contains &gt; 9000 projects\n\n\n\n21.10.4 Number of packages hosted by PyPI over history\n\nSource: “Ecosystem-level determinants of sustained activity in open-source projects: a case study of the PyPI ecosystem” by Marat Valiev, Bogdan Vasilescu & James Herbsleb\n\n\n21.10.5 What does it mean to be a PyPI package:\nEase of installation: - can be installed by users via pip install (it’s actually the default!) - universal binaries available for packages that are written solely in Python\nDiscoverability: - listed as a package on PyPI\nHOWEVER, there is no required check for your package is required to pass… As long as you can bundle it as something that PyPI recognizes as an sdist or wheels then it can go on PyPI… This allows the process to be fully automated, but QC is lower than it is for CRAN.\n\n\n21.10.6 How to submit a package to PyPI\nSee the “How to package a Python” chapter of the Python Packages book.\n\n\n21.10.7 Points for discussion\n\nIs one model better or worse?\nImportance & complimentarity of organizations like rOpenSci & pyOpenSci with CRAN and PyPI, respecitively",
    "crumbs": [
      "packaging-ci-cd-publish.html",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Deploying and publishing packages</span>"
    ]
  },
  {
    "objectID": "lectures/210-deploy-and-publish.html#peer-review-facilitates-package-publishing",
    "href": "lectures/210-deploy-and-publish.html#peer-review-facilitates-package-publishing",
    "title": "21  Deploying and publishing packages",
    "section": "21.11 Peer review facilitates package publishing",
    "text": "21.11 Peer review facilitates package publishing\n\n21.11.1 rOpenSci\n\naims and goals:\nrOpenSci fosters a culture that values open and reproducible research using shared data and reusable software.\nWe do this by: - Creating technical infrastructure in the form of carefully vetted, staff- and community-contributed R software tools that lower barriers to working with scientific data sources on the web\n\nCreating social infrastructure through a welcoming and diverse community\nMaking the right data, tools and best practices more discoverable\nBuilding capacity of software users and developers and fostering a sense of pride in their work\nPromoting advocacy for a culture of data sharing and reusable software.\n\nSource: https://ropensci.org/about/\n\n\n\n21.11.2 rOpenSci’s open peer review process\n\nAuthors submit complete R packages to rOpenSci.\nEditors check that packages fit into rOpenSci’s scope, run a series of automated tests to ensure a baseline of code quality and completeness, and then assign two independent reviewers.\nReviewers comment on usability, quality, and style of software code as well as documentation.\nAuthors make changes in response.\nOnce reviewers are satisfied with the updates, the package receives a badge of approval and joins rOpenSci’s suite of approved pacakges.\nHappens openly, and publicly on GitHub in issues.\nProcess is quite iterative and fast. After reviewers post a first round of extensive reviews, authors and reviewers chat in an informal back-and-forth, only lightly moderated by an editor.\n\nSource: https://numfocus.org/blog/how-ropensci-uses-code-review-to-promote-reproducible-science\n\n\n21.11.3 rOpenSci’s Guidance and Standards\nWhat aspects of a package are reviewed?\n\nhigh-level best practices:\n\nis the code reusable (e.g. follow the DRY principle)?\nare sufficient edge cases tested?\netc\n\nlow-level standards:\n\nare naming conventions for functions followed?\ndid they make the best choices of dependencies for the package’s intended tasks?\netc\n\n\nSource: https://numfocus.org/blog/how-ropensci-uses-code-review-to-promote-reproducible-science\n\n\n21.11.4 rOpenSci’s Review Guidebook\n\nhttps://devguide.ropensci.org/\n\n\n\n21.11.5 rOpenSci-reviewed packages:\n\nhttps://ropensci.org/packages/\n\n\n\n21.11.6 Let’s look at an rOpenSci review!\nAll packages currently under review: https://github.com/ropensci/software-review/issues\n\nReview of tidypmc\n\n\n\n21.11.7 What do you get for having your package reviewed by rOpenSci?\n\nvaluable feedback from the knowledgeable editors and reviewers\nhelp with package maintenance and submission of your package to CRAN\npromotion of your package on their website, blog and social media\npackages that have a short accompanying paper can be automatically submitted to JOSS and fast-tracked for publication.",
    "crumbs": [
      "packaging-ci-cd-publish.html",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Deploying and publishing packages</span>"
    ]
  },
  {
    "objectID": "lectures/210-deploy-and-publish.html#pyopensci",
    "href": "lectures/210-deploy-and-publish.html#pyopensci",
    "title": "21  Deploying and publishing packages",
    "section": "21.12 pyOpenSci",
    "text": "21.12 pyOpenSci\n\nA new organization, modelled after rOpenSci\nscope is Python packages\nFirst package submitted to pyOpenSci was in May 2019\n\n\n21.12.1 Semantic versioning case study - answers\nIn 2008, Python bumped versions from 2.7.17 to 3.0.0. Some changes in the 3.0.0 release included: - print became a function - integer division resulted in creation of a float, instead of an integer - Some well-known APIs no longer returned lists (e.g., dict.keys, dict.values, map) - and many more (see here if interested)\nSource\nIn 2009, Python bumped versions from 3.0.1 to 3.1.0. Some changes in the 3.1.0 release included: - Addition of an ordered dictionary type - A pure Python reference implementation of the import statement - New syntax for nested with statements\nSource\nIn 2017, Python bumped versions from 3.6.3 to 3.6.4. Some changes in the 3.6.4 release included:\n\nFixed several issues in printing tracebacks (PyTraceBack_Print()).\nFix the interactive interpreter looping endlessly when no memory.\nFixed an assertion failure in Python parser in case of a bad unicodedata.normalize()\n\nSource",
    "crumbs": [
      "packaging-ci-cd-publish.html",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Deploying and publishing packages</span>"
    ]
  },
  {
    "objectID": "lectures/220-licenses-and-copyright.html",
    "href": "lectures/220-licenses-and-copyright.html",
    "title": "22  Copyright and licenses",
    "section": "",
    "text": "22.1 Learning objectives:\nBy the end of this lecture, students should be able to: - Explain who owns the copyright of code they write in a give situation, and why - Choose an appropriate license for software (i.e., packages or analysis code) - Choose an appropriate license for your non-software materials",
    "crumbs": [
      "licenses-copyright-wrapup.html",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Copyright and licenses</span>"
    ]
  },
  {
    "objectID": "lectures/220-licenses-and-copyright.html#full-disclosure---i-am-not-a-lawyer",
    "href": "lectures/220-licenses-and-copyright.html#full-disclosure---i-am-not-a-lawyer",
    "title": "22  Copyright and licenses",
    "section": "22.2 FULL DISCLOSURE - I AM NOT A LAWYER!",
    "text": "22.2 FULL DISCLOSURE - I AM NOT A LAWYER!",
    "crumbs": [
      "licenses-copyright-wrapup.html",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Copyright and licenses</span>"
    ]
  },
  {
    "objectID": "lectures/220-licenses-and-copyright.html#software-falls-under-copyright-law",
    "href": "lectures/220-licenses-and-copyright.html#software-falls-under-copyright-law",
    "title": "22  Copyright and licenses",
    "section": "22.3 Software falls under copyright law 🇨🇦🇺🇸",
    "text": "22.3 Software falls under copyright law 🇨🇦🇺🇸\n\nIn both the US and Canada, software code falls under copyright law\nknowing who owns the copyright of software code is critical because the owner controls if and how the code may be:\n\ncopied\ndistributed\nsold\nmodified\nessentially, made profitable\n\n\n\nCopyright protects only the language and words used to express ideas, concepts and themes, not the ideas, concepts or themes themselves.",
    "crumbs": [
      "licenses-copyright-wrapup.html",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Copyright and licenses</span>"
    ]
  },
  {
    "objectID": "lectures/220-licenses-and-copyright.html#canadian-copyright-law-rights-for-software-code",
    "href": "lectures/220-licenses-and-copyright.html#canadian-copyright-law-rights-for-software-code",
    "title": "22  Copyright and licenses",
    "section": "22.4 Canadian copyright law rights for software code 🇨🇦",
    "text": "22.4 Canadian copyright law rights for software code 🇨🇦\nIn Canada, the copyright owner is afforded the following rights:\n\nEconomic rights:\n\nthe right to produce, reproduce, publish, translate, authorize & convert a work\n\nMoral rights:\n\nthe right to claim authorship, the right to remain anonymous, or the right to use a pseudonym or pen name\nthe right to integrity\nthe right of association\n\n\n\nEconomic rights can be transferred to entities that are not the author, whereas moral rights cannot, they can however, be waived.",
    "crumbs": [
      "licenses-copyright-wrapup.html",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Copyright and licenses</span>"
    ]
  },
  {
    "objectID": "lectures/220-licenses-and-copyright.html#us-copyright-law-rights-for-software-code",
    "href": "lectures/220-licenses-and-copyright.html#us-copyright-law-rights-for-software-code",
    "title": "22  Copyright and licenses",
    "section": "22.5 US copyright law rights for software code 🇺🇸",
    "text": "22.5 US copyright law rights for software code 🇺🇸\nIn the US, the copyright owner is afforded four rights:\n\nThe right to reproduce the code\nThe right to create “derivative works” based on the code\nThe right to distribute copies of the code\nThe right to “display” the code",
    "crumbs": [
      "licenses-copyright-wrapup.html",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Copyright and licenses</span>"
    ]
  },
  {
    "objectID": "lectures/220-licenses-and-copyright.html#who-has-copyright-ownership",
    "href": "lectures/220-licenses-and-copyright.html#who-has-copyright-ownership",
    "title": "22  Copyright and licenses",
    "section": "22.6 Who has copyright ownership? 🇨🇦🇺🇸",
    "text": "22.6 Who has copyright ownership? 🇨🇦🇺🇸\nThis is a complicated question, and the answer starts with it depends…\nSo let’s start with the simplest case, you author the code and you are doing this for yourself (i.e., not for your employer, not for a client, etc).\n\nIn such a case, you (the person who typed the code) automatically become the copyright owner.\nIn both Canada and the USA, you do not need to need to affix the copyright symbol © to your work (some other countries do require this however).\n\n\nAlthough the copyright symbol © is not required, it is often used in copyrighted works (along with the name of the copyright owner and the year of first publication) to clearly identify that the code is protected by copyright.\n\n\nIn both Canada and the US, it is possible (and advisable) to register your copyright as evidence that a copyright exists and who the owner is.\n\n\n\n🇨🇦 Canadian Registration of copyright—filing online\n🇺🇸 USA Copyright Registration Portal\n\n\n\n22.6.1 Who owns the code in the case of “work made in the course of employment”?\nIn both Canada and the US, if you write code for work as an employee the copyright ownership is typically assigned to the employer.\nThe specifics differ a little for Canada and the US, and so we’ll discuss each separately.\n\n\n22.6.2 Who owns the code in the case of “work made in the course of employment”? 🇨🇦\nIn the Canada, software code is defined as “work made in the course of employment” (and therefore the copyright ownership is assigned to the employer) if:\n\nThe author of the code was in the employment of some other entity under a contract of service or apprenticeship and the code was written in the course of their employment by that entity. And there exists no agreement (written, or otherwise, and even potentially even presumed) that the employee retains ownership of copyright for the code written during the term of their employment.\n\n\nNote the bolding of the “of” in the sentence above. This is an important distinction from a contract for services (in which the author of the code acts more like an independent consultant, and in such a case it does not appear that “work made in the course of employment” would apply). In this case, the default position is usually that the contractor retains copyright ownership of the work they create, unless the contract specifically assigns copyright to the client. However, usually such a contract is asked for.\n\n\n\n22.6.3 Who owns the code in the case of “work-made-for-hire”? 🇺🇸\nIn the US, software code is defined as “work-made-for-hire” (and therefore the copyright ownership is assigned to the employer) if either:\n\nThe writing of the code is done by an employee in the scope of their employment\nThe writing of the code was specially ordered or commissioned for use in one of ten defined categories (listed here). There must be a written agreement signed by the developer that explicitly states the work is “work-made-for-hire”.\n\n\n\n22.6.4 Implications when you forgoe copyright ownership\nWhen you forgoe copyright ownership, such as in a “work-made-for-hire” situation, if you want to use any of the code that you wrote for that piece of work again in another project, you would need to negotiate a licence to use the code you wrote, the same way any other third-party would…\nAre there things one we can do to mitigate this? Possibly, some things include:\n\n(in a consulting position) negotiating that the client purchase a license to the code you write, as opposed to hiring you to write the code\nAt the beginning of the project, negotiating which code is core to the work, and thus should fall under “work-made-for-hire” and what (pre-built) code (e.g., packages, scripts) are outside the core work and should not.",
    "crumbs": [
      "licenses-copyright-wrapup.html",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Copyright and licenses</span>"
    ]
  },
  {
    "objectID": "lectures/220-licenses-and-copyright.html#why-do-i-need-a-license",
    "href": "lectures/220-licenses-and-copyright.html#why-do-i-need-a-license",
    "title": "22  Copyright and licenses",
    "section": "22.7 Why do I need a license?",
    "text": "22.7 Why do I need a license?\n\nAs mentioned above, creative works (like software code) are automatically eligible for intellectual property (and thus copyright) protection\nReusing creative works without a license is dangerous, because the copyright holders could sue you for copyright infringement\nThus, if you publicly share your creative work (i.e., software code), you should let others know if and how they can reuse it\nThis is done via the inclusion of a LICENSE or LICENSE.txt file in the base directory of the repository that clearly states under which license the content is being made available",
    "crumbs": [
      "licenses-copyright-wrapup.html",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Copyright and licenses</span>"
    ]
  },
  {
    "objectID": "lectures/220-licenses-and-copyright.html#why-do-i-need-a-license-contd",
    "href": "lectures/220-licenses-and-copyright.html#why-do-i-need-a-license-contd",
    "title": "22  Copyright and licenses",
    "section": "22.8 Why do I need a license? (cont’d)",
    "text": "22.8 Why do I need a license? (cont’d)\n\nUnless you include a license that specifies otherwise, nobody else can copy, distribute, or modify your work without being at risk of take-downs, shake-downs, or litigation.\nOnce the work has other contributors (each a copyright holder), “nobody” starts including you!",
    "crumbs": [
      "licenses-copyright-wrapup.html",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Copyright and licenses</span>"
    ]
  },
  {
    "objectID": "lectures/220-licenses-and-copyright.html#how-do-licenses-work",
    "href": "lectures/220-licenses-and-copyright.html#how-do-licenses-work",
    "title": "22  Copyright and licenses",
    "section": "22.9 How do licenses work?",
    "text": "22.9 How do licenses work?\nA license solves this problem by granting rights to others (the licensees) that they would otherwise not have. What rights are being granted under which conditions differs, often only slightly, from one license to another.\n\nNote: licenses are legal documents and written by legal experts. Most of you do not have the legal expertise to write your own license, so DO NOT do this. Instead, choose an already written one that best suits your situation.",
    "crumbs": [
      "licenses-copyright-wrapup.html",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Copyright and licenses</span>"
    ]
  },
  {
    "objectID": "lectures/220-licenses-and-copyright.html#how-to-choose-a-license",
    "href": "lectures/220-licenses-and-copyright.html#how-to-choose-a-license",
    "title": "22  Copyright and licenses",
    "section": "22.10 How to choose a license",
    "text": "22.10 How to choose a license\nIn practice, a few licenses are by far the most popular, and choosealicense.com will help you find a common license that suits your needs. Important considerations include:\n\nWhether you require people distributing derivative works to also require others to distribute their derivative works in the same way.\nWhether the content you are licensing is source code, and if it is, whether you want to require that derivatives of your work to share the source code",
    "crumbs": [
      "licenses-copyright-wrapup.html",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Copyright and licenses</span>"
    ]
  },
  {
    "objectID": "lectures/220-licenses-and-copyright.html#how-to-choose-a-license-contd",
    "href": "lectures/220-licenses-and-copyright.html#how-to-choose-a-license-contd",
    "title": "22  Copyright and licenses",
    "section": "22.11 How to choose a license (cont’d)",
    "text": "22.11 How to choose a license (cont’d)\n\nChoose a license that is in common use\nthis makes life easier for contributors and users, because they are more likely to already be familiar with the license and don’t have to wade through a bunch of jargon to decide if they’re ok with it",
    "crumbs": [
      "licenses-copyright-wrapup.html",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Copyright and licenses</span>"
    ]
  },
  {
    "objectID": "lectures/220-licenses-and-copyright.html#choosing-a-software-license",
    "href": "lectures/220-licenses-and-copyright.html#choosing-a-software-license",
    "title": "22  Copyright and licenses",
    "section": "22.12 Choosing a software license",
    "text": "22.12 Choosing a software license\nLet’s visit https://choosealicense.com/",
    "crumbs": [
      "licenses-copyright-wrapup.html",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Copyright and licenses</span>"
    ]
  },
  {
    "objectID": "lectures/220-licenses-and-copyright.html#open-source-software-is-used-in-business",
    "href": "lectures/220-licenses-and-copyright.html#open-source-software-is-used-in-business",
    "title": "22  Copyright and licenses",
    "section": "22.13 Open source software is used in business",
    "text": "22.13 Open source software is used in business\n\nRStudio Customer Stories\nThe Unreasonable Fear of Infection\n\n\n22.13.1 An Ethical License for Open Source Projects\n\nThe Hippocratic License",
    "crumbs": [
      "licenses-copyright-wrapup.html",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Copyright and licenses</span>"
    ]
  },
  {
    "objectID": "lectures/220-licenses-and-copyright.html#how-do-i-cite-the-code-i-used-from-another-project-that-was-openly-licensed",
    "href": "lectures/220-licenses-and-copyright.html#how-do-i-cite-the-code-i-used-from-another-project-that-was-openly-licensed",
    "title": "22  Copyright and licenses",
    "section": "22.14 How do I cite the code I used from another project that was openly licensed?",
    "text": "22.14 How do I cite the code I used from another project that was openly licensed?\nIf you substantially use licensed code in your project, you need to ensure you satisfy the license the code was shared under. For example, for the MIT license, you should include the original copyright notice and the MIT License text. This requirement is part of the license terms, ensuring that the original authors receive credit for their work and that the recipients of the software are aware of the terms under which it is provided.\nBelow is an example of how a LICENSE file might look when incorporating MIT-licensed code into a new project that is also distributed under the MIT License. This example includes both the original copyright notice for the incorporated code and a copyright notice for the new project.\nMIT License\n\nCopyright (c) [year of new project's creation] [New Project's Author or Organization]\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n---\n\nFor the portions of the project that are derived from [Original Project Name] (https://github.com/original/project/url):\n\nMIT License\n\nCopyright (c) [year of original project's creation] [Original Author or Organization]\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
    "crumbs": [
      "licenses-copyright-wrapup.html",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Copyright and licenses</span>"
    ]
  },
  {
    "objectID": "lectures/220-licenses-and-copyright.html#but-not-all-my-creative-data-science-work-is-code-how-do-i-license-it",
    "href": "lectures/220-licenses-and-copyright.html#but-not-all-my-creative-data-science-work-is-code-how-do-i-license-it",
    "title": "22  Copyright and licenses",
    "section": "22.15 But not all my creative Data Science work is code, how do I license it?",
    "text": "22.15 But not all my creative Data Science work is code, how do I license it?\nAs you all know, code is only one part of what Data Scientists do. We also create visualizations, write reports, create and give presentations, write tutorials, et cetera.\nThe licenses we have explored so far do not really fit these kinds of work, is there something that does?\nYes! The Creative Commons licences were created for such works and they are now widely used in academia and the publishing industry.\n\n22.15.1 Creative Commons licences\n\nChoose a Creative Commons license\nConsiderations for licensors\n\n\nSource: “How to License Poster” by Creative Commons is licensed under CC BY 4.0",
    "crumbs": [
      "licenses-copyright-wrapup.html",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Copyright and licenses</span>"
    ]
  },
  {
    "objectID": "lectures/220-licenses-and-copyright.html#explore-some-licenses",
    "href": "lectures/220-licenses-and-copyright.html#explore-some-licenses",
    "title": "22  Copyright and licenses",
    "section": "22.16 Explore some licenses:",
    "text": "22.16 Explore some licenses:\n\nGit, the source-code management tool\nCPython, the standard implementation of the Python language\nJupyter, the project behind the web-based Python notebooks we’ll be using\nEtherPad, a real-time collaborative editor",
    "crumbs": [
      "licenses-copyright-wrapup.html",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Copyright and licenses</span>"
    ]
  },
  {
    "objectID": "lectures/220-licenses-and-copyright.html#resources",
    "href": "lectures/220-licenses-and-copyright.html#resources",
    "title": "22  Copyright and licenses",
    "section": "22.17 Resources:",
    "text": "22.17 Resources:\n\nWho Owns The Code?\nCanadian Copyright Law - University of Alberta\nDo you actually own the IP generated by your Canadian employees?\nOwnership of Copyright - Canadian Copyright Act\nhttps://choosealicense.com/\nThe Unreasonable Fear of Infection\nFrequently Asked Questions about the GNU Licenses",
    "crumbs": [
      "licenses-copyright-wrapup.html",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Copyright and licenses</span>"
    ]
  },
  {
    "objectID": "lectures/220-licenses-and-copyright.html#attribution",
    "href": "lectures/220-licenses-and-copyright.html#attribution",
    "title": "22  Copyright and licenses",
    "section": "22.18 Attribution:",
    "text": "22.18 Attribution:\n\nmaterials on licenses have been borrowed and derived from the Software Carpentry Version Control with Git - Licensing lesson (licensed with the Creative Commons Attribution 4.0 International) and https://choosealicense.com/ (licensed with the Creative Commons Attribution 3.0 Unported License).",
    "crumbs": [
      "licenses-copyright-wrapup.html",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Copyright and licenses</span>"
    ]
  },
  {
    "objectID": "lectures/230-reproducibility-wrap-up.html",
    "href": "lectures/230-reproducibility-wrap-up.html",
    "title": "23  Workflows for reproducibile and trustworthy data science wrap-up",
    "section": "",
    "text": "23.1 Course learning objectives\nThis topic serves as a wrap-up of the course, summarizing the course learning objectives, redefining what is meant by reproducible and trustworthy data science, as well as contains data analysis project critique exercises to reinforce what has been learned in the course.\nBy the end of this course, students should be able to:",
    "crumbs": [
      "licenses-copyright-wrapup.html",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Workflows for reproducibile and trustworthy data science wrap-up</span>"
    ]
  },
  {
    "objectID": "lectures/230-reproducibility-wrap-up.html#course-learning-objectives",
    "href": "lectures/230-reproducibility-wrap-up.html#course-learning-objectives",
    "title": "23  Workflows for reproducibile and trustworthy data science wrap-up",
    "section": "",
    "text": "Defend and justify the importance of creating data science workflows that are reproducible and trustworthy and the elements that go into such a workflow (e.g., writing clear, robust, accurate and reproducible code, managing and sharing compute environments, defined collaboration strategies, etc).\nConstructively criticize the workflows and data analysis of others in regards to its reproducibility and trustworthiness.\nDevelop a data science project (including code and non-code documents such as reports) that uses reproducible and trustworthy workflows\nDemonstrate how to effectively share and collaborate on data science projects and software by creating robust code packages, using reproducible compute environments, and leveraging collaborative development tools.\nDefend and justify the benefit of, and employ automated testing regimes, continuous integration and continuous deployment for managing and maintaining data science projects and packages.\nDemonstrate strong communication, teamwork, and collaborative skills by working on a significant data science project with peers throughout the course.",
    "crumbs": [
      "licenses-copyright-wrapup.html",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Workflows for reproducibile and trustworthy data science wrap-up</span>"
    ]
  },
  {
    "objectID": "lectures/230-reproducibility-wrap-up.html#definitions-review",
    "href": "lectures/230-reproducibility-wrap-up.html#definitions-review",
    "title": "23  Workflows for reproducibile and trustworthy data science wrap-up",
    "section": "23.2 Definitions review:",
    "text": "23.2 Definitions review:\n\n23.2.1 Data science\nthe study, development and practice of reproducible and auditable processes to obtain insight from data.\nFrom this definition, we must also define reproducible and auditable analysis:\n\n\n23.2.2 Reproducible analysis:\nreaching the same result given the same input, computational methods and conditions \\(^1\\).\n\ninput = data\ncomputational methods = computer code\nconditions = computational environment (e.g., programming language & it’s dependencies)\n\n\n\n23.2.3 Auditable/transparent analysis,\na readable record of the steps used to carry out the analysis as well as a record of how the analysis methods evolved \\(^2\\).\n\nNational Academies of Sciences, 2019\nParker, 2017 and Ram, 2013",
    "crumbs": [
      "licenses-copyright-wrapup.html",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Workflows for reproducibile and trustworthy data science wrap-up</span>"
    ]
  },
  {
    "objectID": "lectures/230-reproducibility-wrap-up.html#what-makes-trustworthy-data-science",
    "href": "lectures/230-reproducibility-wrap-up.html#what-makes-trustworthy-data-science",
    "title": "23  Workflows for reproducibile and trustworthy data science wrap-up",
    "section": "23.3 What makes trustworthy data science?",
    "text": "23.3 What makes trustworthy data science?\nSome possible criteria:\n\nIt should be reproducible and auditable\nIt should be correct\nIt should be fair, equitable and honest\n\nThere are many ways a data science can be untrustworthy… In this course we will focus on workflows that can help build trust. I highly recommend taking a course in data science ethics* to help round out your education in how to do this. Further training in statistics and machine learning will also help with making sure your analysis is correct.\n\n* UBC’s CPSC 430 (Computers and Society) will have a section reserved for DSCI minor students next year in 2022 T1, which will focus on ethics in data science.\n\n\nExercise\nAnswer the questions below to more concretely connect with the criteria suggested above.\n\nGive an example of a data science workflow that affords reproducibility, and one that affords auditable analysis.\nCan a data analysis be reproducible but not auditable? How about auditable, but not reproducible?\nName at least two ways that a data analysis project could be correct (or incorrect).",
    "crumbs": [
      "licenses-copyright-wrapup.html",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Workflows for reproducibile and trustworthy data science wrap-up</span>"
    ]
  },
  {
    "objectID": "lectures/230-reproducibility-wrap-up.html#critiquing-data-analysis-projects",
    "href": "lectures/230-reproducibility-wrap-up.html#critiquing-data-analysis-projects",
    "title": "23  Workflows for reproducibile and trustworthy data science wrap-up",
    "section": "23.4 Critiquing data analysis projects",
    "text": "23.4 Critiquing data analysis projects\nCritiquing is defined as evaluating something in a detailed and analytical way (source: Oxford Languages Dictionary). It is used in many domains as a means to improve something (related to peer review), but also serves as an excellent pedagogical tool to actively practice evaluation.\nWe will work together in class to critique the following projects from the lens of reproducible and trustworthy workflows:\n\nGenomic data and code to accompany the “Accelerating Gene Discovery by Phenotyping Whole-Genome Sequenced Multi-mutation Strains and Using the Sequence Kernel Association Test (SKAT)” manuscript by Timbers et al., PLoS Genetics, 2015\nData and code to accompany the “Altmetrics in the wild: Using social media to explore scholarly impact” manuscript by Priem et al., arXiv, 2021\nCode to accompany the “Deep convolutional models improve predictions of macaque V1 responses to natural images” manuscript by Cadena et al., PLoS Computational Biology, 2019\n\n\n23.4.1 Exercise\nWhen prompted for each project listed above:\n\nIn groups of ~5, take 10 minutes to review the project from the lens of reproducible and trustworthy workflows. You want to evaluate the project with the questions in mind:\n\nWould I know where to get started reproducing the project?\nIf I could get started, do I think I could reproduce the project?\n\nAs a group, come up with at least 1-2 things that have been done well, as well as 1-2 things that could be improved from this lens. Justify these with respect to reproducibility and trustworthiness.\nChoose one person to be the reporter to bring back these critiques to the larger group.",
    "crumbs": [
      "licenses-copyright-wrapup.html",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Workflows for reproducibile and trustworthy data science wrap-up</span>"
    ]
  },
  {
    "objectID": "lectures/900-functions-in-python.html",
    "href": "lectures/900-functions-in-python.html",
    "title": "24  Defining functions in Python",
    "section": "",
    "text": "24.0.1 Default function arguments\nIn this course, we will get to practice writing functions (and then testing them) in the Python programming language. We will only cover it here briefly. To learn more about functions in Python, we refer you to the Module 6: Functions Fundamentals and Best Practices from the Programming in Python for Data Science course.\nFunctions in Python are defined using the reserved word def. A colon follows the function arguments to indicate where the function body should starte. White space, in particular indentation, is used to indicate which code belongs in the function body and which does not (unindented code following the function is not part of the function body). A general form for fucntion definition is shown below:\nHere’s a working example:\nNote that functions in Python are objects.\nDefault values can be specified in the function definition:\ndef math_two_numbers(x, y, operation = \"add\"):\n    if (operation == \"add\"):\n        return x + y\n    else:\n        return x - y\nmath_two_numbers (1, 4)\n\n5\nmath_two_numbers (1, 4, \"subtract\")\n\n-3",
    "crumbs": [
      "appendix.html",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Defining functions in Python</span>"
    ]
  },
  {
    "objectID": "lectures/900-functions-in-python.html#function-documentation",
    "href": "lectures/900-functions-in-python.html#function-documentation",
    "title": "24  Defining functions in Python",
    "section": "24.1 Function documentation",
    "text": "24.1 Function documentation\nFunction documentation is extremely useful at both the time of function creation/development, as well as at the time of function usage by the user. At the time of creation/development, it is useful for clearly delineating and communication the planned function’s specifications. At the time of usage, it is often the primary documentation a user will refer to in order to understand how to use the function. If a function is not well documented, it will not be well understood or widely used.\n\n24.1.1 Function documentation in Python\nIn Python, we write function documentation using docstrings. These come after the function definition and are denoted by three apostrophes (''' ... '''). There are many ways to write and format the content of the docstrings. For the purpose of this course, we will adopt the numpy style docstrings as they are easy to read and following by many data scientists.\nExample numpy style docstrings for math_two_numbers function:\n\ndef math_two_numbers(x, y, operation = \"add\"):\n    '''\n    Perform a mathematical operation (addition or subtraction) on two numbers.\n\n    Parameters\n    ----------\n    x : float or int\n        The first number to use in the operation.\n    y : float or int\n        The second number to use in the operation.\n    operation : str, optional\n        The operation to perform on the two numbers.\n        Can be 'add' for addition (default) or 'sub' for subtraction.\n\n    Returns\n    -------\n    float or int\n        The result of the mathematical operation performed on `x` and `y`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; math_two_numbers(4, 5)\n    9\n\n    &gt;&gt;&gt; math_two_numbers(10, 3, operation=\"sub\")\n    7\n    '''\n    if (operation == \"add\"):\n        return x + y\n    else:\n        return x - y\n\nOnce we define a function with docstrings, we (and other users) can get it as as part of the help documentation via help(math_two_numbers) or ?math_two_numbers:\n\nhelp(math_two_numbers)\n\nHelp on function math_two_numbers in module __main__:\n\nmath_two_numbers(x, y, operation='add')\n    Perform a mathematical operation (addition or subtraction) on two numbers.\n    \n    Parameters\n    ----------\n    x : float or int\n        The first number to use in the operation.\n    y : float or int\n        The second number to use in the operation.\n    operation : str, optional\n        The operation to perform on the two numbers.\n        Can be 'add' for addition (default) or 'sub' for subtraction.\n    \n    Returns\n    -------\n    float or int\n        The result of the mathematical operation performed on `x` and `y`.\n    \n    Examples\n    --------\n    &gt;&gt;&gt; math_two_numbers(4, 5)\n    9\n    \n    &gt;&gt;&gt; math_two_numbers(10, 3, operation=\"sub\")\n    7",
    "crumbs": [
      "appendix.html",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Defining functions in Python</span>"
    ]
  },
  {
    "objectID": "lectures/910-functions-in-r.html",
    "href": "lectures/910-functions-in-r.html",
    "title": "25  Defining functions in R",
    "section": "",
    "text": "25.1 Defining functions in R:\nIn this course, we will get to practice writing functions (and then testing them) in the R programming language. Given this has not been covered in the course pre-requisites, we will cover it here briefly. To learn more about functions in R, we refer you to the functions chapter in the Advanced R book.\nExample:\nadd_two_numbers &lt;- function(x, y) {\n  x + y\n}\n\nadd_two_numbers(1, 4)\n\n[1] 5\nmath_two_numbers &lt;- function(x, y, operation) {\n  if (operation == \"add\") {\n    return(x + y)\n  }\n  x - y\n}\nmath_two_numbers (1, 4, \"add\")\n\n[1] 5\nmath_two_numbers (1, 4, \"subtract\")\n\n[1] -3",
    "crumbs": [
      "appendix.html",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Defining functions in R</span>"
    ]
  },
  {
    "objectID": "lectures/910-functions-in-r.html#defining-functions-in-r",
    "href": "lectures/910-functions-in-r.html#defining-functions-in-r",
    "title": "25  Defining functions in R",
    "section": "",
    "text": "Use variable &lt;- function(…arguments…) { …body… } to create a function and give it a name\n\n\n\n\nFunctions in R are objects. This is referred to as “first-class functions”.\nThe last line of the function returns the object created, or listed there. To return a value early use the special word return (as shown below).\n\n\n\n\n\n25.1.1 Default function arguments\nDefault values can be specified in the function definition:\n\nmath_two_numbers &lt;- function(x, y, operation = \"add\") {\n  if (operation == \"add\") {\n    return(x + y)\n  }\n  x - y\n}\n\n\nmath_two_numbers (1, 4)\n\n[1] 5\n\n\n\nmath_two_numbers (1, 4, \"subtract\")\n\n[1] -3\n\n\n\nOptional - Advanced\n\nExtra arguments via ...\nIf we want our function to be able to take extra arguments that we don’t specify, we must explicitly convert ... to a list (otherwise ... will just return the first object passed to ...):\n\nadd &lt;- function(x, y, ...) {\n    print(list(...))\n}\nadd(1, 3, 5, 6)\n\n[[1]]\n[1] 5\n\n[[2]]\n[1] 6\n\n\n\n\n\n\n25.1.2 Lexical scoping in R\nR’s lexical scoping follows several rules, we will cover the following 3:\n\nName masking\nDynamic lookup\nA fresh start\n\n\nName masking\nObject names which are defined inside a function mask object names defined outside of that function. For example, below x is first defined in the global environment, and then also defined in the function environment. x in the function environment masks x in the global environment, so the value of 5 is used in to_add + x instead of 10:\n\nx &lt;- 10\n\nadd_to_x &lt;- function(to_add) {\n    x &lt;- 5\n    to_add + x\n}\n\nadd_to_x(2)\n\n[1] 7\n\n\nIf a function refers to an object name that is not defined inside the function, then R looks at the environment one level above. And if it is not found there, it again, looks another level above. This will happen all the way up the environment tree until it searches all possible environments for that session (including the global environment and loaded packages).\n\nAttribution: Derived from Advanced R by Hadley Wickham\nHere x is not defined in the function environment, and so R then looks one-level up to the global environment to try to find x. In this case it does and it is then used to calculate to_add + x:\n\nx &lt;- 10\n\nadd_to_x &lt;- function(to_add) {\n    to_add + x\n}\n\nadd_to_x(2)\n\n[1] 12\n\n\n\n\nDynamic lookup\nR does not look up the values for objects it references when it is defined/created, instead it does this when the function is called. This can lead to the function returning different things depending on the values of the objects it references outside of the function’s environment.\nFor example, each time the function add_to_x is run, R looks up the current value of x and uses that to compute to_add + x:\n\nadd_to_x &lt;- function(to_add) {\n    to_add + x\n}\n\nx &lt;- 10\nadd_to_x(2)\n\n[1] 12\n\nx &lt;- 20\nadd_to_x(2)\n\n[1] 22\n\n\n\n\nA fresh start\nFunctions in R have no memory of what happened the last time they were called. This happens because a new function environment is created, R created a new environment in which to execute it.\nFor example, even though we update the value of x inside the function by adding two to it every time we execute it, it returns the same value each time, because R creates a new function environment each time we run it and that environment has no recollection of what happened the last time the function was run:\n\nx &lt;- 10\n\nadd_to_x &lt;- function(to_add) {\n    x &lt;- to_add + x\n    x\n}\n\nadd_to_x(2)\n\n[1] 12\n\nadd_to_x(2)\n\n[1] 12\n\nadd_to_x(2)\n\n[1] 12\n\n\n\n\n\n25.1.3 Lazy evaluation\nIn R, function arguments are lazily evaluated: they’re only evaluated if accessed.\nKnowing that, now consider the add_one function written in both R and Python below:\n# R code (this would work)\nadd_one &lt;- function(x, y) {\n    x &lt;- x + 1\n    return(x)\n}\n# Python code (this would not work)\ndef add_one(x, y):\n    x = x + 1\n    return x\nWhy will the above add_one function will work in R, but the equivalent version of the function in python would break?\n\nPython evaluates the function arguments before it evaluates the function and because it doesn’t know what y is, it will break even though it is not used in the function.\nR performs lazy evaluation, meaning it delays the evaluation of the function arguments until its value is needed within/inside the function. Since y is never referenced inside the function, R doesn’t complain, or even notice it.\n\n\nadd_one in R\nadd_one &lt;- function(x, y) {\n    x &lt;- x + 1\n    return(x)\n}\nThis works:\nadd_one(2, 1)\n3\nand so does this:\nadd_one(2)\n3\n\n\nadd_one in Python\ndef add_one(x, y):\n    x = x + 1\n    return x`\nThis works:\nadd_one(2, 1)\n3\nThis does not:\nadd_one(2)\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-5-f2e542671748&gt; in &lt;module&gt;\n----&gt; 1 add_one(2)\n\nTypeError: add_one() missing 1 required positional argument: 'y'\n\n\nThe power of lazy evaluation\nLet’s you have easy to use interactive code like this:\n\ndplyr::select(mtcars, mpg, cyl, hp, qsec)\n\n                     mpg cyl  hp  qsec\nMazda RX4           21.0   6 110 16.46\nMazda RX4 Wag       21.0   6 110 17.02\nDatsun 710          22.8   4  93 18.61\nHornet 4 Drive      21.4   6 110 19.44\nHornet Sportabout   18.7   8 175 17.02\nValiant             18.1   6 105 20.22\nDuster 360          14.3   8 245 15.84\nMerc 240D           24.4   4  62 20.00\nMerc 230            22.8   4  95 22.90\nMerc 280            19.2   6 123 18.30\nMerc 280C           17.8   6 123 18.90\nMerc 450SE          16.4   8 180 17.40\nMerc 450SL          17.3   8 180 17.60\nMerc 450SLC         15.2   8 180 18.00\nCadillac Fleetwood  10.4   8 205 17.98\nLincoln Continental 10.4   8 215 17.82\nChrysler Imperial   14.7   8 230 17.42\nFiat 128            32.4   4  66 19.47\nHonda Civic         30.4   4  52 18.52\nToyota Corolla      33.9   4  65 19.90\nToyota Corona       21.5   4  97 20.01\nDodge Challenger    15.5   8 150 16.87\nAMC Javelin         15.2   8 150 17.30\nCamaro Z28          13.3   8 245 15.41\nPontiac Firebird    19.2   8 175 17.05\nFiat X1-9           27.3   4  66 18.90\nPorsche 914-2       26.0   4  91 16.70\nLotus Europa        30.4   4 113 16.90\nFord Pantera L      15.8   8 264 14.50\nFerrari Dino        19.7   6 175 15.50\nMaserati Bora       15.0   8 335 14.60\nVolvo 142E          21.4   4 109 18.60\n\n\nNotes: - There’s more than just lazy evaluation happening in the code above, but lazy evaluation is part of it. - package::function() is a way to use a function from an R package without loading the entire library.",
    "crumbs": [
      "appendix.html",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Defining functions in R</span>"
    ]
  },
  {
    "objectID": "lectures/910-functions-in-r.html#exception-handling",
    "href": "lectures/910-functions-in-r.html#exception-handling",
    "title": "25  Defining functions in R",
    "section": "25.2 Exception handling",
    "text": "25.2 Exception handling\nSometimes our code is correct but we still encounter errors. This commonly occurs with functions when users attempt to use them in weird and creative ways that the developer did not intend. Developers are well advised to try to anticipate some of this user behaviour and guard against this in a way that can be of help to the user. One way to do this is to have a function fail intentionally when incorrect user input is given.\nImagine we have a simple function to convert temperatures from Fahrenheit to Celsius:\n\nfahr_to_celsius &lt;- function(temp) {\n  (temp - 32) * 5/9\n}\n\nWhat if our user anticipates that our function can handle numbers written as strings? When this happens our users gets a somewhat cryptic error message (which is inversely correlated with their knowledge of the R programming language):\nfahr_to_celsius(\"thirty\")\nError in temp - 32: non-numeric argument to binary operator\nTraceback:\n\n1. fahr_to_celsius(\"thirty\")\nThis error message is not as helpful as it could be! Also, if this calculation happened to take a long time, it might be nicer to check the data type before we attempt the calculation and then if the wrong data type was given, throw an purposeful error with a more helpful error message. One way we can do this simply in R is using a conditional (i.e., if statement) to test the type and then calling stop with a useful error message if the type is incorrect.\nFor example with the fahr_to_celsius function, we can use if(!is.numeric(temp)) to check whether temp is not numeric. And then if it is not, stop causes a break and the printing out of the more helpful error message “fahr_to_celsius expects a vector of numeric values\n\nfahr_to_celsius &lt;- function(temp) {\n  if(!is.numeric(temp)) {\n    stop(\"`fahr_to_celsius` expects a vector of numeric values\")\n  }\n  (temp - 32) * 5/9\n}\n\nfahr_to_celsius(\"thirty\")\nError in fahr_to_celsius(\"thirty\"): `fahr_to_celsius` expects a vector of numeric values\nTraceback:\n\n1. fahr_to_celsius(\"thirty\")\n2. stop(\"`fahr_to_celsius` expects a vector of numeric values\")   # at line 3 of file &lt;text&gt;\nIf you wanted to issue a warning instead of an error, you could use warning in place of stop in the example above. However, in most cases it is better practice to throw an error than to print a warning…\nMore advanced exception handling exists in R, and I invite interested learner to read more about it here in the Condition Handling and Defensive Programming sections of Advanced R.",
    "crumbs": [
      "appendix.html",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Defining functions in R</span>"
    ]
  },
  {
    "objectID": "lectures/910-functions-in-r.html#programming-with-the-tidyverse-functions",
    "href": "lectures/910-functions-in-r.html#programming-with-the-tidyverse-functions",
    "title": "25  Defining functions in R",
    "section": "25.3 Programming with the tidyverse functions",
    "text": "25.3 Programming with the tidyverse functions\nThe functions from the tidyverse are beautiful to use interactively - with these functions, we can “pretend” that the data frame column names are objects in the global environment and refer to them without quotations (e.g., \"\")\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(gapminder)\n\ngapminder |&gt;\n  filter(country == \"Canada\", year == 1952)\n\n# A tibble: 1 × 6\n  country continent  year lifeExp      pop gdpPercap\n  &lt;fct&gt;   &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n1 Canada  Americas   1952    68.8 14785584    11367.\n\n\nexpecially when compared with base R:\n\ngapminder[gapminder$country == \"Canada\" & gapminder$year == 1952, ]\n\n# A tibble: 1 × 6\n  country continent  year lifeExp      pop gdpPercap\n  &lt;fct&gt;   &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n1 Canada  Americas   1952    68.8 14785584    11367.\n\n\nHowever, the beauty of being able to refer to data frame column names in R without quotations, leads to problems when we try to use them in a function. For example,\nfilter_gap &lt;- function(col, val) {\n    filter(gapminder, col == val)\n}\n\nfilter_gap(country, \"Canada\")\nError: object 'country' not found\nTraceback:\n\n1. filter_gap(country, \"Canada\")\n2. filter(gapminder, col == val)   # at line 4 of file &lt;text&gt;\n3. filter.tbl_df(gapminder, col == val)\n4. filter_impl(.data, quo)\n\nWhy does filter work with non-quoted variable names, but our function filter_gap fail? At a very high-level, this is because filter is doing more behind the scenes to handle these unquoted column names than we see without looking at the source code. So to make this work for our function, we need to do a little more work too.\nGiven that this is not a course in R programming, we will not go into the details of why this happens, but we will show one way of how to handle this. For learners intersted to learn more about why this happens, we recommend reading these two resources:\n\nProgramming with dplyr\nMetaprogramming\n\n\n25.3.1 Defining tidyverse functions by embracing column names: { }\nIn the newest release of the rlang R package, there has been the introduction of the {{ (pronounced “curly curly”) operator. This operator does the necessary work behind the scenes so that you can continue to use unquoted column names with the tidyverse functions even when you use them in functions that you write yourself.\nTo use the {{ operator, we “embrace” the unquoted column names when we refer to them inside our function body. An important note is that there are several ways to implement the usage of unquoted column names in R, and the {{ operator only works with the tidyverse functions. Please refer to the Metaprogramming chapter of Advanced R if you need to do this with non-tidyverse functions.\nHere’s the function we show above working when we use the {{ operator to “embrace” the unquoted column names:\n\nfilter_gap &lt;- function(col, val) {\n  filter(gapminder, {{col}} == val)\n}\n\n\nfilter_gap(country, \"Canada\")\n\n# A tibble: 12 × 6\n   country continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;   &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Canada  Americas   1952    68.8 14785584    11367.\n 2 Canada  Americas   1957    70.0 17010154    12490.\n 3 Canada  Americas   1962    71.3 18985849    13462.\n 4 Canada  Americas   1967    72.1 20819767    16077.\n 5 Canada  Americas   1972    72.9 22284500    18971.\n 6 Canada  Americas   1977    74.2 23796400    22091.\n 7 Canada  Americas   1982    75.8 25201900    22899.\n 8 Canada  Americas   1987    76.9 26549700    26627.\n 9 Canada  Americas   1992    78.0 28523502    26343.\n10 Canada  Americas   1997    78.6 30305843    28955.\n11 Canada  Americas   2002    79.8 31902268    33329.\n12 Canada  Americas   2007    80.7 33390141    36319.\n\n\n\n\n25.3.2 The walrus operator := is needed when assigning values\nFor similar reasons, the walrus operator (:=) is needed when writing functions that create new columns using unquoted column names with the tidyverse functions:\n\ngroup_summary &lt;- function(data, group, col, fun) {\n  data |&gt;\n    group_by({{ group }}) |&gt;\n    summarise( {{ col }} := fun({{ col }}))\n}\n\n\ngroup_summary(gapminder, continent, gdpPercap, mean)\n\n# A tibble: 5 × 2\n  continent gdpPercap\n  &lt;fct&gt;         &lt;dbl&gt;\n1 Africa        2194.\n2 Americas      7136.\n3 Asia          7902.\n4 Europe       14469.\n5 Oceania      18622.",
    "crumbs": [
      "appendix.html",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Defining functions in R</span>"
    ]
  },
  {
    "objectID": "lectures/910-functions-in-r.html#function-documentation",
    "href": "lectures/910-functions-in-r.html#function-documentation",
    "title": "25  Defining functions in R",
    "section": "25.4 Function documentation",
    "text": "25.4 Function documentation\nFunction documentation is extremely useful at both the time of function creation/development, as well as at the time of function usage by the user. At the time of creation/development, it is useful for clearly delineating and communication the planned function’s specifications. At the time of usage, it is often the primary documentation a user will refer to in order to understand how to use the function. If a function is not well documented, it will not be well understood or widely used.\n\n25.4.1 Function documentation in R\nIn R, we write function documentation in a style that the roxygen2 R package can use to automatically generate nicely formatted documentation, accessible via R’s help function, when we create R packages. This is not available to us until we package our functions in software packages, however it is a good practice to get into because it helps ensure you document the key aspects of a function for your user, and it means that when you go to create an R package to share your function, you will have already correctly formatted the function documentation - saving future you some work!\n\n#' Converts temperatures from Fahrenheit to Celsius.\n#'\n#' @param temp a vector of temperatures in Fahrenheit\n#'\n#' @return a vector of temperatures in Celsius\n#'\n#' @examples\n#' fahr_to_celcius(-20)\nfahr_to_celsius &lt;- function(temp) {\n    (temp - 32) * 5/9\n}\n\nRStudio has a nice feature to provide a roxygen2 formatted skeleton for function documentation. To use it, you put the cursor in the function definition, and then select the menu items “Code” &gt; “Insert Roxygen Skeleton”:",
    "crumbs": [
      "appendix.html",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Defining functions in R</span>"
    ]
  },
  {
    "objectID": "lectures/910-functions-in-r.html#practice-writing-functions-in-r",
    "href": "lectures/910-functions-in-r.html#practice-writing-functions-in-r",
    "title": "25  Defining functions in R",
    "section": "25.5 Practice writing functions in R",
    "text": "25.5 Practice writing functions in R\nTo practice writing functions in R, you can attempt this worksheet: https://github.com/UBC-DSCI/dsci-310-student/blob/main/practice/worksheet_functions_in_r.ipynb\nNote: to access the automated software tests for feedback on your answers, you will want to clone or download this GitHub repo and navigate to the practice directory.",
    "crumbs": [
      "appendix.html",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Defining functions in R</span>"
    ]
  },
  {
    "objectID": "lectures/910-functions-in-r.html#defining-functions-in-python",
    "href": "lectures/910-functions-in-r.html#defining-functions-in-python",
    "title": "25  Defining functions in R",
    "section": "25.6 Defining functions in Python",
    "text": "25.6 Defining functions in Python\nIn this course, we will get to practice writing functions (and then testing them) in the Python programming language. We will only cover it here briefly. To learn more about functions in Python, we refer you to the Module 6: Functions Fundamentals and Best Practices from the Programming in Python for Data Science course.\nFunctions in Python are defined using the reserved word def. A colon follows the function arguments to indicate where the function body should starte. White space, in particular indentation, is used to indicate which code belongs in the function body and which does not. A general form for fucntion definition is shown below:\ndef function_name(…function arguments…):\n    …function body…\nHere’s a working example:\n\nadd_two_numbers &lt;- function(x, y) {\n  x + y\n}\n\nadd_two_numbers(1, 4)\n\n[1] 5\n\n\n\nFunctions in R are objects. This is referred to as “first-class functions”.\nThe last line of the function returns the object created, or listed there. To return a value early use the special word return (as shown below).\n\n\nmath_two_numbers &lt;- function(x, y, operation) {\n  if (operation == \"add\") {\n    return(x + y)\n  }\n  x - y\n}\n\n\nmath_two_numbers (1, 4, \"add\")\n\n[1] 5\n\n\n\nmath_two_numbers (1, 4, \"subtract\")\n\n[1] -3\n\n\n\n25.6.1 Default function arguments\nDefault values can be specified in the function definition:\n\nmath_two_numbers &lt;- function(x, y, operation = \"add\") {\n  if (operation == \"add\") {\n    return(x + y)\n  }\n  x - y\n}\n\n\nmath_two_numbers (1, 4)\n\n[1] 5\n\n\n\nmath_two_numbers (1, 4, \"subtract\")\n\n[1] -3\n\n\n\nOptional - Advanced\n\nExtra arguments via ...\nIf we want our function to be able to take extra arguments that we don’t specify, we must explicitly convert ... to a list (otherwise ... will just return the first object passed to ...):",
    "crumbs": [
      "appendix.html",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Defining functions in R</span>"
    ]
  },
  {
    "objectID": "lectures/920-reproducible-reports.html",
    "href": "lectures/920-reproducible-reports.html",
    "title": "26  Reproducible reports",
    "section": "",
    "text": "26.1 Other kinds of reproducible reports\nIn addition to Quarto, there are other tools and technologies commonly used for creating reproducible reports in data science. We think at the moment, Quarto is the best tool for this course, but we kept the notes on the others here for those who are interested.",
    "crumbs": [
      "appendix.html",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Reproducible reports</span>"
    ]
  },
  {
    "objectID": "lectures/920-reproducible-reports.html#introduction-to-r-markdown",
    "href": "lectures/920-reproducible-reports.html#introduction-to-r-markdown",
    "title": "26  Reproducible reports",
    "section": "26.2 Introduction to R Markdown",
    "text": "26.2 Introduction to R Markdown\nR Markdown is one implementation of a reproducible reporting tool. It is very user friendly and has become very powerful - allowing a great deal of control over formatting with the ability to render to many different outputs. Including: - PDF - html - Word - Powerpoint presentation - many more!\nIt is a mix of markdown primarily used for narrative text, and code chunks where code can be executed in an engine. It was originally created for the R programming language, but can be used with several others, including Python. See here in the R Markdown tutorial for how to use it with other languages, and here for a GitHub repository with a working example of this.\n\n26.2.1 R Markdown resources\n\nR Markdown tutorial\nR Markdown: The Definitive Guide\nR Markdown documentation\n\n\nExercise - get to know R Markdown\nLet’s get to know R Markdown! Open RStudio and create a new R Markdown document, choosing HTML as the output format. Look at the source document that is created, where is the narrative text written? Where is the code written? How does this differ from Jupyter?\n\n\nExercise - render your first document\nThere are two ways to render a document from the Rmd source to the desired output. One is using a button in RStudio - the knit button that looks like this:\n\nTry clicking that button and see what happens!\nAnother way you can do this is through code! Try running this in the R console (replacing \"FILEPATH/FILE.Rmd\" with the file path to where you saved this R Markdown document:\nrmarkdown::render(\"FILEPATH/FILE.Rmd\", output_format = \"html_document\")\n\n\nExercise - checkout the new visual markdown editor\nRStudio has implemented a new feature for working with R Markdown to make it more similar to working with Jupyter - it is called the visual markdown editor. Checkout this feature by clicking the visual markdown editor button when you have an R Markdown file open in the editor. The button looks like this:\n\n\n\n\n26.2.2 A helpful hint for successfully working with R Markdown documents\nGiven that you need to render the entire document to see your Markdown and LaTeX rendered, it is important to “knit” often as you make changes. If you make an error in a LaTeX equation for example, it will stop the knitting/rendering process and you will not get to see the rendered document. So by knitting/rendering often you will know where the last changes you made are and then will be able to easily identify and fix your errors.\n\n\n26.2.3 Running, editing and creating code chunks\nJust like Jupyter notebooks, R Markdown has code cells, although they’re more commonly referred to as code “chunks” or “blocks”. These are based off fenced Markdown code blocks and always start and end with 3 backticks (```), just like in Markdown. Unique to R Markdown is that the leading three backticks are followed by curly braces containing the language engine you want to run, which for r looks like this {r}. Additional metadata can be included, for example a name to reference the code chunk:\n```{r my first code chunk}\nx &lt;- 5\nx\n```\nThere are other language engines that can be used in RMarkdown, you can learn more about that here.\nAll code cells are run when you knit/render the entire document (like pressing “Run all” in JupyterLab). By default, the code in the chunk and the code output will be included in your rendered document. You can also run the code by clicking the green play button on the right-hand side of the code chunk.\n\n\n26.2.4 Naming code chunks and R Markdown document sections\nWhen you include Markdown headers (using the # symbol) R Studio automatically creates a pop-up-like menu for you to use to navigate the document, which you can access by clicking the bar below this editor panel. It looks like:\n\n\nBy clicking on any of the headings in the pop-up-like menu, RStudio will navigate you to that section of the R Markdown document. Try clicking on one to see how it works.\nIn addition to Markdown headings, RStudio also keeps track of code chunks in that menu. By default RStudio names the chunks by their position (e.g. Chunk 1, Chunk 2, etc). But in reality those names are not that useful and it is more helpful to give code chunks meaningful names. For example, in the code chunk below where we use a for loop to sum the numbers from 1 to 10, we name the chunk “for-loop-sum”.\n```{r for-loop-sum}\n# initialize sum to 0\nloop_sum &lt;- 0\n\n# loop of a sequence from 1 to 10 and calculate the sum\nfor (i in seq(1:10)){\n  loop_sum &lt;- loop_sum + i\n}\n\nprint(loop_sum)\n```\nDo not duplicate code chunk names, this will break the rendering of your document!\n\n\n26.2.5 Code chunk options\nThere are many code chunk options that you can set. These options let you customize chunk behavior, including whether a chunk is evaluated, whether to include the output in the rendered document, etc. A short list of code chunk options is shown below, but you can find an extensive list starting on the second page of this document.\n\nYou can set the chunk options at either a global level (once set they will be applied to all code chunks in the .Rmd file) or locally for a specific chunk (these will override the global chunk options if they are contradictory).\nGlobal options are usually set in one chunk at the top of the document and looks like this (this is a screenshot):\n{r setup, include=FALSE} knitr::opts_chunk$set(echo = FALSE)\nGlobal chunk options are set by adding them as arguments to knitr::opts_chunk$set(...) (put them in place of ... and separate multiple options with a comma). The only global chunk options set in this document is echo = FALSE, which hides the code chunks and only shows the output, something that can be useful for non-technical reports.\nLocal chunk options are set by adding the options in the curly braces of a code chunk after the language engine and code chunk name. For example, to not display warnings in a single code chunk we would use the warning = FALSE code chunk as follows:\n{r correlation no warning, warning = FALSE} # some R code that throws a warning cor( c( 1 , 1 ), c( 2 , 3 ) )\n\n\n\n\n\n\nTip\n\n\n\n\nR Markdown support inline evaluated code via the following syntax Adding 3 to 4 gives `r 4 + 3`.     The value of `x` is currently `r x`.\nLatex equations can be written the same way as in Jupyter notebooks and standard markdown documents. - $\\alpha = 5$ for inline latex and $$\\alpha = 5$$ for a math block. - When hovering over equations, R will display the rendered equation in a pop up.\nR Markdown is built upon the Pandoc Markdown engine. This is useful to know since the Pandoc manual is a great exhaustive resource for looking up anything Markdown related.\nOne of the features made available thanks to Pandoc is support for citations and bibliographies.\n\nLet’s cite the R-package by typing citation() into the console, and copying the BibTeX citation into a new document that we call rstudio-demo.bib     and adding an identifier string (a key) before the first comma,     e.g.r-lang`.\nInclude the following field in the YAML metadata in the beginning of the document: bibliography: rstudio-demo.bib, then cite it somewhere in the text by adding [@r-lang]. The bibliography will be appended to the document, so it is advisable to add a heading saying # References at the very end.\n\nWhen working with R Markdown (and code in general) be careful that you don’t copy stylized quotation marks because these will not work. For example, this will throw an error:\n\na = “This string”\nIt should look like this instead:\na = \"This string\"\n\n\n\n\n26.2.6 Creating a table of contents\nA table of contents can be automatically generated by specifying toc: true in the YAML front matter. It must be indented as so:\noutput:\n  github_document:\n    toc: true\nThis works for all output file types I have explored so far!\n\n\n26.2.7 Create tables and table descriptions\nData frames and other rectangular objects in R can be made into nice tables using knitr::kable. At the most basic level the syntax is:\nknitr::kable(data_frame)\nTo have a table description stay associated with the table, use the caption argument. For example:\nknitr::kable(data_frame, caption = \"Table 1. This is a summary of the data set\")\nA couple notes: - If you render to github_document or html_document you do need to manually write the “Table 1.” in your figure caption. If you render to pdf_document you do not. - Want fancier tables? Use the kableExtra package! See docs here: https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html\n\n\n26.2.8 Resize figures and keep figure descriptions associated with their figures\nThere are two ways to do this, and you they depend on whether you are creating the figure from an R object vs a file.\n\n1. Resizing figures and writing figure captions generated from an R plot object\nHere you need to:\n\nfig.width=&lt;WIDTH&gt;, fig.height=&lt;HEIGHT&gt; in the R code chunk options.\nfig.cap = \"CAPTION\" in the R code chunk options.\n\n\n\n2. Resizing figures and writing figure captions generated from a saved file (e.g., .png)\nHere you need to:\n\nuse knitr::include_graphics(&lt;FILE_PATH&gt;) inside a code chunk to point the figure file\nfig.cap in the R code chunk options to write the figure caption\nuse out.width = '40%' and out.height = '40%' in the R code chunk options to set the figure size.\n\n\n\n\n26.2.9 Referencing values from code inline in the narrative text\nYou can refer to values stored in code inside your markdown text using the r object surrounded by backticks.\nThis is extremely useful for iterating over an analysis near the final stages when you already have the report written, or when doing parameterized reports (more on these in a bit).\nHowever, the value you represent must be a vector.\n\n\n26.2.10 Create a references section for citing external sources\nDO NOT format references by hand, you will drive yourself nuts, especially the more references you collect! Instead use R Markdowns bibliography/citiation functionality. Below is a brief introduction, full docs here.\n\nSteps to citing in R Markdown\n\nAdd bibliography: PATH/FILENAME.bib to the YAML front matter\nAt the very end of the .Rmd file add # References\nCreate PATH/FILENAME.bib (this is a plain text file) and place citations in there using BibTeX citation format (see examples here).\nIn the .Rmd mardown text, use @label (corresponds to the first string inside the {} in the .bib file for the reference of interest) to do a in-text citation.\n\n\n\nGetting BibTeX citations\nBibTeX citations for code (R & Python programming languages and their respective packages) can be obtained from the sources: - citatation() function in R (no argument will give you the citation for R, adding a package name as a string to the function call will give you the citation for that package) - For Python, I have found I have had to do this somewhat manually from package GitHub repos or docs pages (some packages exist to do this, but none seem to work flawlessly out of box). See my demo .bib file here for an example: https://github.com/ttimbers/breast_cancer_predictor/blob/master/doc/breast_cancer_refs.bib\nBibTeX citations for papers can usually be found via Google scholar:\n\n\n1. Search for the paper on Google Scholar\n\nvisit https://scholar.google.com/\n\n\n\n2. Use Goolge Scholar to get the citation\n\nClick on the quotation icon under the paper:\n\n\n\nClick on the BibTeX link on the pop-up box:\n\n\n\nCopy the BibTeX citation to your .bib file\n\n@article{ahmad2007k,\n  title={A k-mean clustering algorithm for mixed numeric and categorical data},\n  author={Ahmad, Amir and Dey, Lipika},\n  journal={Data \\& Knowledge Engineering},\n  volume={63},\n  number={2},\n  pages={503--527},\n  year={2007},\n  publisher={Elsevier}\n}\n\nNote: The Zotero reference manager now works nicely with R Markdown too! You might want to try this for your project, see here: https://www.rstudio.com/blog/rstudio-1-4-preview-citations/#citations-from-zotero",
    "crumbs": [
      "appendix.html",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Reproducible reports</span>"
    ]
  },
  {
    "objectID": "lectures/920-reproducible-reports.html#advanced-reporting",
    "href": "lectures/920-reproducible-reports.html#advanced-reporting",
    "title": "26  Reproducible reports",
    "section": "26.3 Advanced reporting",
    "text": "26.3 Advanced reporting\nIn a more complex project, we need to do the following things that are not available to us directly though RMarkdown or Jupyter notebooks on their own:\n\nAutomatically generate figure numbering based on order of appearance in the report.\nAutomatically generate table numbering based on order of appearance in the report.\nCross reference figures and tables as so to keep the automatically generated figure/table numbers.\n\nJupyter notebooks on their also lack some functionality directly available in RMarkdown: 1. Removing code cells from final report 2. Build a bibliography (only not available in Jupyter directly) 3. Render code inline in Markdown\nThus to address both of these issues, we can layer additional packages on top of RMarkdown and Jupyter to gain this needed functionality. In the case of RMarkdown, we layer on the bookdown R package (rticle package can also be used too). And for Jupyter, we layer on the jupyter-book package.",
    "crumbs": [
      "appendix.html",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Reproducible reports</span>"
    ]
  },
  {
    "objectID": "lectures/920-reproducible-reports.html#r-markdown-extensions",
    "href": "lectures/920-reproducible-reports.html#r-markdown-extensions",
    "title": "26  Reproducible reports",
    "section": "26.4 R Markdown extensions",
    "text": "26.4 R Markdown extensions\nWhat if you need more formatting than is available to you in vanilla R Markdown? For example, cross referencing between sections for a report or thesis? Or journal article templates for research manuscripts? You can add additional functionality like this by adding the use of the bookdown or rticle R packages, respectively.\nBelow are links to the RMarkdown documentation for the advanced reporting tasks that we need to layer on bookdown for:\n\n26.4.1 Figure formatting\n\nAutomatic figure numbering\nCross referencing figures in the text using automatically generated figure numbers\n\n\n\n26.4.2 Table formatting\n\nAutomatic table numbering and figure captions via knitr::kable’s caption argument\nCross referencing tables in the text using automatically generated table numbers",
    "crumbs": [
      "appendix.html",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Reproducible reports</span>"
    ]
  },
  {
    "objectID": "lectures/920-reproducible-reports.html#jupyter-as-a-reproducible-reporting-tool",
    "href": "lectures/920-reproducible-reports.html#jupyter-as-a-reproducible-reporting-tool",
    "title": "26  Reproducible reports",
    "section": "26.5 Jupyter as a reproducible reporting tool",
    "text": "26.5 Jupyter as a reproducible reporting tool\nJupyter notebooks along are reproducible reporting tools, and can be used as such, However, they have less power and functionality on their own compared to R Markdown on it’s own. To address this issue, a new tool, called Jupyter book, has been developed. It adds many of the features we have discussed above to Jupyter and makes it comparable to R Markdown.One important thing to note is that it is in early development stages still and changes to the package are being released frequently.\n\nJupyter book files\nA minimal Jupyter book report needs several files inside a directory, here is a quick rundown of each:\n\n_config.yml: configuration file for the Jupyter Book\n_toc.yml: table of contents file for the Jupyter Book\n&lt;jbook&gt;.ipynb: Jupyter Notebook with contents of the report\nreferences.bib: BibTeX references file for the report\n\nHere are the general Jupyter books docs: https://jupyterbook.org/intro.html\nBelow we link to particular sections useful for the tasks you need to accomplish for advanced reporting:\n\n\n26.5.1 Removing code cells from final report\nTo do this we add tags to the cell meta data (e.g., remove-input). For details see these docs:\n\nRemoving code cell input\n\n\n\n26.5.2 Building and formatting bibliographies\n\nBuilding and formatting bibliographies\n\n\n\n26.5.3 Render code inline in Markdown cells\n\nRender code inline in Markdown cells\n\n\n\n26.5.4 Figure formatting\n\nAutomatic figure numbering\nCross referencing figures in the text using automatically generated figure numbers\n\n\n\n26.5.5 Table formatting\n\nAutomatic table numbering - see the second example in this section\nCross referencing tables in the text using automatically generated table numbers",
    "crumbs": [
      "appendix.html",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Reproducible reports</span>"
    ]
  },
  {
    "objectID": "lectures/920-reproducible-reports.html#rendering-bookdown-and-jupyter-book-documents-at-the-command-line",
    "href": "lectures/920-reproducible-reports.html#rendering-bookdown-and-jupyter-book-documents-at-the-command-line",
    "title": "26  Reproducible reports",
    "section": "26.6 Rendering bookdown and jupyter-book documents at the command line",
    "text": "26.6 Rendering bookdown and jupyter-book documents at the command line\nJust like scripts, we can execute bookdown and jupyter-bookdocuments at the command line. This is a useful feature for reproducibility and automation of our analysis.\n\n26.6.1 Rendering bookdown documents at the command line\nTo render bookdown to HTML:\nRscript -e \"rmarkdown::render('doc/rmd_example/rmd_example.Rmd', 'bookdown::html_document2')\"\nTo render bookdown to PDF using LaTeX:\nRscript -e \"rmarkdown::render('doc/rmd_example/rmd_example.Rmd', 'bookdown::pdf_document2')\"\n\n\n26.6.2 Rendering jupyter-book documents at the command line\nDetailed docs for building a HTML book are here: https://jupyterbook.org/en/stable/basics/build.html\nTo render jupyter-book to HTML:\njb build &lt;folder_containing_report_files&gt;\nDetailed docs for building PDFs are here: https://jupyterbook.org/en/stable/advanced/pdf.html#build-a-pdf (including what dependencies are needed).\nTo render jupyter-book to PDF (converts HTML to PDF):\njupyter-book build &lt;folder_containing_report_files&gt; --builder pdfhtml\nTo render jupyter-book to PDF using LaTeX:\njb build &lt;folder_containing_report_files&gt; --builder pdflatex",
    "crumbs": [
      "appendix.html",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Reproducible reports</span>"
    ]
  }
]